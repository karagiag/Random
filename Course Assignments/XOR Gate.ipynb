{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import multilayer_perceptron as mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array([[0,0],[0,1],[1,0],[1,1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = mlp.MLPClassifier(hidden_layer_sizes=(5),activation='tanh', solver='sgd', alpha=0.001, learning_rate='constant',\n",
    "                   max_iter=10000, random_state=None, verbose=True, tol=0.00001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.89830024\n",
      "Iteration 2, loss = 0.89791456\n",
      "Iteration 3, loss = 0.89736564\n",
      "Iteration 4, loss = 0.89667100\n",
      "Iteration 5, loss = 0.89584664\n",
      "Iteration 6, loss = 0.89490715\n",
      "Iteration 7, loss = 0.89386580\n",
      "Iteration 8, loss = 0.89273465\n",
      "Iteration 9, loss = 0.89152462\n",
      "Iteration 10, loss = 0.89024564\n",
      "Iteration 11, loss = 0.88890666\n",
      "Iteration 12, loss = 0.88751581\n",
      "Iteration 13, loss = 0.88608040\n",
      "Iteration 14, loss = 0.88460703\n",
      "Iteration 15, loss = 0.88310166\n",
      "Iteration 16, loss = 0.88156964\n",
      "Iteration 17, loss = 0.88001580\n",
      "Iteration 18, loss = 0.87844446\n",
      "Iteration 19, loss = 0.87685951\n",
      "Iteration 20, loss = 0.87526443\n",
      "Iteration 21, loss = 0.87366234\n",
      "Iteration 22, loss = 0.87205602\n",
      "Iteration 23, loss = 0.87044798\n",
      "Iteration 24, loss = 0.86884043\n",
      "Iteration 25, loss = 0.86723535\n",
      "Iteration 26, loss = 0.86563451\n",
      "Iteration 27, loss = 0.86403946\n",
      "Iteration 28, loss = 0.86245158\n",
      "Iteration 29, loss = 0.86087211\n",
      "Iteration 30, loss = 0.85930212\n",
      "Iteration 31, loss = 0.85774255\n",
      "Iteration 32, loss = 0.85619424\n",
      "Iteration 33, loss = 0.85465790\n",
      "Iteration 34, loss = 0.85313417\n",
      "Iteration 35, loss = 0.85162360\n",
      "Iteration 36, loss = 0.85012664\n",
      "Iteration 37, loss = 0.84864369\n",
      "Iteration 38, loss = 0.84717509\n",
      "Iteration 39, loss = 0.84572112\n",
      "Iteration 40, loss = 0.84428201\n",
      "Iteration 41, loss = 0.84285794\n",
      "Iteration 42, loss = 0.84144906\n",
      "Iteration 43, loss = 0.84005547\n",
      "Iteration 44, loss = 0.83867726\n",
      "Iteration 45, loss = 0.83731447\n",
      "Iteration 46, loss = 0.83596713\n",
      "Iteration 47, loss = 0.83463523\n",
      "Iteration 48, loss = 0.83331875\n",
      "Iteration 49, loss = 0.83201766\n",
      "Iteration 50, loss = 0.83073190\n",
      "Iteration 51, loss = 0.82946141\n",
      "Iteration 52, loss = 0.82820609\n",
      "Iteration 53, loss = 0.82696587\n",
      "Iteration 54, loss = 0.82574063\n",
      "Iteration 55, loss = 0.82453026\n",
      "Iteration 56, loss = 0.82333465\n",
      "Iteration 57, loss = 0.82215368\n",
      "Iteration 58, loss = 0.82098720\n",
      "Iteration 59, loss = 0.81983510\n",
      "Iteration 60, loss = 0.81869721\n",
      "Iteration 61, loss = 0.81757341\n",
      "Iteration 62, loss = 0.81646355\n",
      "Iteration 63, loss = 0.81536747\n",
      "Iteration 64, loss = 0.81428503\n",
      "Iteration 65, loss = 0.81321607\n",
      "Iteration 66, loss = 0.81216044\n",
      "Iteration 67, loss = 0.81111799\n",
      "Iteration 68, loss = 0.81008855\n",
      "Iteration 69, loss = 0.80907197\n",
      "Iteration 70, loss = 0.80806810\n",
      "Iteration 71, loss = 0.80707678\n",
      "Iteration 72, loss = 0.80609785\n",
      "Iteration 73, loss = 0.80513115\n",
      "Iteration 74, loss = 0.80417654\n",
      "Iteration 75, loss = 0.80323385\n",
      "Iteration 76, loss = 0.80230294\n",
      "Iteration 77, loss = 0.80138365\n",
      "Iteration 78, loss = 0.80047582\n",
      "Iteration 79, loss = 0.79957931\n",
      "Iteration 80, loss = 0.79869397\n",
      "Iteration 81, loss = 0.79781964\n",
      "Iteration 82, loss = 0.79695619\n",
      "Iteration 83, loss = 0.79610345\n",
      "Iteration 84, loss = 0.79526130\n",
      "Iteration 85, loss = 0.79442959\n",
      "Iteration 86, loss = 0.79360816\n",
      "Iteration 87, loss = 0.79279689\n",
      "Iteration 88, loss = 0.79199564\n",
      "Iteration 89, loss = 0.79120426\n",
      "Iteration 90, loss = 0.79042262\n",
      "Iteration 91, loss = 0.78965058\n",
      "Iteration 92, loss = 0.78888802\n",
      "Iteration 93, loss = 0.78813479\n",
      "Iteration 94, loss = 0.78739078\n",
      "Iteration 95, loss = 0.78665585\n",
      "Iteration 96, loss = 0.78592987\n",
      "Iteration 97, loss = 0.78521271\n",
      "Iteration 98, loss = 0.78450426\n",
      "Iteration 99, loss = 0.78380440\n",
      "Iteration 100, loss = 0.78311299\n",
      "Iteration 101, loss = 0.78242992\n",
      "Iteration 102, loss = 0.78175507\n",
      "Iteration 103, loss = 0.78108833\n",
      "Iteration 104, loss = 0.78042958\n",
      "Iteration 105, loss = 0.77977870\n",
      "Iteration 106, loss = 0.77913559\n",
      "Iteration 107, loss = 0.77850013\n",
      "Iteration 108, loss = 0.77787222\n",
      "Iteration 109, loss = 0.77725174\n",
      "Iteration 110, loss = 0.77663859\n",
      "Iteration 111, loss = 0.77603267\n",
      "Iteration 112, loss = 0.77543387\n",
      "Iteration 113, loss = 0.77484208\n",
      "Iteration 114, loss = 0.77425721\n",
      "Iteration 115, loss = 0.77367916\n",
      "Iteration 116, loss = 0.77310783\n",
      "Iteration 117, loss = 0.77254312\n",
      "Iteration 118, loss = 0.77198494\n",
      "Iteration 119, loss = 0.77143319\n",
      "Iteration 120, loss = 0.77088778\n",
      "Iteration 121, loss = 0.77034862\n",
      "Iteration 122, loss = 0.76981562\n",
      "Iteration 123, loss = 0.76928868\n",
      "Iteration 124, loss = 0.76876773\n",
      "Iteration 125, loss = 0.76825267\n",
      "Iteration 126, loss = 0.76774342\n",
      "Iteration 127, loss = 0.76723989\n",
      "Iteration 128, loss = 0.76674200\n",
      "Iteration 129, loss = 0.76624967\n",
      "Iteration 130, loss = 0.76576283\n",
      "Iteration 131, loss = 0.76528138\n",
      "Iteration 132, loss = 0.76480525\n",
      "Iteration 133, loss = 0.76433436\n",
      "Iteration 134, loss = 0.76386864\n",
      "Iteration 135, loss = 0.76340801\n",
      "Iteration 136, loss = 0.76295241\n",
      "Iteration 137, loss = 0.76250174\n",
      "Iteration 138, loss = 0.76205595\n",
      "Iteration 139, loss = 0.76161496\n",
      "Iteration 140, loss = 0.76117870\n",
      "Iteration 141, loss = 0.76074711\n",
      "Iteration 142, loss = 0.76032012\n",
      "Iteration 143, loss = 0.75989765\n",
      "Iteration 144, loss = 0.75947964\n",
      "Iteration 145, loss = 0.75906604\n",
      "Iteration 146, loss = 0.75865677\n",
      "Iteration 147, loss = 0.75825177\n",
      "Iteration 148, loss = 0.75785098\n",
      "Iteration 149, loss = 0.75745433\n",
      "Iteration 150, loss = 0.75706178\n",
      "Iteration 151, loss = 0.75667326\n",
      "Iteration 152, loss = 0.75628871\n",
      "Iteration 153, loss = 0.75590807\n",
      "Iteration 154, loss = 0.75553128\n",
      "Iteration 155, loss = 0.75515830\n",
      "Iteration 156, loss = 0.75478907\n",
      "Iteration 157, loss = 0.75442352\n",
      "Iteration 158, loss = 0.75406162\n",
      "Iteration 159, loss = 0.75370330\n",
      "Iteration 160, loss = 0.75334852\n",
      "Iteration 161, loss = 0.75299722\n",
      "Iteration 162, loss = 0.75264935\n",
      "Iteration 163, loss = 0.75230487\n",
      "Iteration 164, loss = 0.75196373\n",
      "Iteration 165, loss = 0.75162587\n",
      "Iteration 166, loss = 0.75129125\n",
      "Iteration 167, loss = 0.75095983\n",
      "Iteration 168, loss = 0.75063155\n",
      "Iteration 169, loss = 0.75030638\n",
      "Iteration 170, loss = 0.74998427\n",
      "Iteration 171, loss = 0.74966518\n",
      "Iteration 172, loss = 0.74934905\n",
      "Iteration 173, loss = 0.74903586\n",
      "Iteration 174, loss = 0.74872556\n",
      "Iteration 175, loss = 0.74841810\n",
      "Iteration 176, loss = 0.74811346\n",
      "Iteration 177, loss = 0.74781158\n",
      "Iteration 178, loss = 0.74751243\n",
      "Iteration 179, loss = 0.74721597\n",
      "Iteration 180, loss = 0.74692217\n",
      "Iteration 181, loss = 0.74663098\n",
      "Iteration 182, loss = 0.74634236\n",
      "Iteration 183, loss = 0.74605629\n",
      "Iteration 184, loss = 0.74577273\n",
      "Iteration 185, loss = 0.74549164\n",
      "Iteration 186, loss = 0.74521299\n",
      "Iteration 187, loss = 0.74493674\n",
      "Iteration 188, loss = 0.74466287\n",
      "Iteration 189, loss = 0.74439133\n",
      "Iteration 190, loss = 0.74412209\n",
      "Iteration 191, loss = 0.74385513\n",
      "Iteration 192, loss = 0.74359041\n",
      "Iteration 193, loss = 0.74332790\n",
      "Iteration 194, loss = 0.74306758\n",
      "Iteration 195, loss = 0.74280940\n",
      "Iteration 196, loss = 0.74255335\n",
      "Iteration 197, loss = 0.74229939\n",
      "Iteration 198, loss = 0.74204749\n",
      "Iteration 199, loss = 0.74179763\n",
      "Iteration 200, loss = 0.74154978\n",
      "Iteration 201, loss = 0.74130391\n",
      "Iteration 202, loss = 0.74106000\n",
      "Iteration 203, loss = 0.74081802\n",
      "Iteration 204, loss = 0.74057793\n",
      "Iteration 205, loss = 0.74033973\n",
      "Iteration 206, loss = 0.74010338\n",
      "Iteration 207, loss = 0.73986885\n",
      "Iteration 208, loss = 0.73963613\n",
      "Iteration 209, loss = 0.73940519\n",
      "Iteration 210, loss = 0.73917601\n",
      "Iteration 211, loss = 0.73894856\n",
      "Iteration 212, loss = 0.73872282\n",
      "Iteration 213, loss = 0.73849876\n",
      "Iteration 214, loss = 0.73827638\n",
      "Iteration 215, loss = 0.73805563\n",
      "Iteration 216, loss = 0.73783651\n",
      "Iteration 217, loss = 0.73761900\n",
      "Iteration 218, loss = 0.73740306\n",
      "Iteration 219, loss = 0.73718869\n",
      "Iteration 220, loss = 0.73697585\n",
      "Iteration 221, loss = 0.73676454\n",
      "Iteration 222, loss = 0.73655473\n",
      "Iteration 223, loss = 0.73634640\n",
      "Iteration 224, loss = 0.73613954\n",
      "Iteration 225, loss = 0.73593412\n",
      "Iteration 226, loss = 0.73573013\n",
      "Iteration 227, loss = 0.73552755\n",
      "Iteration 228, loss = 0.73532636\n",
      "Iteration 229, loss = 0.73512654\n",
      "Iteration 230, loss = 0.73492809\n",
      "Iteration 231, loss = 0.73473097\n",
      "Iteration 232, loss = 0.73453518\n",
      "Iteration 233, loss = 0.73434070\n",
      "Iteration 234, loss = 0.73414750\n",
      "Iteration 235, loss = 0.73395559\n",
      "Iteration 236, loss = 0.73376494\n",
      "Iteration 237, loss = 0.73357553\n",
      "Iteration 238, loss = 0.73338735\n",
      "Iteration 239, loss = 0.73320039\n",
      "Iteration 240, loss = 0.73301463\n",
      "Iteration 241, loss = 0.73283005\n",
      "Iteration 242, loss = 0.73264665\n",
      "Iteration 243, loss = 0.73246441\n",
      "Iteration 244, loss = 0.73228332\n",
      "Iteration 245, loss = 0.73210335\n",
      "Iteration 246, loss = 0.73192451\n",
      "Iteration 247, loss = 0.73174677\n",
      "Iteration 248, loss = 0.73157012\n",
      "Iteration 249, loss = 0.73139455\n",
      "Iteration 250, loss = 0.73122006\n",
      "Iteration 251, loss = 0.73104661\n",
      "Iteration 252, loss = 0.73087421\n",
      "Iteration 253, loss = 0.73070285\n",
      "Iteration 254, loss = 0.73053250\n",
      "Iteration 255, loss = 0.73036316\n",
      "Iteration 256, loss = 0.73019482\n",
      "Iteration 257, loss = 0.73002747\n",
      "Iteration 258, loss = 0.72986109\n",
      "Iteration 259, loss = 0.72969568\n",
      "Iteration 260, loss = 0.72953122\n",
      "Iteration 261, loss = 0.72936770\n",
      "Iteration 262, loss = 0.72920512\n",
      "Iteration 263, loss = 0.72904346\n",
      "Iteration 264, loss = 0.72888271\n",
      "Iteration 265, loss = 0.72872286\n",
      "Iteration 266, loss = 0.72856391\n",
      "Iteration 267, loss = 0.72840585\n",
      "Iteration 268, loss = 0.72824866\n",
      "Iteration 269, loss = 0.72809233\n",
      "Iteration 270, loss = 0.72793686\n",
      "Iteration 271, loss = 0.72778224\n",
      "Iteration 272, loss = 0.72762846\n",
      "Iteration 273, loss = 0.72747550\n",
      "Iteration 274, loss = 0.72732337\n",
      "Iteration 275, loss = 0.72717205\n",
      "Iteration 276, loss = 0.72702154\n",
      "Iteration 277, loss = 0.72687182\n",
      "Iteration 278, loss = 0.72672289\n",
      "Iteration 279, loss = 0.72657474\n",
      "Iteration 280, loss = 0.72642737\n",
      "Iteration 281, loss = 0.72628076\n",
      "Iteration 282, loss = 0.72613490\n",
      "Iteration 283, loss = 0.72598980\n",
      "Iteration 284, loss = 0.72584544\n",
      "Iteration 285, loss = 0.72570181\n",
      "Iteration 286, loss = 0.72555891\n",
      "Iteration 287, loss = 0.72541674\n",
      "Iteration 288, loss = 0.72527528\n",
      "Iteration 289, loss = 0.72513452\n",
      "Iteration 290, loss = 0.72499447\n",
      "Iteration 291, loss = 0.72485511\n",
      "Iteration 292, loss = 0.72471643\n",
      "Iteration 293, loss = 0.72457844\n",
      "Iteration 294, loss = 0.72444112\n",
      "Iteration 295, loss = 0.72430447\n",
      "Iteration 296, loss = 0.72416849\n",
      "Iteration 297, loss = 0.72403316\n",
      "Iteration 298, loss = 0.72389848\n",
      "Iteration 299, loss = 0.72376444\n",
      "Iteration 300, loss = 0.72363104\n",
      "Iteration 301, loss = 0.72349828\n",
      "Iteration 302, loss = 0.72336614\n",
      "Iteration 303, loss = 0.72323462\n",
      "Iteration 304, loss = 0.72310372\n",
      "Iteration 305, loss = 0.72297343\n",
      "Iteration 306, loss = 0.72284374\n",
      "Iteration 307, loss = 0.72271466\n",
      "Iteration 308, loss = 0.72258616\n",
      "Iteration 309, loss = 0.72245826\n",
      "Iteration 310, loss = 0.72233094\n",
      "Iteration 311, loss = 0.72220421\n",
      "Iteration 312, loss = 0.72207804\n",
      "Iteration 313, loss = 0.72195245\n",
      "Iteration 314, loss = 0.72182742\n",
      "Iteration 315, loss = 0.72170295\n",
      "Iteration 316, loss = 0.72157904\n",
      "Iteration 317, loss = 0.72145567\n",
      "Iteration 318, loss = 0.72133286\n",
      "Iteration 319, loss = 0.72121058\n",
      "Iteration 320, loss = 0.72108885\n",
      "Iteration 321, loss = 0.72096764\n",
      "Iteration 322, loss = 0.72084697\n",
      "Iteration 323, loss = 0.72072682\n",
      "Iteration 324, loss = 0.72060719\n",
      "Iteration 325, loss = 0.72048807\n",
      "Iteration 326, loss = 0.72036947\n",
      "Iteration 327, loss = 0.72025138\n",
      "Iteration 328, loss = 0.72013379\n",
      "Iteration 329, loss = 0.72001670\n",
      "Iteration 330, loss = 0.71990010\n",
      "Iteration 331, loss = 0.71978400\n",
      "Iteration 332, loss = 0.71966839\n",
      "Iteration 333, loss = 0.71955327\n",
      "Iteration 334, loss = 0.71943862\n",
      "Iteration 335, loss = 0.71932445\n",
      "Iteration 336, loss = 0.71921076\n",
      "Iteration 337, loss = 0.71909754\n",
      "Iteration 338, loss = 0.71898478\n",
      "Iteration 339, loss = 0.71887249\n",
      "Iteration 340, loss = 0.71876066\n",
      "Iteration 341, loss = 0.71864928\n",
      "Iteration 342, loss = 0.71853836\n",
      "Iteration 343, loss = 0.71842789\n",
      "Iteration 344, loss = 0.71831787\n",
      "Iteration 345, loss = 0.71820829\n",
      "Iteration 346, loss = 0.71809915\n",
      "Iteration 347, loss = 0.71799044\n",
      "Iteration 348, loss = 0.71788218\n",
      "Iteration 349, loss = 0.71777434\n",
      "Iteration 350, loss = 0.71766693\n",
      "Iteration 351, loss = 0.71755995\n",
      "Iteration 352, loss = 0.71745339\n",
      "Iteration 353, loss = 0.71734725\n",
      "Iteration 354, loss = 0.71724153\n",
      "Iteration 355, loss = 0.71713622\n",
      "Iteration 356, loss = 0.71703132\n",
      "Iteration 357, loss = 0.71692683\n",
      "Iteration 358, loss = 0.71682274\n",
      "Iteration 359, loss = 0.71671906\n",
      "Iteration 360, loss = 0.71661578\n",
      "Iteration 361, loss = 0.71651289\n",
      "Iteration 362, loss = 0.71641040\n",
      "Iteration 363, loss = 0.71630831\n",
      "Iteration 364, loss = 0.71620660\n",
      "Iteration 365, loss = 0.71610528\n",
      "Iteration 366, loss = 0.71600434\n",
      "Iteration 367, loss = 0.71590379\n",
      "Iteration 368, loss = 0.71580361\n",
      "Iteration 369, loss = 0.71570382\n",
      "Iteration 370, loss = 0.71560439\n",
      "Iteration 371, loss = 0.71550534\n",
      "Iteration 372, loss = 0.71540666\n",
      "Iteration 373, loss = 0.71530835\n",
      "Iteration 374, loss = 0.71521041\n",
      "Iteration 375, loss = 0.71511282\n",
      "Iteration 376, loss = 0.71501560\n",
      "Iteration 377, loss = 0.71491874\n",
      "Iteration 378, loss = 0.71482223\n",
      "Iteration 379, loss = 0.71472608\n",
      "Iteration 380, loss = 0.71463028\n",
      "Iteration 381, loss = 0.71453483\n",
      "Iteration 382, loss = 0.71443972\n",
      "Iteration 383, loss = 0.71434497\n",
      "Iteration 384, loss = 0.71425055\n",
      "Iteration 385, loss = 0.71415648\n",
      "Iteration 386, loss = 0.71406275\n",
      "Iteration 387, loss = 0.71396936\n",
      "Iteration 388, loss = 0.71387630\n",
      "Iteration 389, loss = 0.71378357\n",
      "Iteration 390, loss = 0.71369118\n",
      "Iteration 391, loss = 0.71359912\n",
      "Iteration 392, loss = 0.71350738\n",
      "Iteration 393, loss = 0.71341597\n",
      "Iteration 394, loss = 0.71332489\n",
      "Iteration 395, loss = 0.71323413\n",
      "Iteration 396, loss = 0.71314368\n",
      "Iteration 397, loss = 0.71305356\n",
      "Iteration 398, loss = 0.71296375\n",
      "Iteration 399, loss = 0.71287426\n",
      "Iteration 400, loss = 0.71278508\n",
      "Iteration 401, loss = 0.71269621\n",
      "Iteration 402, loss = 0.71260766\n",
      "Iteration 403, loss = 0.71251941\n",
      "Iteration 404, loss = 0.71243146\n",
      "Iteration 405, loss = 0.71234382\n",
      "Iteration 406, loss = 0.71225649\n",
      "Iteration 407, loss = 0.71216945\n",
      "Iteration 408, loss = 0.71208272\n",
      "Iteration 409, loss = 0.71199628\n",
      "Iteration 410, loss = 0.71191014\n",
      "Iteration 411, loss = 0.71182429\n",
      "Iteration 412, loss = 0.71173874\n",
      "Iteration 413, loss = 0.71165348\n",
      "Iteration 414, loss = 0.71156850\n",
      "Iteration 415, loss = 0.71148382\n",
      "Iteration 416, loss = 0.71139942\n",
      "Iteration 417, loss = 0.71131531\n",
      "Iteration 418, loss = 0.71123149\n",
      "Iteration 419, loss = 0.71114794\n",
      "Iteration 420, loss = 0.71106468\n",
      "Iteration 421, loss = 0.71098169\n",
      "Iteration 422, loss = 0.71089899\n",
      "Iteration 423, loss = 0.71081655\n",
      "Iteration 424, loss = 0.71073440\n",
      "Iteration 425, loss = 0.71065252\n",
      "Iteration 426, loss = 0.71057091\n",
      "Iteration 427, loss = 0.71048957\n",
      "Iteration 428, loss = 0.71040850\n",
      "Iteration 429, loss = 0.71032770\n",
      "Iteration 430, loss = 0.71024716\n",
      "Iteration 431, loss = 0.71016689\n",
      "Iteration 432, loss = 0.71008689\n",
      "Iteration 433, loss = 0.71000714\n",
      "Iteration 434, loss = 0.70992766\n",
      "Iteration 435, loss = 0.70984844\n",
      "Iteration 436, loss = 0.70976947\n",
      "Iteration 437, loss = 0.70969077\n",
      "Iteration 438, loss = 0.70961232\n",
      "Iteration 439, loss = 0.70953412\n",
      "Iteration 440, loss = 0.70945618\n",
      "Iteration 441, loss = 0.70937849\n",
      "Iteration 442, loss = 0.70930105\n",
      "Iteration 443, loss = 0.70922386\n",
      "Iteration 444, loss = 0.70914692\n",
      "Iteration 445, loss = 0.70907023\n",
      "Iteration 446, loss = 0.70899378\n",
      "Iteration 447, loss = 0.70891758\n",
      "Iteration 448, loss = 0.70884162\n",
      "Iteration 449, loss = 0.70876590\n",
      "Iteration 450, loss = 0.70869043\n",
      "Iteration 451, loss = 0.70861519\n",
      "Iteration 452, loss = 0.70854019\n",
      "Iteration 453, loss = 0.70846544\n",
      "Iteration 454, loss = 0.70839091\n",
      "Iteration 455, loss = 0.70831663\n",
      "Iteration 456, loss = 0.70824258\n",
      "Iteration 457, loss = 0.70816876\n",
      "Iteration 458, loss = 0.70809517\n",
      "Iteration 459, loss = 0.70802181\n",
      "Iteration 460, loss = 0.70794869\n",
      "Iteration 461, loss = 0.70787579\n",
      "Iteration 462, loss = 0.70780312\n",
      "Iteration 463, loss = 0.70773068\n",
      "Iteration 464, loss = 0.70765846\n",
      "Iteration 465, loss = 0.70758647\n",
      "Iteration 466, loss = 0.70751470\n",
      "Iteration 467, loss = 0.70744315\n",
      "Iteration 468, loss = 0.70737183\n",
      "Iteration 469, loss = 0.70730072\n",
      "Iteration 470, loss = 0.70722984\n",
      "Iteration 471, loss = 0.70715917\n",
      "Iteration 472, loss = 0.70708872\n",
      "Iteration 473, loss = 0.70701849\n",
      "Iteration 474, loss = 0.70694847\n",
      "Iteration 475, loss = 0.70687866\n",
      "Iteration 476, loss = 0.70680907\n",
      "Iteration 477, loss = 0.70673969\n",
      "Iteration 478, loss = 0.70667053\n",
      "Iteration 479, loss = 0.70660157\n",
      "Iteration 480, loss = 0.70653282\n",
      "Iteration 481, loss = 0.70646428\n",
      "Iteration 482, loss = 0.70639595\n",
      "Iteration 483, loss = 0.70632782\n",
      "Iteration 484, loss = 0.70625990\n",
      "Iteration 485, loss = 0.70619219\n",
      "Iteration 486, loss = 0.70612468\n",
      "Iteration 487, loss = 0.70605737\n",
      "Iteration 488, loss = 0.70599026\n",
      "Iteration 489, loss = 0.70592336\n",
      "Iteration 490, loss = 0.70585665\n",
      "Iteration 491, loss = 0.70579014\n",
      "Iteration 492, loss = 0.70572384\n",
      "Iteration 493, loss = 0.70565772\n",
      "Iteration 494, loss = 0.70559181\n",
      "Iteration 495, loss = 0.70552609\n",
      "Iteration 496, loss = 0.70546057\n",
      "Iteration 497, loss = 0.70539523\n",
      "Iteration 498, loss = 0.70533010\n",
      "Iteration 499, loss = 0.70526515\n",
      "Iteration 500, loss = 0.70520040\n",
      "Iteration 501, loss = 0.70513583\n",
      "Iteration 502, loss = 0.70507146\n",
      "Iteration 503, loss = 0.70500727\n",
      "Iteration 504, loss = 0.70494328\n",
      "Iteration 505, loss = 0.70487946\n",
      "Iteration 506, loss = 0.70481584\n",
      "Iteration 507, loss = 0.70475240\n",
      "Iteration 508, loss = 0.70468915\n",
      "Iteration 509, loss = 0.70462607\n",
      "Iteration 510, loss = 0.70456319\n",
      "Iteration 511, loss = 0.70450048\n",
      "Iteration 512, loss = 0.70443796\n",
      "Iteration 513, loss = 0.70437561\n",
      "Iteration 514, loss = 0.70431345\n",
      "Iteration 515, loss = 0.70425146\n",
      "Iteration 516, loss = 0.70418965\n",
      "Iteration 517, loss = 0.70412802\n",
      "Iteration 518, loss = 0.70406657\n",
      "Iteration 519, loss = 0.70400529\n",
      "Iteration 520, loss = 0.70394419\n",
      "Iteration 521, loss = 0.70388326\n",
      "Iteration 522, loss = 0.70382251\n",
      "Iteration 523, loss = 0.70376193\n",
      "Iteration 524, loss = 0.70370152\n",
      "Iteration 525, loss = 0.70364128\n",
      "Iteration 526, loss = 0.70358121\n",
      "Iteration 527, loss = 0.70352131\n",
      "Iteration 528, loss = 0.70346159\n",
      "Iteration 529, loss = 0.70340203\n",
      "Iteration 530, loss = 0.70334263\n",
      "Iteration 531, loss = 0.70328341\n",
      "Iteration 532, loss = 0.70322435\n",
      "Iteration 533, loss = 0.70316545\n",
      "Iteration 534, loss = 0.70310672\n",
      "Iteration 535, loss = 0.70304816\n",
      "Iteration 536, loss = 0.70298976\n",
      "Iteration 537, loss = 0.70293152\n",
      "Iteration 538, loss = 0.70287344\n",
      "Iteration 539, loss = 0.70281552\n",
      "Iteration 540, loss = 0.70275777\n",
      "Iteration 541, loss = 0.70270017\n",
      "Iteration 542, loss = 0.70264274\n",
      "Iteration 543, loss = 0.70258546\n",
      "Iteration 544, loss = 0.70252834\n",
      "Iteration 545, loss = 0.70247137\n",
      "Iteration 546, loss = 0.70241457\n",
      "Iteration 547, loss = 0.70235792\n",
      "Iteration 548, loss = 0.70230142\n",
      "Iteration 549, loss = 0.70224508\n",
      "Iteration 550, loss = 0.70218889\n",
      "Iteration 551, loss = 0.70213286\n",
      "Iteration 552, loss = 0.70207698\n",
      "Iteration 553, loss = 0.70202125\n",
      "Iteration 554, loss = 0.70196567\n",
      "Iteration 555, loss = 0.70191024\n",
      "Iteration 556, loss = 0.70185497\n",
      "Iteration 557, loss = 0.70179984\n",
      "Iteration 558, loss = 0.70174486\n",
      "Iteration 559, loss = 0.70169003\n",
      "Iteration 560, loss = 0.70163534\n",
      "Iteration 561, loss = 0.70158081\n",
      "Iteration 562, loss = 0.70152642\n",
      "Iteration 563, loss = 0.70147217\n",
      "Iteration 564, loss = 0.70141807\n",
      "Iteration 565, loss = 0.70136412\n",
      "Iteration 566, loss = 0.70131030\n",
      "Iteration 567, loss = 0.70125663\n",
      "Iteration 568, loss = 0.70120311\n",
      "Iteration 569, loss = 0.70114972\n",
      "Iteration 570, loss = 0.70109648\n",
      "Iteration 571, loss = 0.70104338\n",
      "Iteration 572, loss = 0.70099042\n",
      "Iteration 573, loss = 0.70093760\n",
      "Iteration 574, loss = 0.70088491\n",
      "Iteration 575, loss = 0.70083237\n",
      "Iteration 576, loss = 0.70077996\n",
      "Iteration 577, loss = 0.70072769\n",
      "Iteration 578, loss = 0.70067556\n",
      "Iteration 579, loss = 0.70062356\n",
      "Iteration 580, loss = 0.70057170\n",
      "Iteration 581, loss = 0.70051997\n",
      "Iteration 582, loss = 0.70046838\n",
      "Iteration 583, loss = 0.70041692\n",
      "Iteration 584, loss = 0.70036560\n",
      "Iteration 585, loss = 0.70031441\n",
      "Iteration 586, loss = 0.70026335\n",
      "Iteration 587, loss = 0.70021242\n",
      "Iteration 588, loss = 0.70016162\n",
      "Iteration 589, loss = 0.70011096\n",
      "Iteration 590, loss = 0.70006042\n",
      "Iteration 591, loss = 0.70001001\n",
      "Iteration 592, loss = 0.69995974\n",
      "Iteration 593, loss = 0.69990959\n",
      "Iteration 594, loss = 0.69985956\n",
      "Iteration 595, loss = 0.69980967\n",
      "Iteration 596, loss = 0.69975990\n",
      "Iteration 597, loss = 0.69971026\n",
      "Iteration 598, loss = 0.69966075\n",
      "Iteration 599, loss = 0.69961136\n",
      "Iteration 600, loss = 0.69956209\n",
      "Iteration 601, loss = 0.69951295\n",
      "Iteration 602, loss = 0.69946393\n",
      "Iteration 603, loss = 0.69941504\n",
      "Iteration 604, loss = 0.69936627\n",
      "Iteration 605, loss = 0.69931762\n",
      "Iteration 606, loss = 0.69926909\n",
      "Iteration 607, loss = 0.69922068\n",
      "Iteration 608, loss = 0.69917240\n",
      "Iteration 609, loss = 0.69912423\n",
      "Iteration 610, loss = 0.69907619\n",
      "Iteration 611, loss = 0.69902826\n",
      "Iteration 612, loss = 0.69898045\n",
      "Iteration 613, loss = 0.69893276\n",
      "Iteration 614, loss = 0.69888519\n",
      "Iteration 615, loss = 0.69883773\n",
      "Iteration 616, loss = 0.69879040\n",
      "Iteration 617, loss = 0.69874317\n",
      "Iteration 618, loss = 0.69869607\n",
      "Iteration 619, loss = 0.69864908\n",
      "Iteration 620, loss = 0.69860220\n",
      "Iteration 621, loss = 0.69855544\n",
      "Iteration 622, loss = 0.69850879\n",
      "Iteration 623, loss = 0.69846226\n",
      "Iteration 624, loss = 0.69841583\n",
      "Iteration 625, loss = 0.69836952\n",
      "Iteration 626, loss = 0.69832333\n",
      "Iteration 627, loss = 0.69827724\n",
      "Iteration 628, loss = 0.69823127\n",
      "Iteration 629, loss = 0.69818540\n",
      "Iteration 630, loss = 0.69813965\n",
      "Iteration 631, loss = 0.69809401\n",
      "Iteration 632, loss = 0.69804847\n",
      "Iteration 633, loss = 0.69800305\n",
      "Iteration 634, loss = 0.69795773\n",
      "Iteration 635, loss = 0.69791252\n",
      "Iteration 636, loss = 0.69786742\n",
      "Iteration 637, loss = 0.69782242\n",
      "Iteration 638, loss = 0.69777753\n",
      "Iteration 639, loss = 0.69773275\n",
      "Iteration 640, loss = 0.69768807\n",
      "Iteration 641, loss = 0.69764350\n",
      "Iteration 642, loss = 0.69759904\n",
      "Iteration 643, loss = 0.69755468\n",
      "Iteration 644, loss = 0.69751042\n",
      "Iteration 645, loss = 0.69746626\n",
      "Iteration 646, loss = 0.69742221\n",
      "Iteration 647, loss = 0.69737826\n",
      "Iteration 648, loss = 0.69733442\n",
      "Iteration 649, loss = 0.69729067\n",
      "Iteration 650, loss = 0.69724703\n",
      "Iteration 651, loss = 0.69720349\n",
      "Iteration 652, loss = 0.69716005\n",
      "Iteration 653, loss = 0.69711671\n",
      "Iteration 654, loss = 0.69707347\n",
      "Iteration 655, loss = 0.69703033\n",
      "Iteration 656, loss = 0.69698728\n",
      "Iteration 657, loss = 0.69694434\n",
      "Iteration 658, loss = 0.69690149\n",
      "Iteration 659, loss = 0.69685875\n",
      "Iteration 660, loss = 0.69681610\n",
      "Iteration 661, loss = 0.69677354\n",
      "Iteration 662, loss = 0.69673109\n",
      "Iteration 663, loss = 0.69668872\n",
      "Iteration 664, loss = 0.69664646\n",
      "Iteration 665, loss = 0.69660429\n",
      "Iteration 666, loss = 0.69656222\n",
      "Iteration 667, loss = 0.69652024\n",
      "Iteration 668, loss = 0.69647835\n",
      "Iteration 669, loss = 0.69643656\n",
      "Iteration 670, loss = 0.69639486\n",
      "Iteration 671, loss = 0.69635326\n",
      "Iteration 672, loss = 0.69631174\n",
      "Iteration 673, loss = 0.69627032\n",
      "Iteration 674, loss = 0.69622900\n",
      "Iteration 675, loss = 0.69618776\n",
      "Iteration 676, loss = 0.69614661\n",
      "Iteration 677, loss = 0.69610556\n",
      "Iteration 678, loss = 0.69606460\n",
      "Iteration 679, loss = 0.69602372\n",
      "Iteration 680, loss = 0.69598294\n",
      "Iteration 681, loss = 0.69594224\n",
      "Iteration 682, loss = 0.69590164\n",
      "Iteration 683, loss = 0.69586112\n",
      "Iteration 684, loss = 0.69582069\n",
      "Iteration 685, loss = 0.69578035\n",
      "Iteration 686, loss = 0.69574010\n",
      "Iteration 687, loss = 0.69569993\n",
      "Iteration 688, loss = 0.69565985\n",
      "Iteration 689, loss = 0.69561986\n",
      "Iteration 690, loss = 0.69557995\n",
      "Iteration 691, loss = 0.69554013\n",
      "Iteration 692, loss = 0.69550039\n",
      "Iteration 693, loss = 0.69546074\n",
      "Iteration 694, loss = 0.69542118\n",
      "Iteration 695, loss = 0.69538170\n",
      "Iteration 696, loss = 0.69534230\n",
      "Iteration 697, loss = 0.69530298\n",
      "Iteration 698, loss = 0.69526375\n",
      "Iteration 699, loss = 0.69522461\n",
      "Iteration 700, loss = 0.69518554\n",
      "Iteration 701, loss = 0.69514656\n",
      "Iteration 702, loss = 0.69510766\n",
      "Iteration 703, loss = 0.69506884\n",
      "Iteration 704, loss = 0.69503010\n",
      "Iteration 705, loss = 0.69499144\n",
      "Iteration 706, loss = 0.69495287\n",
      "Iteration 707, loss = 0.69491437\n",
      "Iteration 708, loss = 0.69487596\n",
      "Iteration 709, loss = 0.69483762\n",
      "Iteration 710, loss = 0.69479936\n",
      "Iteration 711, loss = 0.69476119\n",
      "Iteration 712, loss = 0.69472309\n",
      "Iteration 713, loss = 0.69468507\n",
      "Iteration 714, loss = 0.69464712\n",
      "Iteration 715, loss = 0.69460926\n",
      "Iteration 716, loss = 0.69457147\n",
      "Iteration 717, loss = 0.69453376\n",
      "Iteration 718, loss = 0.69449612\n",
      "Iteration 719, loss = 0.69445857\n",
      "Iteration 720, loss = 0.69442109\n",
      "Iteration 721, loss = 0.69438368\n",
      "Iteration 722, loss = 0.69434635\n",
      "Iteration 723, loss = 0.69430910\n",
      "Iteration 724, loss = 0.69427191\n",
      "Iteration 725, loss = 0.69423481\n",
      "Iteration 726, loss = 0.69419778\n",
      "Iteration 727, loss = 0.69416082\n",
      "Iteration 728, loss = 0.69412394\n",
      "Iteration 729, loss = 0.69408713\n",
      "Iteration 730, loss = 0.69405039\n",
      "Iteration 731, loss = 0.69401373\n",
      "Iteration 732, loss = 0.69397713\n",
      "Iteration 733, loss = 0.69394061\n",
      "Iteration 734, loss = 0.69390417\n",
      "Iteration 735, loss = 0.69386779\n",
      "Iteration 736, loss = 0.69383148\n",
      "Iteration 737, loss = 0.69379525\n",
      "Iteration 738, loss = 0.69375909\n",
      "Iteration 739, loss = 0.69372299\n",
      "Iteration 740, loss = 0.69368697\n",
      "Iteration 741, loss = 0.69365102\n",
      "Iteration 742, loss = 0.69361513\n",
      "Iteration 743, loss = 0.69357932\n",
      "Iteration 744, loss = 0.69354357\n",
      "Iteration 745, loss = 0.69350789\n",
      "Iteration 746, loss = 0.69347229\n",
      "Iteration 747, loss = 0.69343674\n",
      "Iteration 748, loss = 0.69340127\n",
      "Iteration 749, loss = 0.69336587\n",
      "Iteration 750, loss = 0.69333053\n",
      "Iteration 751, loss = 0.69329526\n",
      "Iteration 752, loss = 0.69326005\n",
      "Iteration 753, loss = 0.69322491\n",
      "Iteration 754, loss = 0.69318984\n",
      "Iteration 755, loss = 0.69315484\n",
      "Iteration 756, loss = 0.69311990\n",
      "Iteration 757, loss = 0.69308502\n",
      "Iteration 758, loss = 0.69305021\n",
      "Iteration 759, loss = 0.69301547\n",
      "Iteration 760, loss = 0.69298078\n",
      "Iteration 761, loss = 0.69294617\n",
      "Iteration 762, loss = 0.69291162\n",
      "Iteration 763, loss = 0.69287713\n",
      "Iteration 764, loss = 0.69284270\n",
      "Iteration 765, loss = 0.69280834\n",
      "Iteration 766, loss = 0.69277404\n",
      "Iteration 767, loss = 0.69273980\n",
      "Iteration 768, loss = 0.69270563\n",
      "Iteration 769, loss = 0.69267152\n",
      "Iteration 770, loss = 0.69263747\n",
      "Iteration 771, loss = 0.69260348\n",
      "Iteration 772, loss = 0.69256955\n",
      "Iteration 773, loss = 0.69253568\n",
      "Iteration 774, loss = 0.69250188\n",
      "Iteration 775, loss = 0.69246813\n",
      "Iteration 776, loss = 0.69243445\n",
      "Iteration 777, loss = 0.69240082\n",
      "Iteration 778, loss = 0.69236726\n",
      "Iteration 779, loss = 0.69233375\n",
      "Iteration 780, loss = 0.69230031\n",
      "Iteration 781, loss = 0.69226692\n",
      "Iteration 782, loss = 0.69223359\n",
      "Iteration 783, loss = 0.69220032\n",
      "Iteration 784, loss = 0.69216711\n",
      "Iteration 785, loss = 0.69213396\n",
      "Iteration 786, loss = 0.69210086\n",
      "Iteration 787, loss = 0.69206783\n",
      "Iteration 788, loss = 0.69203485\n",
      "Iteration 789, loss = 0.69200192\n",
      "Iteration 790, loss = 0.69196906\n",
      "Iteration 791, loss = 0.69193625\n",
      "Iteration 792, loss = 0.69190349\n",
      "Iteration 793, loss = 0.69187080\n",
      "Iteration 794, loss = 0.69183816\n",
      "Iteration 795, loss = 0.69180557\n",
      "Iteration 796, loss = 0.69177304\n",
      "Iteration 797, loss = 0.69174057\n",
      "Iteration 798, loss = 0.69170815\n",
      "Iteration 799, loss = 0.69167578\n",
      "Iteration 800, loss = 0.69164347\n",
      "Iteration 801, loss = 0.69161122\n",
      "Iteration 802, loss = 0.69157901\n",
      "Iteration 803, loss = 0.69154687\n",
      "Iteration 804, loss = 0.69151477\n",
      "Iteration 805, loss = 0.69148273\n",
      "Iteration 806, loss = 0.69145074\n",
      "Iteration 807, loss = 0.69141881\n",
      "Iteration 808, loss = 0.69138692\n",
      "Iteration 809, loss = 0.69135509\n",
      "Iteration 810, loss = 0.69132332\n",
      "Iteration 811, loss = 0.69129159\n",
      "Iteration 812, loss = 0.69125992\n",
      "Iteration 813, loss = 0.69122829\n",
      "Iteration 814, loss = 0.69119672\n",
      "Iteration 815, loss = 0.69116520\n",
      "Iteration 816, loss = 0.69113374\n",
      "Iteration 817, loss = 0.69110232\n",
      "Iteration 818, loss = 0.69107095\n",
      "Iteration 819, loss = 0.69103963\n",
      "Iteration 820, loss = 0.69100836\n",
      "Iteration 821, loss = 0.69097715\n",
      "Iteration 822, loss = 0.69094598\n",
      "Iteration 823, loss = 0.69091486\n",
      "Iteration 824, loss = 0.69088379\n",
      "Iteration 825, loss = 0.69085277\n",
      "Iteration 826, loss = 0.69082180\n",
      "Iteration 827, loss = 0.69079088\n",
      "Iteration 828, loss = 0.69076000\n",
      "Iteration 829, loss = 0.69072918\n",
      "Iteration 830, loss = 0.69069840\n",
      "Iteration 831, loss = 0.69066767\n",
      "Iteration 832, loss = 0.69063699\n",
      "Iteration 833, loss = 0.69060635\n",
      "Iteration 834, loss = 0.69057576\n",
      "Iteration 835, loss = 0.69054522\n",
      "Iteration 836, loss = 0.69051472\n",
      "Iteration 837, loss = 0.69048428\n",
      "Iteration 838, loss = 0.69045387\n",
      "Iteration 839, loss = 0.69042352\n",
      "Iteration 840, loss = 0.69039321\n",
      "Iteration 841, loss = 0.69036294\n",
      "Iteration 842, loss = 0.69033273\n",
      "Iteration 843, loss = 0.69030255\n",
      "Iteration 844, loss = 0.69027242\n",
      "Iteration 845, loss = 0.69024234\n",
      "Iteration 846, loss = 0.69021230\n",
      "Iteration 847, loss = 0.69018231\n",
      "Iteration 848, loss = 0.69015236\n",
      "Iteration 849, loss = 0.69012245\n",
      "Iteration 850, loss = 0.69009259\n",
      "Iteration 851, loss = 0.69006277\n",
      "Iteration 852, loss = 0.69003300\n",
      "Iteration 853, loss = 0.69000327\n",
      "Iteration 854, loss = 0.68997358\n",
      "Iteration 855, loss = 0.68994393\n",
      "Iteration 856, loss = 0.68991433\n",
      "Iteration 857, loss = 0.68988477\n",
      "Iteration 858, loss = 0.68985526\n",
      "Iteration 859, loss = 0.68982578\n",
      "Iteration 860, loss = 0.68979635\n",
      "Iteration 861, loss = 0.68976696\n",
      "Iteration 862, loss = 0.68973761\n",
      "Iteration 863, loss = 0.68970830\n",
      "Iteration 864, loss = 0.68967903\n",
      "Iteration 865, loss = 0.68964981\n",
      "Iteration 866, loss = 0.68962063\n",
      "Iteration 867, loss = 0.68959148\n",
      "Iteration 868, loss = 0.68956238\n",
      "Iteration 869, loss = 0.68953332\n",
      "Iteration 870, loss = 0.68950429\n",
      "Iteration 871, loss = 0.68947531\n",
      "Iteration 872, loss = 0.68944637\n",
      "Iteration 873, loss = 0.68941747\n",
      "Iteration 874, loss = 0.68938860\n",
      "Iteration 875, loss = 0.68935978\n",
      "Iteration 876, loss = 0.68933100\n",
      "Iteration 877, loss = 0.68930225\n",
      "Iteration 878, loss = 0.68927354\n",
      "Iteration 879, loss = 0.68924488\n",
      "Iteration 880, loss = 0.68921625\n",
      "Iteration 881, loss = 0.68918765\n",
      "Iteration 882, loss = 0.68915910\n",
      "Iteration 883, loss = 0.68913059\n",
      "Iteration 884, loss = 0.68910211\n",
      "Iteration 885, loss = 0.68907367\n",
      "Iteration 886, loss = 0.68904526\n",
      "Iteration 887, loss = 0.68901690\n",
      "Iteration 888, loss = 0.68898857\n",
      "Iteration 889, loss = 0.68896028\n",
      "Iteration 890, loss = 0.68893202\n",
      "Iteration 891, loss = 0.68890381\n",
      "Iteration 892, loss = 0.68887563\n",
      "Iteration 893, loss = 0.68884748\n",
      "Iteration 894, loss = 0.68881937\n",
      "Iteration 895, loss = 0.68879130\n",
      "Iteration 896, loss = 0.68876326\n",
      "Iteration 897, loss = 0.68873526\n",
      "Iteration 898, loss = 0.68870729\n",
      "Iteration 899, loss = 0.68867936\n",
      "Iteration 900, loss = 0.68865146\n",
      "Iteration 901, loss = 0.68862360\n",
      "Iteration 902, loss = 0.68859578\n",
      "Iteration 903, loss = 0.68856798\n",
      "Iteration 904, loss = 0.68854023\n",
      "Iteration 905, loss = 0.68851250\n",
      "Iteration 906, loss = 0.68848481\n",
      "Iteration 907, loss = 0.68845716\n",
      "Iteration 908, loss = 0.68842954\n",
      "Iteration 909, loss = 0.68840195\n",
      "Iteration 910, loss = 0.68837440\n",
      "Iteration 911, loss = 0.68834687\n",
      "Iteration 912, loss = 0.68831939\n",
      "Iteration 913, loss = 0.68829193\n",
      "Iteration 914, loss = 0.68826451\n",
      "Iteration 915, loss = 0.68823712\n",
      "Iteration 916, loss = 0.68820976\n",
      "Iteration 917, loss = 0.68818244\n",
      "Iteration 918, loss = 0.68815515\n",
      "Iteration 919, loss = 0.68812789\n",
      "Iteration 920, loss = 0.68810066\n",
      "Iteration 921, loss = 0.68807347\n",
      "Iteration 922, loss = 0.68804630\n",
      "Iteration 923, loss = 0.68801917\n",
      "Iteration 924, loss = 0.68799207\n",
      "Iteration 925, loss = 0.68796500\n",
      "Iteration 926, loss = 0.68793796\n",
      "Iteration 927, loss = 0.68791095\n",
      "Iteration 928, loss = 0.68788397\n",
      "Iteration 929, loss = 0.68785703\n",
      "Iteration 930, loss = 0.68783011\n",
      "Iteration 931, loss = 0.68780322\n",
      "Iteration 932, loss = 0.68777637\n",
      "Iteration 933, loss = 0.68774954\n",
      "Iteration 934, loss = 0.68772275\n",
      "Iteration 935, loss = 0.68769598\n",
      "Iteration 936, loss = 0.68766924\n",
      "Iteration 937, loss = 0.68764254\n",
      "Iteration 938, loss = 0.68761586\n",
      "Iteration 939, loss = 0.68758921\n",
      "Iteration 940, loss = 0.68756259\n",
      "Iteration 941, loss = 0.68753600\n",
      "Iteration 942, loss = 0.68750944\n",
      "Iteration 943, loss = 0.68748290\n",
      "Iteration 944, loss = 0.68745640\n",
      "Iteration 945, loss = 0.68742992\n",
      "Iteration 946, loss = 0.68740347\n",
      "Iteration 947, loss = 0.68737705\n",
      "Iteration 948, loss = 0.68735066\n",
      "Iteration 949, loss = 0.68732429\n",
      "Iteration 950, loss = 0.68729796\n",
      "Iteration 951, loss = 0.68727165\n",
      "Iteration 952, loss = 0.68724536\n",
      "Iteration 953, loss = 0.68721911\n",
      "Iteration 954, loss = 0.68719288\n",
      "Iteration 955, loss = 0.68716668\n",
      "Iteration 956, loss = 0.68714050\n",
      "Iteration 957, loss = 0.68711436\n",
      "Iteration 958, loss = 0.68708823\n",
      "Iteration 959, loss = 0.68706214\n",
      "Iteration 960, loss = 0.68703607\n",
      "Iteration 961, loss = 0.68701003\n",
      "Iteration 962, loss = 0.68698401\n",
      "Iteration 963, loss = 0.68695802\n",
      "Iteration 964, loss = 0.68693206\n",
      "Iteration 965, loss = 0.68690612\n",
      "Iteration 966, loss = 0.68688020\n",
      "Iteration 967, loss = 0.68685431\n",
      "Iteration 968, loss = 0.68682845\n",
      "Iteration 969, loss = 0.68680261\n",
      "Iteration 970, loss = 0.68677680\n",
      "Iteration 971, loss = 0.68675101\n",
      "Iteration 972, loss = 0.68672525\n",
      "Iteration 973, loss = 0.68669951\n",
      "Iteration 974, loss = 0.68667379\n",
      "Iteration 975, loss = 0.68664810\n",
      "Iteration 976, loss = 0.68662244\n",
      "Iteration 977, loss = 0.68659679\n",
      "Iteration 978, loss = 0.68657118\n",
      "Iteration 979, loss = 0.68654558\n",
      "Iteration 980, loss = 0.68652001\n",
      "Iteration 981, loss = 0.68649446\n",
      "Iteration 982, loss = 0.68646894\n",
      "Iteration 983, loss = 0.68644344\n",
      "Iteration 984, loss = 0.68641796\n",
      "Iteration 985, loss = 0.68639251\n",
      "Iteration 986, loss = 0.68636707\n",
      "Iteration 987, loss = 0.68634167\n",
      "Iteration 988, loss = 0.68631628\n",
      "Iteration 989, loss = 0.68629092\n",
      "Iteration 990, loss = 0.68626558\n",
      "Iteration 991, loss = 0.68624026\n",
      "Iteration 992, loss = 0.68621496\n",
      "Iteration 993, loss = 0.68618969\n",
      "Iteration 994, loss = 0.68616443\n",
      "Iteration 995, loss = 0.68613920\n",
      "Iteration 996, loss = 0.68611399\n",
      "Iteration 997, loss = 0.68608881\n",
      "Iteration 998, loss = 0.68606364\n",
      "Iteration 999, loss = 0.68603850\n",
      "Iteration 1000, loss = 0.68601337\n",
      "Iteration 1001, loss = 0.68598827\n",
      "Iteration 1002, loss = 0.68596319\n",
      "Iteration 1003, loss = 0.68593813\n",
      "Iteration 1004, loss = 0.68591309\n",
      "Iteration 1005, loss = 0.68588807\n",
      "Iteration 1006, loss = 0.68586308\n",
      "Iteration 1007, loss = 0.68583810\n",
      "Iteration 1008, loss = 0.68581314\n",
      "Iteration 1009, loss = 0.68578820\n",
      "Iteration 1010, loss = 0.68576329\n",
      "Iteration 1011, loss = 0.68573839\n",
      "Iteration 1012, loss = 0.68571351\n",
      "Iteration 1013, loss = 0.68568865\n",
      "Iteration 1014, loss = 0.68566382\n",
      "Iteration 1015, loss = 0.68563900\n",
      "Iteration 1016, loss = 0.68561420\n",
      "Iteration 1017, loss = 0.68558942\n",
      "Iteration 1018, loss = 0.68556466\n",
      "Iteration 1019, loss = 0.68553992\n",
      "Iteration 1020, loss = 0.68551519\n",
      "Iteration 1021, loss = 0.68549049\n",
      "Iteration 1022, loss = 0.68546581\n",
      "Iteration 1023, loss = 0.68544114\n",
      "Iteration 1024, loss = 0.68541649\n",
      "Iteration 1025, loss = 0.68539186\n",
      "Iteration 1026, loss = 0.68536725\n",
      "Iteration 1027, loss = 0.68534266\n",
      "Iteration 1028, loss = 0.68531808\n",
      "Iteration 1029, loss = 0.68529353\n",
      "Iteration 1030, loss = 0.68526899\n",
      "Iteration 1031, loss = 0.68524447\n",
      "Iteration 1032, loss = 0.68521996\n",
      "Iteration 1033, loss = 0.68519548\n",
      "Iteration 1034, loss = 0.68517101\n",
      "Iteration 1035, loss = 0.68514656\n",
      "Iteration 1036, loss = 0.68512212\n",
      "Iteration 1037, loss = 0.68509771\n",
      "Iteration 1038, loss = 0.68507331\n",
      "Iteration 1039, loss = 0.68504893\n",
      "Iteration 1040, loss = 0.68502456\n",
      "Iteration 1041, loss = 0.68500021\n",
      "Iteration 1042, loss = 0.68497588\n",
      "Iteration 1043, loss = 0.68495156\n",
      "Iteration 1044, loss = 0.68492726\n",
      "Iteration 1045, loss = 0.68490298\n",
      "Iteration 1046, loss = 0.68487871\n",
      "Iteration 1047, loss = 0.68485446\n",
      "Iteration 1048, loss = 0.68483022\n",
      "Iteration 1049, loss = 0.68480600\n",
      "Iteration 1050, loss = 0.68478180\n",
      "Iteration 1051, loss = 0.68475761\n",
      "Iteration 1052, loss = 0.68473344\n",
      "Iteration 1053, loss = 0.68470928\n",
      "Iteration 1054, loss = 0.68468514\n",
      "Iteration 1055, loss = 0.68466101\n",
      "Iteration 1056, loss = 0.68463690\n",
      "Iteration 1057, loss = 0.68461281\n",
      "Iteration 1058, loss = 0.68458873\n",
      "Iteration 1059, loss = 0.68456466\n",
      "Iteration 1060, loss = 0.68454061\n",
      "Iteration 1061, loss = 0.68451657\n",
      "Iteration 1062, loss = 0.68449255\n",
      "Iteration 1063, loss = 0.68446854\n",
      "Iteration 1064, loss = 0.68444455\n",
      "Iteration 1065, loss = 0.68442057\n",
      "Iteration 1066, loss = 0.68439660\n",
      "Iteration 1067, loss = 0.68437265\n",
      "Iteration 1068, loss = 0.68434872\n",
      "Iteration 1069, loss = 0.68432479\n",
      "Iteration 1070, loss = 0.68430088\n",
      "Iteration 1071, loss = 0.68427699\n",
      "Iteration 1072, loss = 0.68425311\n",
      "Iteration 1073, loss = 0.68422924\n",
      "Iteration 1074, loss = 0.68420538\n",
      "Iteration 1075, loss = 0.68418154\n",
      "Iteration 1076, loss = 0.68415771\n",
      "Iteration 1077, loss = 0.68413390\n",
      "Iteration 1078, loss = 0.68411010\n",
      "Iteration 1079, loss = 0.68408631\n",
      "Iteration 1080, loss = 0.68406253\n",
      "Iteration 1081, loss = 0.68403877\n",
      "Iteration 1082, loss = 0.68401501\n",
      "Iteration 1083, loss = 0.68399128\n",
      "Iteration 1084, loss = 0.68396755\n",
      "Iteration 1085, loss = 0.68394384\n",
      "Iteration 1086, loss = 0.68392013\n",
      "Iteration 1087, loss = 0.68389645\n",
      "Iteration 1088, loss = 0.68387277\n",
      "Iteration 1089, loss = 0.68384910\n",
      "Iteration 1090, loss = 0.68382545\n",
      "Iteration 1091, loss = 0.68380181\n",
      "Iteration 1092, loss = 0.68377818\n",
      "Iteration 1093, loss = 0.68375456\n",
      "Iteration 1094, loss = 0.68373096\n",
      "Iteration 1095, loss = 0.68370736\n",
      "Iteration 1096, loss = 0.68368378\n",
      "Iteration 1097, loss = 0.68366020\n",
      "Iteration 1098, loss = 0.68363664\n",
      "Iteration 1099, loss = 0.68361309\n",
      "Iteration 1100, loss = 0.68358955\n",
      "Iteration 1101, loss = 0.68356603\n",
      "Iteration 1102, loss = 0.68354251\n",
      "Iteration 1103, loss = 0.68351900\n",
      "Iteration 1104, loss = 0.68349551\n",
      "Iteration 1105, loss = 0.68347202\n",
      "Iteration 1106, loss = 0.68344855\n",
      "Iteration 1107, loss = 0.68342508\n",
      "Iteration 1108, loss = 0.68340163\n",
      "Iteration 1109, loss = 0.68337819\n",
      "Iteration 1110, loss = 0.68335475\n",
      "Iteration 1111, loss = 0.68333133\n",
      "Iteration 1112, loss = 0.68330792\n",
      "Iteration 1113, loss = 0.68328451\n",
      "Iteration 1114, loss = 0.68326112\n",
      "Iteration 1115, loss = 0.68323774\n",
      "Iteration 1116, loss = 0.68321436\n",
      "Iteration 1117, loss = 0.68319100\n",
      "Iteration 1118, loss = 0.68316764\n",
      "Iteration 1119, loss = 0.68314430\n",
      "Iteration 1120, loss = 0.68312096\n",
      "Iteration 1121, loss = 0.68309763\n",
      "Iteration 1122, loss = 0.68307431\n",
      "Iteration 1123, loss = 0.68305100\n",
      "Iteration 1124, loss = 0.68302770\n",
      "Iteration 1125, loss = 0.68300441\n",
      "Iteration 1126, loss = 0.68298113\n",
      "Iteration 1127, loss = 0.68295786\n",
      "Iteration 1128, loss = 0.68293459\n",
      "Iteration 1129, loss = 0.68291133\n",
      "Iteration 1130, loss = 0.68288809\n",
      "Iteration 1131, loss = 0.68286485\n",
      "Iteration 1132, loss = 0.68284161\n",
      "Iteration 1133, loss = 0.68281839\n",
      "Iteration 1134, loss = 0.68279518\n",
      "Iteration 1135, loss = 0.68277197\n",
      "Iteration 1136, loss = 0.68274877\n",
      "Iteration 1137, loss = 0.68272558\n",
      "Iteration 1138, loss = 0.68270239\n",
      "Iteration 1139, loss = 0.68267922\n",
      "Iteration 1140, loss = 0.68265605\n",
      "Iteration 1141, loss = 0.68263289\n",
      "Iteration 1142, loss = 0.68260974\n",
      "Iteration 1143, loss = 0.68258659\n",
      "Iteration 1144, loss = 0.68256345\n",
      "Iteration 1145, loss = 0.68254032\n",
      "Iteration 1146, loss = 0.68251720\n",
      "Iteration 1147, loss = 0.68249408\n",
      "Iteration 1148, loss = 0.68247097\n",
      "Iteration 1149, loss = 0.68244787\n",
      "Iteration 1150, loss = 0.68242477\n",
      "Iteration 1151, loss = 0.68240168\n",
      "Iteration 1152, loss = 0.68237860\n",
      "Iteration 1153, loss = 0.68235553\n",
      "Iteration 1154, loss = 0.68233246\n",
      "Iteration 1155, loss = 0.68230939\n",
      "Iteration 1156, loss = 0.68228634\n",
      "Iteration 1157, loss = 0.68226329\n",
      "Iteration 1158, loss = 0.68224024\n",
      "Iteration 1159, loss = 0.68221721\n",
      "Iteration 1160, loss = 0.68219418\n",
      "Iteration 1161, loss = 0.68217115\n",
      "Iteration 1162, loss = 0.68214813\n",
      "Iteration 1163, loss = 0.68212512\n",
      "Iteration 1164, loss = 0.68210211\n",
      "Iteration 1165, loss = 0.68207911\n",
      "Iteration 1166, loss = 0.68205611\n",
      "Iteration 1167, loss = 0.68203312\n",
      "Iteration 1168, loss = 0.68201013\n",
      "Iteration 1169, loss = 0.68198715\n",
      "Iteration 1170, loss = 0.68196418\n",
      "Iteration 1171, loss = 0.68194121\n",
      "Iteration 1172, loss = 0.68191825\n",
      "Iteration 1173, loss = 0.68189529\n",
      "Iteration 1174, loss = 0.68187233\n",
      "Iteration 1175, loss = 0.68184938\n",
      "Iteration 1176, loss = 0.68182644\n",
      "Iteration 1177, loss = 0.68180350\n",
      "Iteration 1178, loss = 0.68178057\n",
      "Iteration 1179, loss = 0.68175764\n",
      "Iteration 1180, loss = 0.68173471\n",
      "Iteration 1181, loss = 0.68171179\n",
      "Iteration 1182, loss = 0.68168887\n",
      "Iteration 1183, loss = 0.68166596\n",
      "Iteration 1184, loss = 0.68164305\n",
      "Iteration 1185, loss = 0.68162015\n",
      "Iteration 1186, loss = 0.68159725\n",
      "Iteration 1187, loss = 0.68157436\n",
      "Iteration 1188, loss = 0.68155146\n",
      "Iteration 1189, loss = 0.68152858\n",
      "Iteration 1190, loss = 0.68150569\n",
      "Iteration 1191, loss = 0.68148281\n",
      "Iteration 1192, loss = 0.68145994\n",
      "Iteration 1193, loss = 0.68143707\n",
      "Iteration 1194, loss = 0.68141420\n",
      "Iteration 1195, loss = 0.68139133\n",
      "Iteration 1196, loss = 0.68136847\n",
      "Iteration 1197, loss = 0.68134561\n",
      "Iteration 1198, loss = 0.68132276\n",
      "Iteration 1199, loss = 0.68129991\n",
      "Iteration 1200, loss = 0.68127706\n",
      "Iteration 1201, loss = 0.68125421\n",
      "Iteration 1202, loss = 0.68123137\n",
      "Iteration 1203, loss = 0.68120853\n",
      "Iteration 1204, loss = 0.68118570\n",
      "Iteration 1205, loss = 0.68116286\n",
      "Iteration 1206, loss = 0.68114003\n",
      "Iteration 1207, loss = 0.68111720\n",
      "Iteration 1208, loss = 0.68109438\n",
      "Iteration 1209, loss = 0.68107156\n",
      "Iteration 1210, loss = 0.68104874\n",
      "Iteration 1211, loss = 0.68102592\n",
      "Iteration 1212, loss = 0.68100310\n",
      "Iteration 1213, loss = 0.68098029\n",
      "Iteration 1214, loss = 0.68095748\n",
      "Iteration 1215, loss = 0.68093467\n",
      "Iteration 1216, loss = 0.68091186\n",
      "Iteration 1217, loss = 0.68088906\n",
      "Iteration 1218, loss = 0.68086625\n",
      "Iteration 1219, loss = 0.68084345\n",
      "Iteration 1220, loss = 0.68082065\n",
      "Iteration 1221, loss = 0.68079786\n",
      "Iteration 1222, loss = 0.68077506\n",
      "Iteration 1223, loss = 0.68075227\n",
      "Iteration 1224, loss = 0.68072947\n",
      "Iteration 1225, loss = 0.68070668\n",
      "Iteration 1226, loss = 0.68068389\n",
      "Iteration 1227, loss = 0.68066111\n",
      "Iteration 1228, loss = 0.68063832\n",
      "Iteration 1229, loss = 0.68061553\n",
      "Iteration 1230, loss = 0.68059275\n",
      "Iteration 1231, loss = 0.68056997\n",
      "Iteration 1232, loss = 0.68054718\n",
      "Iteration 1233, loss = 0.68052440\n",
      "Iteration 1234, loss = 0.68050162\n",
      "Iteration 1235, loss = 0.68047884\n",
      "Iteration 1236, loss = 0.68045606\n",
      "Iteration 1237, loss = 0.68043328\n",
      "Iteration 1238, loss = 0.68041051\n",
      "Iteration 1239, loss = 0.68038773\n",
      "Iteration 1240, loss = 0.68036495\n",
      "Iteration 1241, loss = 0.68034218\n",
      "Iteration 1242, loss = 0.68031940\n",
      "Iteration 1243, loss = 0.68029663\n",
      "Iteration 1244, loss = 0.68027385\n",
      "Iteration 1245, loss = 0.68025108\n",
      "Iteration 1246, loss = 0.68022830\n",
      "Iteration 1247, loss = 0.68020553\n",
      "Iteration 1248, loss = 0.68018275\n",
      "Iteration 1249, loss = 0.68015998\n",
      "Iteration 1250, loss = 0.68013720\n",
      "Iteration 1251, loss = 0.68011443\n",
      "Iteration 1252, loss = 0.68009165\n",
      "Iteration 1253, loss = 0.68006888\n",
      "Iteration 1254, loss = 0.68004610\n",
      "Iteration 1255, loss = 0.68002332\n",
      "Iteration 1256, loss = 0.68000055\n",
      "Iteration 1257, loss = 0.67997777\n",
      "Iteration 1258, loss = 0.67995499\n",
      "Iteration 1259, loss = 0.67993221\n",
      "Iteration 1260, loss = 0.67990943\n",
      "Iteration 1261, loss = 0.67988665\n",
      "Iteration 1262, loss = 0.67986387\n",
      "Iteration 1263, loss = 0.67984109\n",
      "Iteration 1264, loss = 0.67981831\n",
      "Iteration 1265, loss = 0.67979552\n",
      "Iteration 1266, loss = 0.67977274\n",
      "Iteration 1267, loss = 0.67974995\n",
      "Iteration 1268, loss = 0.67972716\n",
      "Iteration 1269, loss = 0.67970437\n",
      "Iteration 1270, loss = 0.67968158\n",
      "Iteration 1271, loss = 0.67965879\n",
      "Iteration 1272, loss = 0.67963600\n",
      "Iteration 1273, loss = 0.67961320\n",
      "Iteration 1274, loss = 0.67959041\n",
      "Iteration 1275, loss = 0.67956761\n",
      "Iteration 1276, loss = 0.67954481\n",
      "Iteration 1277, loss = 0.67952201\n",
      "Iteration 1278, loss = 0.67949920\n",
      "Iteration 1279, loss = 0.67947640\n",
      "Iteration 1280, loss = 0.67945359\n",
      "Iteration 1281, loss = 0.67943078\n",
      "Iteration 1282, loss = 0.67940797\n",
      "Iteration 1283, loss = 0.67938516\n",
      "Iteration 1284, loss = 0.67936234\n",
      "Iteration 1285, loss = 0.67933953\n",
      "Iteration 1286, loss = 0.67931671\n",
      "Iteration 1287, loss = 0.67929388\n",
      "Iteration 1288, loss = 0.67927106\n",
      "Iteration 1289, loss = 0.67924823\n",
      "Iteration 1290, loss = 0.67922540\n",
      "Iteration 1291, loss = 0.67920257\n",
      "Iteration 1292, loss = 0.67917973\n",
      "Iteration 1293, loss = 0.67915690\n",
      "Iteration 1294, loss = 0.67913406\n",
      "Iteration 1295, loss = 0.67911121\n",
      "Iteration 1296, loss = 0.67908837\n",
      "Iteration 1297, loss = 0.67906552\n",
      "Iteration 1298, loss = 0.67904267\n",
      "Iteration 1299, loss = 0.67901981\n",
      "Iteration 1300, loss = 0.67899695\n",
      "Iteration 1301, loss = 0.67897409\n",
      "Iteration 1302, loss = 0.67895123\n",
      "Iteration 1303, loss = 0.67892836\n",
      "Iteration 1304, loss = 0.67890549\n",
      "Iteration 1305, loss = 0.67888261\n",
      "Iteration 1306, loss = 0.67885973\n",
      "Iteration 1307, loss = 0.67883685\n",
      "Iteration 1308, loss = 0.67881397\n",
      "Iteration 1309, loss = 0.67879108\n",
      "Iteration 1310, loss = 0.67876819\n",
      "Iteration 1311, loss = 0.67874529\n",
      "Iteration 1312, loss = 0.67872239\n",
      "Iteration 1313, loss = 0.67869949\n",
      "Iteration 1314, loss = 0.67867658\n",
      "Iteration 1315, loss = 0.67865367\n",
      "Iteration 1316, loss = 0.67863075\n",
      "Iteration 1317, loss = 0.67860783\n",
      "Iteration 1318, loss = 0.67858491\n",
      "Iteration 1319, loss = 0.67856198\n",
      "Iteration 1320, loss = 0.67853905\n",
      "Iteration 1321, loss = 0.67851611\n",
      "Iteration 1322, loss = 0.67849317\n",
      "Iteration 1323, loss = 0.67847022\n",
      "Iteration 1324, loss = 0.67844727\n",
      "Iteration 1325, loss = 0.67842432\n",
      "Iteration 1326, loss = 0.67840136\n",
      "Iteration 1327, loss = 0.67837840\n",
      "Iteration 1328, loss = 0.67835543\n",
      "Iteration 1329, loss = 0.67833245\n",
      "Iteration 1330, loss = 0.67830948\n",
      "Iteration 1331, loss = 0.67828649\n",
      "Iteration 1332, loss = 0.67826351\n",
      "Iteration 1333, loss = 0.67824051\n",
      "Iteration 1334, loss = 0.67821752\n",
      "Iteration 1335, loss = 0.67819451\n",
      "Iteration 1336, loss = 0.67817151\n",
      "Iteration 1337, loss = 0.67814849\n",
      "Iteration 1338, loss = 0.67812548\n",
      "Iteration 1339, loss = 0.67810245\n",
      "Iteration 1340, loss = 0.67807942\n",
      "Iteration 1341, loss = 0.67805639\n",
      "Iteration 1342, loss = 0.67803335\n",
      "Iteration 1343, loss = 0.67801031\n",
      "Iteration 1344, loss = 0.67798726\n",
      "Iteration 1345, loss = 0.67796420\n",
      "Iteration 1346, loss = 0.67794114\n",
      "Iteration 1347, loss = 0.67791807\n",
      "Iteration 1348, loss = 0.67789500\n",
      "Iteration 1349, loss = 0.67787192\n",
      "Iteration 1350, loss = 0.67784883\n",
      "Iteration 1351, loss = 0.67782574\n",
      "Iteration 1352, loss = 0.67780265\n",
      "Iteration 1353, loss = 0.67777954\n",
      "Iteration 1354, loss = 0.67775644\n",
      "Iteration 1355, loss = 0.67773332\n",
      "Iteration 1356, loss = 0.67771020\n",
      "Iteration 1357, loss = 0.67768707\n",
      "Iteration 1358, loss = 0.67766394\n",
      "Iteration 1359, loss = 0.67764080\n",
      "Iteration 1360, loss = 0.67761765\n",
      "Iteration 1361, loss = 0.67759450\n",
      "Iteration 1362, loss = 0.67757134\n",
      "Iteration 1363, loss = 0.67754818\n",
      "Iteration 1364, loss = 0.67752500\n",
      "Iteration 1365, loss = 0.67750183\n",
      "Iteration 1366, loss = 0.67747864\n",
      "Iteration 1367, loss = 0.67745545\n",
      "Iteration 1368, loss = 0.67743225\n",
      "Iteration 1369, loss = 0.67740904\n",
      "Iteration 1370, loss = 0.67738583\n",
      "Iteration 1371, loss = 0.67736261\n",
      "Iteration 1372, loss = 0.67733938\n",
      "Iteration 1373, loss = 0.67731615\n",
      "Iteration 1374, loss = 0.67729291\n",
      "Iteration 1375, loss = 0.67726966\n",
      "Iteration 1376, loss = 0.67724641\n",
      "Iteration 1377, loss = 0.67722314\n",
      "Iteration 1378, loss = 0.67719987\n",
      "Iteration 1379, loss = 0.67717660\n",
      "Iteration 1380, loss = 0.67715331\n",
      "Iteration 1381, loss = 0.67713002\n",
      "Iteration 1382, loss = 0.67710672\n",
      "Iteration 1383, loss = 0.67708341\n",
      "Iteration 1384, loss = 0.67706010\n",
      "Iteration 1385, loss = 0.67703678\n",
      "Iteration 1386, loss = 0.67701345\n",
      "Iteration 1387, loss = 0.67699011\n",
      "Iteration 1388, loss = 0.67696676\n",
      "Iteration 1389, loss = 0.67694341\n",
      "Iteration 1390, loss = 0.67692005\n",
      "Iteration 1391, loss = 0.67689668\n",
      "Iteration 1392, loss = 0.67687330\n",
      "Iteration 1393, loss = 0.67684992\n",
      "Iteration 1394, loss = 0.67682653\n",
      "Iteration 1395, loss = 0.67680312\n",
      "Iteration 1396, loss = 0.67677971\n",
      "Iteration 1397, loss = 0.67675630\n",
      "Iteration 1398, loss = 0.67673287\n",
      "Iteration 1399, loss = 0.67670944\n",
      "Iteration 1400, loss = 0.67668599\n",
      "Iteration 1401, loss = 0.67666254\n",
      "Iteration 1402, loss = 0.67663908\n",
      "Iteration 1403, loss = 0.67661561\n",
      "Iteration 1404, loss = 0.67659213\n",
      "Iteration 1405, loss = 0.67656865\n",
      "Iteration 1406, loss = 0.67654515\n",
      "Iteration 1407, loss = 0.67652165\n",
      "Iteration 1408, loss = 0.67649814\n",
      "Iteration 1409, loss = 0.67647462\n",
      "Iteration 1410, loss = 0.67645109\n",
      "Iteration 1411, loss = 0.67642755\n",
      "Iteration 1412, loss = 0.67640400\n",
      "Iteration 1413, loss = 0.67638045\n",
      "Iteration 1414, loss = 0.67635688\n",
      "Iteration 1415, loss = 0.67633331\n",
      "Iteration 1416, loss = 0.67630972\n",
      "Iteration 1417, loss = 0.67628613\n",
      "Iteration 1418, loss = 0.67626253\n",
      "Iteration 1419, loss = 0.67623891\n",
      "Iteration 1420, loss = 0.67621529\n",
      "Iteration 1421, loss = 0.67619166\n",
      "Iteration 1422, loss = 0.67616802\n",
      "Iteration 1423, loss = 0.67614437\n",
      "Iteration 1424, loss = 0.67612071\n",
      "Iteration 1425, loss = 0.67609704\n",
      "Iteration 1426, loss = 0.67607336\n",
      "Iteration 1427, loss = 0.67604968\n",
      "Iteration 1428, loss = 0.67602598\n",
      "Iteration 1429, loss = 0.67600227\n",
      "Iteration 1430, loss = 0.67597855\n",
      "Iteration 1431, loss = 0.67595483\n",
      "Iteration 1432, loss = 0.67593109\n",
      "Iteration 1433, loss = 0.67590734\n",
      "Iteration 1434, loss = 0.67588358\n",
      "Iteration 1435, loss = 0.67585982\n",
      "Iteration 1436, loss = 0.67583604\n",
      "Iteration 1437, loss = 0.67581225\n",
      "Iteration 1438, loss = 0.67578845\n",
      "Iteration 1439, loss = 0.67576464\n",
      "Iteration 1440, loss = 0.67574082\n",
      "Iteration 1441, loss = 0.67571699\n",
      "Iteration 1442, loss = 0.67569316\n",
      "Iteration 1443, loss = 0.67566931\n",
      "Iteration 1444, loss = 0.67564544\n",
      "Iteration 1445, loss = 0.67562157\n",
      "Iteration 1446, loss = 0.67559769\n",
      "Iteration 1447, loss = 0.67557380\n",
      "Iteration 1448, loss = 0.67554990\n",
      "Iteration 1449, loss = 0.67552598\n",
      "Iteration 1450, loss = 0.67550206\n",
      "Iteration 1451, loss = 0.67547812\n",
      "Iteration 1452, loss = 0.67545418\n",
      "Iteration 1453, loss = 0.67543022\n",
      "Iteration 1454, loss = 0.67540625\n",
      "Iteration 1455, loss = 0.67538227\n",
      "Iteration 1456, loss = 0.67535828\n",
      "Iteration 1457, loss = 0.67533428\n",
      "Iteration 1458, loss = 0.67531027\n",
      "Iteration 1459, loss = 0.67528624\n",
      "Iteration 1460, loss = 0.67526221\n",
      "Iteration 1461, loss = 0.67523816\n",
      "Iteration 1462, loss = 0.67521410\n",
      "Iteration 1463, loss = 0.67519004\n",
      "Iteration 1464, loss = 0.67516595\n",
      "Iteration 1465, loss = 0.67514186\n",
      "Iteration 1466, loss = 0.67511776\n",
      "Iteration 1467, loss = 0.67509364\n",
      "Iteration 1468, loss = 0.67506952\n",
      "Iteration 1469, loss = 0.67504538\n",
      "Iteration 1470, loss = 0.67502123\n",
      "Iteration 1471, loss = 0.67499707\n",
      "Iteration 1472, loss = 0.67497289\n",
      "Iteration 1473, loss = 0.67494871\n",
      "Iteration 1474, loss = 0.67492451\n",
      "Iteration 1475, loss = 0.67490030\n",
      "Iteration 1476, loss = 0.67487608\n",
      "Iteration 1477, loss = 0.67485185\n",
      "Iteration 1478, loss = 0.67482760\n",
      "Iteration 1479, loss = 0.67480334\n",
      "Iteration 1480, loss = 0.67477907\n",
      "Iteration 1481, loss = 0.67475479\n",
      "Iteration 1482, loss = 0.67473050\n",
      "Iteration 1483, loss = 0.67470619\n",
      "Iteration 1484, loss = 0.67468187\n",
      "Iteration 1485, loss = 0.67465754\n",
      "Iteration 1486, loss = 0.67463320\n",
      "Iteration 1487, loss = 0.67460884\n",
      "Iteration 1488, loss = 0.67458447\n",
      "Iteration 1489, loss = 0.67456009\n",
      "Iteration 1490, loss = 0.67453570\n",
      "Iteration 1491, loss = 0.67451129\n",
      "Iteration 1492, loss = 0.67448687\n",
      "Iteration 1493, loss = 0.67446244\n",
      "Iteration 1494, loss = 0.67443799\n",
      "Iteration 1495, loss = 0.67441353\n",
      "Iteration 1496, loss = 0.67438906\n",
      "Iteration 1497, loss = 0.67436458\n",
      "Iteration 1498, loss = 0.67434008\n",
      "Iteration 1499, loss = 0.67431557\n",
      "Iteration 1500, loss = 0.67429105\n",
      "Iteration 1501, loss = 0.67426652\n",
      "Iteration 1502, loss = 0.67424197\n",
      "Iteration 1503, loss = 0.67421741\n",
      "Iteration 1504, loss = 0.67419283\n",
      "Iteration 1505, loss = 0.67416824\n",
      "Iteration 1506, loss = 0.67414364\n",
      "Iteration 1507, loss = 0.67411902\n",
      "Iteration 1508, loss = 0.67409440\n",
      "Iteration 1509, loss = 0.67406975\n",
      "Iteration 1510, loss = 0.67404510\n",
      "Iteration 1511, loss = 0.67402043\n",
      "Iteration 1512, loss = 0.67399575\n",
      "Iteration 1513, loss = 0.67397105\n",
      "Iteration 1514, loss = 0.67394634\n",
      "Iteration 1515, loss = 0.67392161\n",
      "Iteration 1516, loss = 0.67389688\n",
      "Iteration 1517, loss = 0.67387213\n",
      "Iteration 1518, loss = 0.67384736\n",
      "Iteration 1519, loss = 0.67382258\n",
      "Iteration 1520, loss = 0.67379779\n",
      "Iteration 1521, loss = 0.67377298\n",
      "Iteration 1522, loss = 0.67374816\n",
      "Iteration 1523, loss = 0.67372332\n",
      "Iteration 1524, loss = 0.67369847\n",
      "Iteration 1525, loss = 0.67367361\n",
      "Iteration 1526, loss = 0.67364873\n",
      "Iteration 1527, loss = 0.67362384\n",
      "Iteration 1528, loss = 0.67359893\n",
      "Iteration 1529, loss = 0.67357401\n",
      "Iteration 1530, loss = 0.67354908\n",
      "Iteration 1531, loss = 0.67352413\n",
      "Iteration 1532, loss = 0.67349917\n",
      "Iteration 1533, loss = 0.67347419\n",
      "Iteration 1534, loss = 0.67344919\n",
      "Iteration 1535, loss = 0.67342419\n",
      "Iteration 1536, loss = 0.67339916\n",
      "Iteration 1537, loss = 0.67337413\n",
      "Iteration 1538, loss = 0.67334908\n",
      "Iteration 1539, loss = 0.67332401\n",
      "Iteration 1540, loss = 0.67329893\n",
      "Iteration 1541, loss = 0.67327383\n",
      "Iteration 1542, loss = 0.67324872\n",
      "Iteration 1543, loss = 0.67322359\n",
      "Iteration 1544, loss = 0.67319845\n",
      "Iteration 1545, loss = 0.67317330\n",
      "Iteration 1546, loss = 0.67314813\n",
      "Iteration 1547, loss = 0.67312294\n",
      "Iteration 1548, loss = 0.67309774\n",
      "Iteration 1549, loss = 0.67307252\n",
      "Iteration 1550, loss = 0.67304729\n",
      "Iteration 1551, loss = 0.67302204\n",
      "Iteration 1552, loss = 0.67299678\n",
      "Iteration 1553, loss = 0.67297150\n",
      "Iteration 1554, loss = 0.67294621\n",
      "Iteration 1555, loss = 0.67292090\n",
      "Iteration 1556, loss = 0.67289557\n",
      "Iteration 1557, loss = 0.67287024\n",
      "Iteration 1558, loss = 0.67284488\n",
      "Iteration 1559, loss = 0.67281951\n",
      "Iteration 1560, loss = 0.67279412\n",
      "Iteration 1561, loss = 0.67276872\n",
      "Iteration 1562, loss = 0.67274330\n",
      "Iteration 1563, loss = 0.67271787\n",
      "Iteration 1564, loss = 0.67269242\n",
      "Iteration 1565, loss = 0.67266695\n",
      "Iteration 1566, loss = 0.67264147\n",
      "Iteration 1567, loss = 0.67261597\n",
      "Iteration 1568, loss = 0.67259046\n",
      "Iteration 1569, loss = 0.67256493\n",
      "Iteration 1570, loss = 0.67253938\n",
      "Iteration 1571, loss = 0.67251382\n",
      "Iteration 1572, loss = 0.67248824\n",
      "Iteration 1573, loss = 0.67246265\n",
      "Iteration 1574, loss = 0.67243704\n",
      "Iteration 1575, loss = 0.67241141\n",
      "Iteration 1576, loss = 0.67238577\n",
      "Iteration 1577, loss = 0.67236011\n",
      "Iteration 1578, loss = 0.67233443\n",
      "Iteration 1579, loss = 0.67230874\n",
      "Iteration 1580, loss = 0.67228303\n",
      "Iteration 1581, loss = 0.67225730\n",
      "Iteration 1582, loss = 0.67223156\n",
      "Iteration 1583, loss = 0.67220580\n",
      "Iteration 1584, loss = 0.67218002\n",
      "Iteration 1585, loss = 0.67215423\n",
      "Iteration 1586, loss = 0.67212842\n",
      "Iteration 1587, loss = 0.67210260\n",
      "Iteration 1588, loss = 0.67207675\n",
      "Iteration 1589, loss = 0.67205089\n",
      "Iteration 1590, loss = 0.67202502\n",
      "Iteration 1591, loss = 0.67199912\n",
      "Iteration 1592, loss = 0.67197321\n",
      "Iteration 1593, loss = 0.67194728\n",
      "Iteration 1594, loss = 0.67192134\n",
      "Iteration 1595, loss = 0.67189538\n",
      "Iteration 1596, loss = 0.67186940\n",
      "Iteration 1597, loss = 0.67184340\n",
      "Iteration 1598, loss = 0.67181739\n",
      "Iteration 1599, loss = 0.67179135\n",
      "Iteration 1600, loss = 0.67176531\n",
      "Iteration 1601, loss = 0.67173924\n",
      "Iteration 1602, loss = 0.67171316\n",
      "Iteration 1603, loss = 0.67168705\n",
      "Iteration 1604, loss = 0.67166094\n",
      "Iteration 1605, loss = 0.67163480\n",
      "Iteration 1606, loss = 0.67160865\n",
      "Iteration 1607, loss = 0.67158247\n",
      "Iteration 1608, loss = 0.67155628\n",
      "Iteration 1609, loss = 0.67153008\n",
      "Iteration 1610, loss = 0.67150385\n",
      "Iteration 1611, loss = 0.67147761\n",
      "Iteration 1612, loss = 0.67145135\n",
      "Iteration 1613, loss = 0.67142507\n",
      "Iteration 1614, loss = 0.67139878\n",
      "Iteration 1615, loss = 0.67137246\n",
      "Iteration 1616, loss = 0.67134613\n",
      "Iteration 1617, loss = 0.67131978\n",
      "Iteration 1618, loss = 0.67129341\n",
      "Iteration 1619, loss = 0.67126702\n",
      "Iteration 1620, loss = 0.67124062\n",
      "Iteration 1621, loss = 0.67121419\n",
      "Iteration 1622, loss = 0.67118775\n",
      "Iteration 1623, loss = 0.67116129\n",
      "Iteration 1624, loss = 0.67113481\n",
      "Iteration 1625, loss = 0.67110832\n",
      "Iteration 1626, loss = 0.67108180\n",
      "Iteration 1627, loss = 0.67105527\n",
      "Iteration 1628, loss = 0.67102872\n",
      "Iteration 1629, loss = 0.67100215\n",
      "Iteration 1630, loss = 0.67097556\n",
      "Iteration 1631, loss = 0.67094895\n",
      "Iteration 1632, loss = 0.67092232\n",
      "Iteration 1633, loss = 0.67089568\n",
      "Iteration 1634, loss = 0.67086901\n",
      "Iteration 1635, loss = 0.67084233\n",
      "Iteration 1636, loss = 0.67081563\n",
      "Iteration 1637, loss = 0.67078891\n",
      "Iteration 1638, loss = 0.67076217\n",
      "Iteration 1639, loss = 0.67073541\n",
      "Iteration 1640, loss = 0.67070863\n",
      "Iteration 1641, loss = 0.67068183\n",
      "Iteration 1642, loss = 0.67065502\n",
      "Iteration 1643, loss = 0.67062818\n",
      "Iteration 1644, loss = 0.67060133\n",
      "Iteration 1645, loss = 0.67057445\n",
      "Iteration 1646, loss = 0.67054756\n",
      "Iteration 1647, loss = 0.67052065\n",
      "Iteration 1648, loss = 0.67049372\n",
      "Iteration 1649, loss = 0.67046677\n",
      "Iteration 1650, loss = 0.67043980\n",
      "Iteration 1651, loss = 0.67041281\n",
      "Iteration 1652, loss = 0.67038580\n",
      "Iteration 1653, loss = 0.67035877\n",
      "Iteration 1654, loss = 0.67033172\n",
      "Iteration 1655, loss = 0.67030465\n",
      "Iteration 1656, loss = 0.67027756\n",
      "Iteration 1657, loss = 0.67025045\n",
      "Iteration 1658, loss = 0.67022333\n",
      "Iteration 1659, loss = 0.67019618\n",
      "Iteration 1660, loss = 0.67016901\n",
      "Iteration 1661, loss = 0.67014182\n",
      "Iteration 1662, loss = 0.67011462\n",
      "Iteration 1663, loss = 0.67008739\n",
      "Iteration 1664, loss = 0.67006014\n",
      "Iteration 1665, loss = 0.67003288\n",
      "Iteration 1666, loss = 0.67000559\n",
      "Iteration 1667, loss = 0.66997828\n",
      "Iteration 1668, loss = 0.66995095\n",
      "Iteration 1669, loss = 0.66992361\n",
      "Iteration 1670, loss = 0.66989624\n",
      "Iteration 1671, loss = 0.66986885\n",
      "Iteration 1672, loss = 0.66984144\n",
      "Iteration 1673, loss = 0.66981401\n",
      "Iteration 1674, loss = 0.66978656\n",
      "Iteration 1675, loss = 0.66975909\n",
      "Iteration 1676, loss = 0.66973160\n",
      "Iteration 1677, loss = 0.66970409\n",
      "Iteration 1678, loss = 0.66967656\n",
      "Iteration 1679, loss = 0.66964900\n",
      "Iteration 1680, loss = 0.66962143\n",
      "Iteration 1681, loss = 0.66959384\n",
      "Iteration 1682, loss = 0.66956622\n",
      "Iteration 1683, loss = 0.66953859\n",
      "Iteration 1684, loss = 0.66951093\n",
      "Iteration 1685, loss = 0.66948325\n",
      "Iteration 1686, loss = 0.66945555\n",
      "Iteration 1687, loss = 0.66942784\n",
      "Iteration 1688, loss = 0.66940010\n",
      "Iteration 1689, loss = 0.66937233\n",
      "Iteration 1690, loss = 0.66934455\n",
      "Iteration 1691, loss = 0.66931675\n",
      "Iteration 1692, loss = 0.66928892\n",
      "Iteration 1693, loss = 0.66926108\n",
      "Iteration 1694, loss = 0.66923321\n",
      "Iteration 1695, loss = 0.66920532\n",
      "Iteration 1696, loss = 0.66917741\n",
      "Iteration 1697, loss = 0.66914948\n",
      "Iteration 1698, loss = 0.66912153\n",
      "Iteration 1699, loss = 0.66909355\n",
      "Iteration 1700, loss = 0.66906556\n",
      "Iteration 1701, loss = 0.66903754\n",
      "Iteration 1702, loss = 0.66900950\n",
      "Iteration 1703, loss = 0.66898144\n",
      "Iteration 1704, loss = 0.66895336\n",
      "Iteration 1705, loss = 0.66892526\n",
      "Iteration 1706, loss = 0.66889713\n",
      "Iteration 1707, loss = 0.66886898\n",
      "Iteration 1708, loss = 0.66884082\n",
      "Iteration 1709, loss = 0.66881262\n",
      "Iteration 1710, loss = 0.66878441\n",
      "Iteration 1711, loss = 0.66875618\n",
      "Iteration 1712, loss = 0.66872792\n",
      "Iteration 1713, loss = 0.66869964\n",
      "Iteration 1714, loss = 0.66867134\n",
      "Iteration 1715, loss = 0.66864302\n",
      "Iteration 1716, loss = 0.66861467\n",
      "Iteration 1717, loss = 0.66858630\n",
      "Iteration 1718, loss = 0.66855791\n",
      "Iteration 1719, loss = 0.66852950\n",
      "Iteration 1720, loss = 0.66850107\n",
      "Iteration 1721, loss = 0.66847261\n",
      "Iteration 1722, loss = 0.66844413\n",
      "Iteration 1723, loss = 0.66841563\n",
      "Iteration 1724, loss = 0.66838711\n",
      "Iteration 1725, loss = 0.66835856\n",
      "Iteration 1726, loss = 0.66832999\n",
      "Iteration 1727, loss = 0.66830140\n",
      "Iteration 1728, loss = 0.66827278\n",
      "Iteration 1729, loss = 0.66824415\n",
      "Iteration 1730, loss = 0.66821549\n",
      "Iteration 1731, loss = 0.66818681\n",
      "Iteration 1732, loss = 0.66815810\n",
      "Iteration 1733, loss = 0.66812937\n",
      "Iteration 1734, loss = 0.66810062\n",
      "Iteration 1735, loss = 0.66807185\n",
      "Iteration 1736, loss = 0.66804305\n",
      "Iteration 1737, loss = 0.66801423\n",
      "Iteration 1738, loss = 0.66798539\n",
      "Iteration 1739, loss = 0.66795652\n",
      "Iteration 1740, loss = 0.66792763\n",
      "Iteration 1741, loss = 0.66789872\n",
      "Iteration 1742, loss = 0.66786978\n",
      "Iteration 1743, loss = 0.66784082\n",
      "Iteration 1744, loss = 0.66781184\n",
      "Iteration 1745, loss = 0.66778283\n",
      "Iteration 1746, loss = 0.66775381\n",
      "Iteration 1747, loss = 0.66772475\n",
      "Iteration 1748, loss = 0.66769568\n",
      "Iteration 1749, loss = 0.66766658\n",
      "Iteration 1750, loss = 0.66763745\n",
      "Iteration 1751, loss = 0.66760831\n",
      "Iteration 1752, loss = 0.66757914\n",
      "Iteration 1753, loss = 0.66754994\n",
      "Iteration 1754, loss = 0.66752072\n",
      "Iteration 1755, loss = 0.66749148\n",
      "Iteration 1756, loss = 0.66746222\n",
      "Iteration 1757, loss = 0.66743293\n",
      "Iteration 1758, loss = 0.66740362\n",
      "Iteration 1759, loss = 0.66737428\n",
      "Iteration 1760, loss = 0.66734492\n",
      "Iteration 1761, loss = 0.66731553\n",
      "Iteration 1762, loss = 0.66728612\n",
      "Iteration 1763, loss = 0.66725669\n",
      "Iteration 1764, loss = 0.66722723\n",
      "Iteration 1765, loss = 0.66719775\n",
      "Iteration 1766, loss = 0.66716825\n",
      "Iteration 1767, loss = 0.66713872\n",
      "Iteration 1768, loss = 0.66710916\n",
      "Iteration 1769, loss = 0.66707958\n",
      "Iteration 1770, loss = 0.66704998\n",
      "Iteration 1771, loss = 0.66702035\n",
      "Iteration 1772, loss = 0.66699070\n",
      "Iteration 1773, loss = 0.66696103\n",
      "Iteration 1774, loss = 0.66693132\n",
      "Iteration 1775, loss = 0.66690160\n",
      "Iteration 1776, loss = 0.66687185\n",
      "Iteration 1777, loss = 0.66684207\n",
      "Iteration 1778, loss = 0.66681228\n",
      "Iteration 1779, loss = 0.66678245\n",
      "Iteration 1780, loss = 0.66675260\n",
      "Iteration 1781, loss = 0.66672273\n",
      "Iteration 1782, loss = 0.66669283\n",
      "Iteration 1783, loss = 0.66666291\n",
      "Iteration 1784, loss = 0.66663296\n",
      "Iteration 1785, loss = 0.66660299\n",
      "Iteration 1786, loss = 0.66657299\n",
      "Iteration 1787, loss = 0.66654296\n",
      "Iteration 1788, loss = 0.66651291\n",
      "Iteration 1789, loss = 0.66648284\n",
      "Iteration 1790, loss = 0.66645274\n",
      "Iteration 1791, loss = 0.66642262\n",
      "Iteration 1792, loss = 0.66639247\n",
      "Iteration 1793, loss = 0.66636229\n",
      "Iteration 1794, loss = 0.66633209\n",
      "Iteration 1795, loss = 0.66630187\n",
      "Iteration 1796, loss = 0.66627162\n",
      "Iteration 1797, loss = 0.66624134\n",
      "Iteration 1798, loss = 0.66621104\n",
      "Iteration 1799, loss = 0.66618071\n",
      "Iteration 1800, loss = 0.66615036\n",
      "Iteration 1801, loss = 0.66611998\n",
      "Iteration 1802, loss = 0.66608957\n",
      "Iteration 1803, loss = 0.66605914\n",
      "Iteration 1804, loss = 0.66602869\n",
      "Iteration 1805, loss = 0.66599821\n",
      "Iteration 1806, loss = 0.66596770\n",
      "Iteration 1807, loss = 0.66593716\n",
      "Iteration 1808, loss = 0.66590660\n",
      "Iteration 1809, loss = 0.66587602\n",
      "Iteration 1810, loss = 0.66584541\n",
      "Iteration 1811, loss = 0.66581477\n",
      "Iteration 1812, loss = 0.66578411\n",
      "Iteration 1813, loss = 0.66575342\n",
      "Iteration 1814, loss = 0.66572270\n",
      "Iteration 1815, loss = 0.66569196\n",
      "Iteration 1816, loss = 0.66566119\n",
      "Iteration 1817, loss = 0.66563039\n",
      "Iteration 1818, loss = 0.66559957\n",
      "Iteration 1819, loss = 0.66556873\n",
      "Iteration 1820, loss = 0.66553785\n",
      "Iteration 1821, loss = 0.66550695\n",
      "Iteration 1822, loss = 0.66547602\n",
      "Iteration 1823, loss = 0.66544507\n",
      "Iteration 1824, loss = 0.66541409\n",
      "Iteration 1825, loss = 0.66538308\n",
      "Iteration 1826, loss = 0.66535205\n",
      "Iteration 1827, loss = 0.66532099\n",
      "Iteration 1828, loss = 0.66528990\n",
      "Iteration 1829, loss = 0.66525879\n",
      "Iteration 1830, loss = 0.66522765\n",
      "Iteration 1831, loss = 0.66519648\n",
      "Iteration 1832, loss = 0.66516529\n",
      "Iteration 1833, loss = 0.66513407\n",
      "Iteration 1834, loss = 0.66510282\n",
      "Iteration 1835, loss = 0.66507154\n",
      "Iteration 1836, loss = 0.66504024\n",
      "Iteration 1837, loss = 0.66500891\n",
      "Iteration 1838, loss = 0.66497755\n",
      "Iteration 1839, loss = 0.66494617\n",
      "Iteration 1840, loss = 0.66491476\n",
      "Iteration 1841, loss = 0.66488332\n",
      "Iteration 1842, loss = 0.66485185\n",
      "Iteration 1843, loss = 0.66482036\n",
      "Iteration 1844, loss = 0.66478884\n",
      "Iteration 1845, loss = 0.66475729\n",
      "Iteration 1846, loss = 0.66472571\n",
      "Iteration 1847, loss = 0.66469411\n",
      "Iteration 1848, loss = 0.66466248\n",
      "Iteration 1849, loss = 0.66463082\n",
      "Iteration 1850, loss = 0.66459913\n",
      "Iteration 1851, loss = 0.66456742\n",
      "Iteration 1852, loss = 0.66453568\n",
      "Iteration 1853, loss = 0.66450391\n",
      "Iteration 1854, loss = 0.66447211\n",
      "Iteration 1855, loss = 0.66444029\n",
      "Iteration 1856, loss = 0.66440843\n",
      "Iteration 1857, loss = 0.66437655\n",
      "Iteration 1858, loss = 0.66434464\n",
      "Iteration 1859, loss = 0.66431271\n",
      "Iteration 1860, loss = 0.66428074\n",
      "Iteration 1861, loss = 0.66424875\n",
      "Iteration 1862, loss = 0.66421673\n",
      "Iteration 1863, loss = 0.66418468\n",
      "Iteration 1864, loss = 0.66415260\n",
      "Iteration 1865, loss = 0.66412049\n",
      "Iteration 1866, loss = 0.66408836\n",
      "Iteration 1867, loss = 0.66405619\n",
      "Iteration 1868, loss = 0.66402400\n",
      "Iteration 1869, loss = 0.66399178\n",
      "Iteration 1870, loss = 0.66395953\n",
      "Iteration 1871, loss = 0.66392726\n",
      "Iteration 1872, loss = 0.66389495\n",
      "Iteration 1873, loss = 0.66386262\n",
      "Iteration 1874, loss = 0.66383025\n",
      "Iteration 1875, loss = 0.66379786\n",
      "Iteration 1876, loss = 0.66376544\n",
      "Iteration 1877, loss = 0.66373299\n",
      "Iteration 1878, loss = 0.66370051\n",
      "Iteration 1879, loss = 0.66366801\n",
      "Iteration 1880, loss = 0.66363547\n",
      "Iteration 1881, loss = 0.66360290\n",
      "Iteration 1882, loss = 0.66357031\n",
      "Iteration 1883, loss = 0.66353769\n",
      "Iteration 1884, loss = 0.66350504\n",
      "Iteration 1885, loss = 0.66347235\n",
      "Iteration 1886, loss = 0.66343964\n",
      "Iteration 1887, loss = 0.66340690\n",
      "Iteration 1888, loss = 0.66337413\n",
      "Iteration 1889, loss = 0.66334134\n",
      "Iteration 1890, loss = 0.66330851\n",
      "Iteration 1891, loss = 0.66327565\n",
      "Iteration 1892, loss = 0.66324277\n",
      "Iteration 1893, loss = 0.66320985\n",
      "Iteration 1894, loss = 0.66317690\n",
      "Iteration 1895, loss = 0.66314393\n",
      "Iteration 1896, loss = 0.66311092\n",
      "Iteration 1897, loss = 0.66307789\n",
      "Iteration 1898, loss = 0.66304483\n",
      "Iteration 1899, loss = 0.66301173\n",
      "Iteration 1900, loss = 0.66297861\n",
      "Iteration 1901, loss = 0.66294546\n",
      "Iteration 1902, loss = 0.66291227\n",
      "Iteration 1903, loss = 0.66287906\n",
      "Iteration 1904, loss = 0.66284582\n",
      "Iteration 1905, loss = 0.66281254\n",
      "Iteration 1906, loss = 0.66277924\n",
      "Iteration 1907, loss = 0.66274591\n",
      "Iteration 1908, loss = 0.66271254\n",
      "Iteration 1909, loss = 0.66267915\n",
      "Iteration 1910, loss = 0.66264573\n",
      "Iteration 1911, loss = 0.66261227\n",
      "Iteration 1912, loss = 0.66257879\n",
      "Iteration 1913, loss = 0.66254528\n",
      "Iteration 1914, loss = 0.66251173\n",
      "Iteration 1915, loss = 0.66247816\n",
      "Iteration 1916, loss = 0.66244455\n",
      "Iteration 1917, loss = 0.66241091\n",
      "Iteration 1918, loss = 0.66237725\n",
      "Iteration 1919, loss = 0.66234355\n",
      "Iteration 1920, loss = 0.66230982\n",
      "Iteration 1921, loss = 0.66227606\n",
      "Iteration 1922, loss = 0.66224227\n",
      "Iteration 1923, loss = 0.66220845\n",
      "Iteration 1924, loss = 0.66217460\n",
      "Iteration 1925, loss = 0.66214072\n",
      "Iteration 1926, loss = 0.66210681\n",
      "Iteration 1927, loss = 0.66207287\n",
      "Iteration 1928, loss = 0.66203889\n",
      "Iteration 1929, loss = 0.66200489\n",
      "Iteration 1930, loss = 0.66197085\n",
      "Iteration 1931, loss = 0.66193678\n",
      "Iteration 1932, loss = 0.66190268\n",
      "Iteration 1933, loss = 0.66186855\n",
      "Iteration 1934, loss = 0.66183439\n",
      "Iteration 1935, loss = 0.66180020\n",
      "Iteration 1936, loss = 0.66176598\n",
      "Iteration 1937, loss = 0.66173172\n",
      "Iteration 1938, loss = 0.66169744\n",
      "Iteration 1939, loss = 0.66166312\n",
      "Iteration 1940, loss = 0.66162877\n",
      "Iteration 1941, loss = 0.66159439\n",
      "Iteration 1942, loss = 0.66155998\n",
      "Iteration 1943, loss = 0.66152553\n",
      "Iteration 1944, loss = 0.66149106\n",
      "Iteration 1945, loss = 0.66145655\n",
      "Iteration 1946, loss = 0.66142201\n",
      "Iteration 1947, loss = 0.66138744\n",
      "Iteration 1948, loss = 0.66135284\n",
      "Iteration 1949, loss = 0.66131820\n",
      "Iteration 1950, loss = 0.66128354\n",
      "Iteration 1951, loss = 0.66124884\n",
      "Iteration 1952, loss = 0.66121411\n",
      "Iteration 1953, loss = 0.66117935\n",
      "Iteration 1954, loss = 0.66114455\n",
      "Iteration 1955, loss = 0.66110973\n",
      "Iteration 1956, loss = 0.66107487\n",
      "Iteration 1957, loss = 0.66103998\n",
      "Iteration 1958, loss = 0.66100505\n",
      "Iteration 1959, loss = 0.66097010\n",
      "Iteration 1960, loss = 0.66093511\n",
      "Iteration 1961, loss = 0.66090009\n",
      "Iteration 1962, loss = 0.66086504\n",
      "Iteration 1963, loss = 0.66082995\n",
      "Iteration 1964, loss = 0.66079484\n",
      "Iteration 1965, loss = 0.66075969\n",
      "Iteration 1966, loss = 0.66072450\n",
      "Iteration 1967, loss = 0.66068929\n",
      "Iteration 1968, loss = 0.66065404\n",
      "Iteration 1969, loss = 0.66061876\n",
      "Iteration 1970, loss = 0.66058345\n",
      "Iteration 1971, loss = 0.66054810\n",
      "Iteration 1972, loss = 0.66051272\n",
      "Iteration 1973, loss = 0.66047731\n",
      "Iteration 1974, loss = 0.66044186\n",
      "Iteration 1975, loss = 0.66040639\n",
      "Iteration 1976, loss = 0.66037088\n",
      "Iteration 1977, loss = 0.66033533\n",
      "Iteration 1978, loss = 0.66029975\n",
      "Iteration 1979, loss = 0.66026414\n",
      "Iteration 1980, loss = 0.66022850\n",
      "Iteration 1981, loss = 0.66019282\n",
      "Iteration 1982, loss = 0.66015711\n",
      "Iteration 1983, loss = 0.66012137\n",
      "Iteration 1984, loss = 0.66008559\n",
      "Iteration 1985, loss = 0.66004978\n",
      "Iteration 1986, loss = 0.66001394\n",
      "Iteration 1987, loss = 0.65997806\n",
      "Iteration 1988, loss = 0.65994215\n",
      "Iteration 1989, loss = 0.65990621\n",
      "Iteration 1990, loss = 0.65987023\n",
      "Iteration 1991, loss = 0.65983422\n",
      "Iteration 1992, loss = 0.65979817\n",
      "Iteration 1993, loss = 0.65976210\n",
      "Iteration 1994, loss = 0.65972598\n",
      "Iteration 1995, loss = 0.65968984\n",
      "Iteration 1996, loss = 0.65965366\n",
      "Iteration 1997, loss = 0.65961744\n",
      "Iteration 1998, loss = 0.65958119\n",
      "Iteration 1999, loss = 0.65954491\n",
      "Iteration 2000, loss = 0.65950859\n",
      "Iteration 2001, loss = 0.65947224\n",
      "Iteration 2002, loss = 0.65943586\n",
      "Iteration 2003, loss = 0.65939944\n",
      "Iteration 2004, loss = 0.65936299\n",
      "Iteration 2005, loss = 0.65932650\n",
      "Iteration 2006, loss = 0.65928998\n",
      "Iteration 2007, loss = 0.65925342\n",
      "Iteration 2008, loss = 0.65921683\n",
      "Iteration 2009, loss = 0.65918020\n",
      "Iteration 2010, loss = 0.65914354\n",
      "Iteration 2011, loss = 0.65910685\n",
      "Iteration 2012, loss = 0.65907012\n",
      "Iteration 2013, loss = 0.65903336\n",
      "Iteration 2014, loss = 0.65899656\n",
      "Iteration 2015, loss = 0.65895973\n",
      "Iteration 2016, loss = 0.65892286\n",
      "Iteration 2017, loss = 0.65888596\n",
      "Iteration 2018, loss = 0.65884902\n",
      "Iteration 2019, loss = 0.65881205\n",
      "Iteration 2020, loss = 0.65877504\n",
      "Iteration 2021, loss = 0.65873800\n",
      "Iteration 2022, loss = 0.65870092\n",
      "Iteration 2023, loss = 0.65866381\n",
      "Iteration 2024, loss = 0.65862666\n",
      "Iteration 2025, loss = 0.65858948\n",
      "Iteration 2026, loss = 0.65855226\n",
      "Iteration 2027, loss = 0.65851500\n",
      "Iteration 2028, loss = 0.65847772\n",
      "Iteration 2029, loss = 0.65844039\n",
      "Iteration 2030, loss = 0.65840303\n",
      "Iteration 2031, loss = 0.65836564\n",
      "Iteration 2032, loss = 0.65832821\n",
      "Iteration 2033, loss = 0.65829074\n",
      "Iteration 2034, loss = 0.65825324\n",
      "Iteration 2035, loss = 0.65821570\n",
      "Iteration 2036, loss = 0.65817813\n",
      "Iteration 2037, loss = 0.65814052\n",
      "Iteration 2038, loss = 0.65810288\n",
      "Iteration 2039, loss = 0.65806520\n",
      "Iteration 2040, loss = 0.65802748\n",
      "Iteration 2041, loss = 0.65798973\n",
      "Iteration 2042, loss = 0.65795194\n",
      "Iteration 2043, loss = 0.65791412\n",
      "Iteration 2044, loss = 0.65787626\n",
      "Iteration 2045, loss = 0.65783836\n",
      "Iteration 2046, loss = 0.65780043\n",
      "Iteration 2047, loss = 0.65776247\n",
      "Iteration 2048, loss = 0.65772446\n",
      "Iteration 2049, loss = 0.65768642\n",
      "Iteration 2050, loss = 0.65764834\n",
      "Iteration 2051, loss = 0.65761023\n",
      "Iteration 2052, loss = 0.65757208\n",
      "Iteration 2053, loss = 0.65753390\n",
      "Iteration 2054, loss = 0.65749567\n",
      "Iteration 2055, loss = 0.65745741\n",
      "Iteration 2056, loss = 0.65741912\n",
      "Iteration 2057, loss = 0.65738079\n",
      "Iteration 2058, loss = 0.65734242\n",
      "Iteration 2059, loss = 0.65730401\n",
      "Iteration 2060, loss = 0.65726557\n",
      "Iteration 2061, loss = 0.65722709\n",
      "Iteration 2062, loss = 0.65718858\n",
      "Iteration 2063, loss = 0.65715002\n",
      "Iteration 2064, loss = 0.65711144\n",
      "Iteration 2065, loss = 0.65707281\n",
      "Iteration 2066, loss = 0.65703415\n",
      "Iteration 2067, loss = 0.65699544\n",
      "Iteration 2068, loss = 0.65695671\n",
      "Iteration 2069, loss = 0.65691793\n",
      "Iteration 2070, loss = 0.65687912\n",
      "Iteration 2071, loss = 0.65684027\n",
      "Iteration 2072, loss = 0.65680139\n",
      "Iteration 2073, loss = 0.65676246\n",
      "Iteration 2074, loss = 0.65672350\n",
      "Iteration 2075, loss = 0.65668450\n",
      "Iteration 2076, loss = 0.65664547\n",
      "Iteration 2077, loss = 0.65660639\n",
      "Iteration 2078, loss = 0.65656728\n",
      "Iteration 2079, loss = 0.65652813\n",
      "Iteration 2080, loss = 0.65648895\n",
      "Iteration 2081, loss = 0.65644972\n",
      "Iteration 2082, loss = 0.65641046\n",
      "Iteration 2083, loss = 0.65637116\n",
      "Iteration 2084, loss = 0.65633182\n",
      "Iteration 2085, loss = 0.65629245\n",
      "Iteration 2086, loss = 0.65625303\n",
      "Iteration 2087, loss = 0.65621358\n",
      "Iteration 2088, loss = 0.65617409\n",
      "Iteration 2089, loss = 0.65613457\n",
      "Iteration 2090, loss = 0.65609500\n",
      "Iteration 2091, loss = 0.65605540\n",
      "Iteration 2092, loss = 0.65601576\n",
      "Iteration 2093, loss = 0.65597608\n",
      "Iteration 2094, loss = 0.65593636\n",
      "Iteration 2095, loss = 0.65589660\n",
      "Iteration 2096, loss = 0.65585681\n",
      "Iteration 2097, loss = 0.65581697\n",
      "Iteration 2098, loss = 0.65577710\n",
      "Iteration 2099, loss = 0.65573719\n",
      "Iteration 2100, loss = 0.65569724\n",
      "Iteration 2101, loss = 0.65565726\n",
      "Iteration 2102, loss = 0.65561723\n",
      "Iteration 2103, loss = 0.65557716\n",
      "Iteration 2104, loss = 0.65553706\n",
      "Iteration 2105, loss = 0.65549692\n",
      "Iteration 2106, loss = 0.65545674\n",
      "Iteration 2107, loss = 0.65541652\n",
      "Iteration 2108, loss = 0.65537626\n",
      "Iteration 2109, loss = 0.65533596\n",
      "Iteration 2110, loss = 0.65529562\n",
      "Iteration 2111, loss = 0.65525525\n",
      "Iteration 2112, loss = 0.65521483\n",
      "Iteration 2113, loss = 0.65517438\n",
      "Iteration 2114, loss = 0.65513388\n",
      "Iteration 2115, loss = 0.65509335\n",
      "Iteration 2116, loss = 0.65505278\n",
      "Iteration 2117, loss = 0.65501217\n",
      "Iteration 2118, loss = 0.65497152\n",
      "Iteration 2119, loss = 0.65493083\n",
      "Iteration 2120, loss = 0.65489010\n",
      "Iteration 2121, loss = 0.65484933\n",
      "Iteration 2122, loss = 0.65480852\n",
      "Iteration 2123, loss = 0.65476767\n",
      "Iteration 2124, loss = 0.65472678\n",
      "Iteration 2125, loss = 0.65468586\n",
      "Iteration 2126, loss = 0.65464489\n",
      "Iteration 2127, loss = 0.65460388\n",
      "Iteration 2128, loss = 0.65456283\n",
      "Iteration 2129, loss = 0.65452175\n",
      "Iteration 2130, loss = 0.65448062\n",
      "Iteration 2131, loss = 0.65443945\n",
      "Iteration 2132, loss = 0.65439825\n",
      "Iteration 2133, loss = 0.65435700\n",
      "Iteration 2134, loss = 0.65431571\n",
      "Iteration 2135, loss = 0.65427438\n",
      "Iteration 2136, loss = 0.65423302\n",
      "Iteration 2137, loss = 0.65419161\n",
      "Iteration 2138, loss = 0.65415016\n",
      "Iteration 2139, loss = 0.65410867\n",
      "Iteration 2140, loss = 0.65406714\n",
      "Iteration 2141, loss = 0.65402557\n",
      "Iteration 2142, loss = 0.65398396\n",
      "Iteration 2143, loss = 0.65394231\n",
      "Iteration 2144, loss = 0.65390062\n",
      "Iteration 2145, loss = 0.65385889\n",
      "Iteration 2146, loss = 0.65381712\n",
      "Iteration 2147, loss = 0.65377530\n",
      "Iteration 2148, loss = 0.65373345\n",
      "Iteration 2149, loss = 0.65369155\n",
      "Iteration 2150, loss = 0.65364962\n",
      "Iteration 2151, loss = 0.65360764\n",
      "Iteration 2152, loss = 0.65356562\n",
      "Iteration 2153, loss = 0.65352356\n",
      "Iteration 2154, loss = 0.65348146\n",
      "Iteration 2155, loss = 0.65343932\n",
      "Iteration 2156, loss = 0.65339714\n",
      "Iteration 2157, loss = 0.65335492\n",
      "Iteration 2158, loss = 0.65331265\n",
      "Iteration 2159, loss = 0.65327035\n",
      "Iteration 2160, loss = 0.65322800\n",
      "Iteration 2161, loss = 0.65318561\n",
      "Iteration 2162, loss = 0.65314318\n",
      "Iteration 2163, loss = 0.65310071\n",
      "Iteration 2164, loss = 0.65305820\n",
      "Iteration 2165, loss = 0.65301564\n",
      "Iteration 2166, loss = 0.65297305\n",
      "Iteration 2167, loss = 0.65293041\n",
      "Iteration 2168, loss = 0.65288773\n",
      "Iteration 2169, loss = 0.65284501\n",
      "Iteration 2170, loss = 0.65280224\n",
      "Iteration 2171, loss = 0.65275944\n",
      "Iteration 2172, loss = 0.65271659\n",
      "Iteration 2173, loss = 0.65267370\n",
      "Iteration 2174, loss = 0.65263077\n",
      "Iteration 2175, loss = 0.65258780\n",
      "Iteration 2176, loss = 0.65254478\n",
      "Iteration 2177, loss = 0.65250173\n",
      "Iteration 2178, loss = 0.65245863\n",
      "Iteration 2179, loss = 0.65241548\n",
      "Iteration 2180, loss = 0.65237230\n",
      "Iteration 2181, loss = 0.65232907\n",
      "Iteration 2182, loss = 0.65228580\n",
      "Iteration 2183, loss = 0.65224249\n",
      "Iteration 2184, loss = 0.65219914\n",
      "Iteration 2185, loss = 0.65215574\n",
      "Iteration 2186, loss = 0.65211230\n",
      "Iteration 2187, loss = 0.65206882\n",
      "Iteration 2188, loss = 0.65202530\n",
      "Iteration 2189, loss = 0.65198173\n",
      "Iteration 2190, loss = 0.65193812\n",
      "Iteration 2191, loss = 0.65189447\n",
      "Iteration 2192, loss = 0.65185077\n",
      "Iteration 2193, loss = 0.65180703\n",
      "Iteration 2194, loss = 0.65176325\n",
      "Iteration 2195, loss = 0.65171943\n",
      "Iteration 2196, loss = 0.65167556\n",
      "Iteration 2197, loss = 0.65163165\n",
      "Iteration 2198, loss = 0.65158770\n",
      "Iteration 2199, loss = 0.65154370\n",
      "Iteration 2200, loss = 0.65149966\n",
      "Iteration 2201, loss = 0.65145558\n",
      "Iteration 2202, loss = 0.65141145\n",
      "Iteration 2203, loss = 0.65136728\n",
      "Iteration 2204, loss = 0.65132306\n",
      "Iteration 2205, loss = 0.65127881\n",
      "Iteration 2206, loss = 0.65123451\n",
      "Iteration 2207, loss = 0.65119016\n",
      "Iteration 2208, loss = 0.65114577\n",
      "Iteration 2209, loss = 0.65110134\n",
      "Iteration 2210, loss = 0.65105687\n",
      "Iteration 2211, loss = 0.65101235\n",
      "Iteration 2212, loss = 0.65096778\n",
      "Iteration 2213, loss = 0.65092318\n",
      "Iteration 2214, loss = 0.65087853\n",
      "Iteration 2215, loss = 0.65083383\n",
      "Iteration 2216, loss = 0.65078909\n",
      "Iteration 2217, loss = 0.65074431\n",
      "Iteration 2218, loss = 0.65069948\n",
      "Iteration 2219, loss = 0.65065461\n",
      "Iteration 2220, loss = 0.65060970\n",
      "Iteration 2221, loss = 0.65056474\n",
      "Iteration 2222, loss = 0.65051973\n",
      "Iteration 2223, loss = 0.65047468\n",
      "Iteration 2224, loss = 0.65042959\n",
      "Iteration 2225, loss = 0.65038445\n",
      "Iteration 2226, loss = 0.65033927\n",
      "Iteration 2227, loss = 0.65029405\n",
      "Iteration 2228, loss = 0.65024877\n",
      "Iteration 2229, loss = 0.65020346\n",
      "Iteration 2230, loss = 0.65015810\n",
      "Iteration 2231, loss = 0.65011269\n",
      "Iteration 2232, loss = 0.65006724\n",
      "Iteration 2233, loss = 0.65002175\n",
      "Iteration 2234, loss = 0.64997621\n",
      "Iteration 2235, loss = 0.64993062\n",
      "Iteration 2236, loss = 0.64988499\n",
      "Iteration 2237, loss = 0.64983932\n",
      "Iteration 2238, loss = 0.64979360\n",
      "Iteration 2239, loss = 0.64974783\n",
      "Iteration 2240, loss = 0.64970202\n",
      "Iteration 2241, loss = 0.64965617\n",
      "Iteration 2242, loss = 0.64961027\n",
      "Iteration 2243, loss = 0.64956432\n",
      "Iteration 2244, loss = 0.64951833\n",
      "Iteration 2245, loss = 0.64947229\n",
      "Iteration 2246, loss = 0.64942621\n",
      "Iteration 2247, loss = 0.64938008\n",
      "Iteration 2248, loss = 0.64933391\n",
      "Iteration 2249, loss = 0.64928769\n",
      "Iteration 2250, loss = 0.64924142\n",
      "Iteration 2251, loss = 0.64919511\n",
      "Iteration 2252, loss = 0.64914876\n",
      "Iteration 2253, loss = 0.64910236\n",
      "Iteration 2254, loss = 0.64905591\n",
      "Iteration 2255, loss = 0.64900941\n",
      "Iteration 2256, loss = 0.64896287\n",
      "Iteration 2257, loss = 0.64891629\n",
      "Iteration 2258, loss = 0.64886965\n",
      "Iteration 2259, loss = 0.64882298\n",
      "Iteration 2260, loss = 0.64877625\n",
      "Iteration 2261, loss = 0.64872948\n",
      "Iteration 2262, loss = 0.64868266\n",
      "Iteration 2263, loss = 0.64863580\n",
      "Iteration 2264, loss = 0.64858889\n",
      "Iteration 2265, loss = 0.64854193\n",
      "Iteration 2266, loss = 0.64849493\n",
      "Iteration 2267, loss = 0.64844788\n",
      "Iteration 2268, loss = 0.64840079\n",
      "Iteration 2269, loss = 0.64835365\n",
      "Iteration 2270, loss = 0.64830646\n",
      "Iteration 2271, loss = 0.64825922\n",
      "Iteration 2272, loss = 0.64821194\n",
      "Iteration 2273, loss = 0.64816461\n",
      "Iteration 2274, loss = 0.64811723\n",
      "Iteration 2275, loss = 0.64806981\n",
      "Iteration 2276, loss = 0.64802234\n",
      "Iteration 2277, loss = 0.64797482\n",
      "Iteration 2278, loss = 0.64792726\n",
      "Iteration 2279, loss = 0.64787965\n",
      "Iteration 2280, loss = 0.64783199\n",
      "Iteration 2281, loss = 0.64778428\n",
      "Iteration 2282, loss = 0.64773653\n",
      "Iteration 2283, loss = 0.64768873\n",
      "Iteration 2284, loss = 0.64764088\n",
      "Iteration 2285, loss = 0.64759298\n",
      "Iteration 2286, loss = 0.64754504\n",
      "Iteration 2287, loss = 0.64749705\n",
      "Iteration 2288, loss = 0.64744901\n",
      "Iteration 2289, loss = 0.64740093\n",
      "Iteration 2290, loss = 0.64735280\n",
      "Iteration 2291, loss = 0.64730462\n",
      "Iteration 2292, loss = 0.64725639\n",
      "Iteration 2293, loss = 0.64720811\n",
      "Iteration 2294, loss = 0.64715979\n",
      "Iteration 2295, loss = 0.64711141\n",
      "Iteration 2296, loss = 0.64706299\n",
      "Iteration 2297, loss = 0.64701453\n",
      "Iteration 2298, loss = 0.64696601\n",
      "Iteration 2299, loss = 0.64691744\n",
      "Iteration 2300, loss = 0.64686883\n",
      "Iteration 2301, loss = 0.64682017\n",
      "Iteration 2302, loss = 0.64677146\n",
      "Iteration 2303, loss = 0.64672271\n",
      "Iteration 2304, loss = 0.64667390\n",
      "Iteration 2305, loss = 0.64662505\n",
      "Iteration 2306, loss = 0.64657614\n",
      "Iteration 2307, loss = 0.64652719\n",
      "Iteration 2308, loss = 0.64647819\n",
      "Iteration 2309, loss = 0.64642914\n",
      "Iteration 2310, loss = 0.64638005\n",
      "Iteration 2311, loss = 0.64633090\n",
      "Iteration 2312, loss = 0.64628171\n",
      "Iteration 2313, loss = 0.64623246\n",
      "Iteration 2314, loss = 0.64618317\n",
      "Iteration 2315, loss = 0.64613383\n",
      "Iteration 2316, loss = 0.64608444\n",
      "Iteration 2317, loss = 0.64603500\n",
      "Iteration 2318, loss = 0.64598551\n",
      "Iteration 2319, loss = 0.64593597\n",
      "Iteration 2320, loss = 0.64588639\n",
      "Iteration 2321, loss = 0.64583675\n",
      "Iteration 2322, loss = 0.64578707\n",
      "Iteration 2323, loss = 0.64573733\n",
      "Iteration 2324, loss = 0.64568755\n",
      "Iteration 2325, loss = 0.64563771\n",
      "Iteration 2326, loss = 0.64558783\n",
      "Iteration 2327, loss = 0.64553790\n",
      "Iteration 2328, loss = 0.64548792\n",
      "Iteration 2329, loss = 0.64543789\n",
      "Iteration 2330, loss = 0.64538780\n",
      "Iteration 2331, loss = 0.64533767\n",
      "Iteration 2332, loss = 0.64528749\n",
      "Iteration 2333, loss = 0.64523726\n",
      "Iteration 2334, loss = 0.64518698\n",
      "Iteration 2335, loss = 0.64513665\n",
      "Iteration 2336, loss = 0.64508627\n",
      "Iteration 2337, loss = 0.64503584\n",
      "Iteration 2338, loss = 0.64498536\n",
      "Iteration 2339, loss = 0.64493483\n",
      "Iteration 2340, loss = 0.64488425\n",
      "Iteration 2341, loss = 0.64483362\n",
      "Iteration 2342, loss = 0.64478294\n",
      "Iteration 2343, loss = 0.64473220\n",
      "Iteration 2344, loss = 0.64468142\n",
      "Iteration 2345, loss = 0.64463059\n",
      "Iteration 2346, loss = 0.64457971\n",
      "Iteration 2347, loss = 0.64452877\n",
      "Iteration 2348, loss = 0.64447779\n",
      "Iteration 2349, loss = 0.64442675\n",
      "Iteration 2350, loss = 0.64437567\n",
      "Iteration 2351, loss = 0.64432453\n",
      "Iteration 2352, loss = 0.64427334\n",
      "Iteration 2353, loss = 0.64422210\n",
      "Iteration 2354, loss = 0.64417081\n",
      "Iteration 2355, loss = 0.64411947\n",
      "Iteration 2356, loss = 0.64406808\n",
      "Iteration 2357, loss = 0.64401664\n",
      "Iteration 2358, loss = 0.64396515\n",
      "Iteration 2359, loss = 0.64391360\n",
      "Iteration 2360, loss = 0.64386201\n",
      "Iteration 2361, loss = 0.64381036\n",
      "Iteration 2362, loss = 0.64375866\n",
      "Iteration 2363, loss = 0.64370691\n",
      "Iteration 2364, loss = 0.64365511\n",
      "Iteration 2365, loss = 0.64360326\n",
      "Iteration 2366, loss = 0.64355136\n",
      "Iteration 2367, loss = 0.64349940\n",
      "Iteration 2368, loss = 0.64344739\n",
      "Iteration 2369, loss = 0.64339533\n",
      "Iteration 2370, loss = 0.64334322\n",
      "Iteration 2371, loss = 0.64329106\n",
      "Iteration 2372, loss = 0.64323885\n",
      "Iteration 2373, loss = 0.64318658\n",
      "Iteration 2374, loss = 0.64313426\n",
      "Iteration 2375, loss = 0.64308189\n",
      "Iteration 2376, loss = 0.64302947\n",
      "Iteration 2377, loss = 0.64297699\n",
      "Iteration 2378, loss = 0.64292447\n",
      "Iteration 2379, loss = 0.64287189\n",
      "Iteration 2380, loss = 0.64281926\n",
      "Iteration 2381, loss = 0.64276657\n",
      "Iteration 2382, loss = 0.64271384\n",
      "Iteration 2383, loss = 0.64266105\n",
      "Iteration 2384, loss = 0.64260821\n",
      "Iteration 2385, loss = 0.64255532\n",
      "Iteration 2386, loss = 0.64250237\n",
      "Iteration 2387, loss = 0.64244937\n",
      "Iteration 2388, loss = 0.64239632\n",
      "Iteration 2389, loss = 0.64234322\n",
      "Iteration 2390, loss = 0.64229006\n",
      "Iteration 2391, loss = 0.64223685\n",
      "Iteration 2392, loss = 0.64218359\n",
      "Iteration 2393, loss = 0.64213028\n",
      "Iteration 2394, loss = 0.64207691\n",
      "Iteration 2395, loss = 0.64202349\n",
      "Iteration 2396, loss = 0.64197001\n",
      "Iteration 2397, loss = 0.64191649\n",
      "Iteration 2398, loss = 0.64186291\n",
      "Iteration 2399, loss = 0.64180927\n",
      "Iteration 2400, loss = 0.64175558\n",
      "Iteration 2401, loss = 0.64170184\n",
      "Iteration 2402, loss = 0.64164805\n",
      "Iteration 2403, loss = 0.64159420\n",
      "Iteration 2404, loss = 0.64154030\n",
      "Iteration 2405, loss = 0.64148635\n",
      "Iteration 2406, loss = 0.64143234\n",
      "Iteration 2407, loss = 0.64137828\n",
      "Iteration 2408, loss = 0.64132416\n",
      "Iteration 2409, loss = 0.64126999\n",
      "Iteration 2410, loss = 0.64121577\n",
      "Iteration 2411, loss = 0.64116150\n",
      "Iteration 2412, loss = 0.64110716\n",
      "Iteration 2413, loss = 0.64105278\n",
      "Iteration 2414, loss = 0.64099834\n",
      "Iteration 2415, loss = 0.64094385\n",
      "Iteration 2416, loss = 0.64088930\n",
      "Iteration 2417, loss = 0.64083470\n",
      "Iteration 2418, loss = 0.64078005\n",
      "Iteration 2419, loss = 0.64072534\n",
      "Iteration 2420, loss = 0.64067057\n",
      "Iteration 2421, loss = 0.64061576\n",
      "Iteration 2422, loss = 0.64056088\n",
      "Iteration 2423, loss = 0.64050596\n",
      "Iteration 2424, loss = 0.64045097\n",
      "Iteration 2425, loss = 0.64039594\n",
      "Iteration 2426, loss = 0.64034085\n",
      "Iteration 2427, loss = 0.64028570\n",
      "Iteration 2428, loss = 0.64023050\n",
      "Iteration 2429, loss = 0.64017525\n",
      "Iteration 2430, loss = 0.64011994\n",
      "Iteration 2431, loss = 0.64006457\n",
      "Iteration 2432, loss = 0.64000915\n",
      "Iteration 2433, loss = 0.63995368\n",
      "Iteration 2434, loss = 0.63989815\n",
      "Iteration 2435, loss = 0.63984256\n",
      "Iteration 2436, loss = 0.63978692\n",
      "Iteration 2437, loss = 0.63973122\n",
      "Iteration 2438, loss = 0.63967547\n",
      "Iteration 2439, loss = 0.63961967\n",
      "Iteration 2440, loss = 0.63956381\n",
      "Iteration 2441, loss = 0.63950789\n",
      "Iteration 2442, loss = 0.63945192\n",
      "Iteration 2443, loss = 0.63939589\n",
      "Iteration 2444, loss = 0.63933981\n",
      "Iteration 2445, loss = 0.63928367\n",
      "Iteration 2446, loss = 0.63922747\n",
      "Iteration 2447, loss = 0.63917122\n",
      "Iteration 2448, loss = 0.63911491\n",
      "Iteration 2449, loss = 0.63905855\n",
      "Iteration 2450, loss = 0.63900213\n",
      "Iteration 2451, loss = 0.63894566\n",
      "Iteration 2452, loss = 0.63888913\n",
      "Iteration 2453, loss = 0.63883254\n",
      "Iteration 2454, loss = 0.63877590\n",
      "Iteration 2455, loss = 0.63871920\n",
      "Iteration 2456, loss = 0.63866245\n",
      "Iteration 2457, loss = 0.63860564\n",
      "Iteration 2458, loss = 0.63854877\n",
      "Iteration 2459, loss = 0.63849184\n",
      "Iteration 2460, loss = 0.63843486\n",
      "Iteration 2461, loss = 0.63837783\n",
      "Iteration 2462, loss = 0.63832073\n",
      "Iteration 2463, loss = 0.63826358\n",
      "Iteration 2464, loss = 0.63820638\n",
      "Iteration 2465, loss = 0.63814911\n",
      "Iteration 2466, loss = 0.63809179\n",
      "Iteration 2467, loss = 0.63803442\n",
      "Iteration 2468, loss = 0.63797698\n",
      "Iteration 2469, loss = 0.63791949\n",
      "Iteration 2470, loss = 0.63786194\n",
      "Iteration 2471, loss = 0.63780434\n",
      "Iteration 2472, loss = 0.63774668\n",
      "Iteration 2473, loss = 0.63768896\n",
      "Iteration 2474, loss = 0.63763118\n",
      "Iteration 2475, loss = 0.63757335\n",
      "Iteration 2476, loss = 0.63751546\n",
      "Iteration 2477, loss = 0.63745751\n",
      "Iteration 2478, loss = 0.63739951\n",
      "Iteration 2479, loss = 0.63734144\n",
      "Iteration 2480, loss = 0.63728332\n",
      "Iteration 2481, loss = 0.63722515\n",
      "Iteration 2482, loss = 0.63716691\n",
      "Iteration 2483, loss = 0.63710862\n",
      "Iteration 2484, loss = 0.63705027\n",
      "Iteration 2485, loss = 0.63699186\n",
      "Iteration 2486, loss = 0.63693339\n",
      "Iteration 2487, loss = 0.63687487\n",
      "Iteration 2488, loss = 0.63681629\n",
      "Iteration 2489, loss = 0.63675765\n",
      "Iteration 2490, loss = 0.63669895\n",
      "Iteration 2491, loss = 0.63664019\n",
      "Iteration 2492, loss = 0.63658138\n",
      "Iteration 2493, loss = 0.63652250\n",
      "Iteration 2494, loss = 0.63646357\n",
      "Iteration 2495, loss = 0.63640458\n",
      "Iteration 2496, loss = 0.63634554\n",
      "Iteration 2497, loss = 0.63628643\n",
      "Iteration 2498, loss = 0.63622727\n",
      "Iteration 2499, loss = 0.63616804\n",
      "Iteration 2500, loss = 0.63610876\n",
      "Iteration 2501, loss = 0.63604942\n",
      "Iteration 2502, loss = 0.63599002\n",
      "Iteration 2503, loss = 0.63593057\n",
      "Iteration 2504, loss = 0.63587105\n",
      "Iteration 2505, loss = 0.63581148\n",
      "Iteration 2506, loss = 0.63575184\n",
      "Iteration 2507, loss = 0.63569215\n",
      "Iteration 2508, loss = 0.63563240\n",
      "Iteration 2509, loss = 0.63557259\n",
      "Iteration 2510, loss = 0.63551272\n",
      "Iteration 2511, loss = 0.63545279\n",
      "Iteration 2512, loss = 0.63539280\n",
      "Iteration 2513, loss = 0.63533275\n",
      "Iteration 2514, loss = 0.63527265\n",
      "Iteration 2515, loss = 0.63521248\n",
      "Iteration 2516, loss = 0.63515225\n",
      "Iteration 2517, loss = 0.63509197\n",
      "Iteration 2518, loss = 0.63503162\n",
      "Iteration 2519, loss = 0.63497122\n",
      "Iteration 2520, loss = 0.63491076\n",
      "Iteration 2521, loss = 0.63485023\n",
      "Iteration 2522, loss = 0.63478965\n",
      "Iteration 2523, loss = 0.63472901\n",
      "Iteration 2524, loss = 0.63466830\n",
      "Iteration 2525, loss = 0.63460754\n",
      "Iteration 2526, loss = 0.63454672\n",
      "Iteration 2527, loss = 0.63448584\n",
      "Iteration 2528, loss = 0.63442489\n",
      "Iteration 2529, loss = 0.63436389\n",
      "Iteration 2530, loss = 0.63430283\n",
      "Iteration 2531, loss = 0.63424170\n",
      "Iteration 2532, loss = 0.63418052\n",
      "Iteration 2533, loss = 0.63411928\n",
      "Iteration 2534, loss = 0.63405797\n",
      "Iteration 2535, loss = 0.63399661\n",
      "Iteration 2536, loss = 0.63393518\n",
      "Iteration 2537, loss = 0.63387370\n",
      "Iteration 2538, loss = 0.63381215\n",
      "Iteration 2539, loss = 0.63375054\n",
      "Iteration 2540, loss = 0.63368888\n",
      "Iteration 2541, loss = 0.63362715\n",
      "Iteration 2542, loss = 0.63356536\n",
      "Iteration 2543, loss = 0.63350351\n",
      "Iteration 2544, loss = 0.63344160\n",
      "Iteration 2545, loss = 0.63337963\n",
      "Iteration 2546, loss = 0.63331759\n",
      "Iteration 2547, loss = 0.63325550\n",
      "Iteration 2548, loss = 0.63319335\n",
      "Iteration 2549, loss = 0.63313113\n",
      "Iteration 2550, loss = 0.63306885\n",
      "Iteration 2551, loss = 0.63300651\n",
      "Iteration 2552, loss = 0.63294411\n",
      "Iteration 2553, loss = 0.63288165\n",
      "Iteration 2554, loss = 0.63281913\n",
      "Iteration 2555, loss = 0.63275655\n",
      "Iteration 2556, loss = 0.63269390\n",
      "Iteration 2557, loss = 0.63263119\n",
      "Iteration 2558, loss = 0.63256842\n",
      "Iteration 2559, loss = 0.63250559\n",
      "Iteration 2560, loss = 0.63244270\n",
      "Iteration 2561, loss = 0.63237975\n",
      "Iteration 2562, loss = 0.63231673\n",
      "Iteration 2563, loss = 0.63225365\n",
      "Iteration 2564, loss = 0.63219051\n",
      "Iteration 2565, loss = 0.63212731\n",
      "Iteration 2566, loss = 0.63206404\n",
      "Iteration 2567, loss = 0.63200072\n",
      "Iteration 2568, loss = 0.63193733\n",
      "Iteration 2569, loss = 0.63187388\n",
      "Iteration 2570, loss = 0.63181037\n",
      "Iteration 2571, loss = 0.63174679\n",
      "Iteration 2572, loss = 0.63168315\n",
      "Iteration 2573, loss = 0.63161945\n",
      "Iteration 2574, loss = 0.63155569\n",
      "Iteration 2575, loss = 0.63149186\n",
      "Iteration 2576, loss = 0.63142798\n",
      "Iteration 2577, loss = 0.63136403\n",
      "Iteration 2578, loss = 0.63130001\n",
      "Iteration 2579, loss = 0.63123594\n",
      "Iteration 2580, loss = 0.63117180\n",
      "Iteration 2581, loss = 0.63110760\n",
      "Iteration 2582, loss = 0.63104333\n",
      "Iteration 2583, loss = 0.63097900\n",
      "Iteration 2584, loss = 0.63091461\n",
      "Iteration 2585, loss = 0.63085016\n",
      "Iteration 2586, loss = 0.63078564\n",
      "Iteration 2587, loss = 0.63072106\n",
      "Iteration 2588, loss = 0.63065642\n",
      "Iteration 2589, loss = 0.63059171\n",
      "Iteration 2590, loss = 0.63052694\n",
      "Iteration 2591, loss = 0.63046211\n",
      "Iteration 2592, loss = 0.63039721\n",
      "Iteration 2593, loss = 0.63033225\n",
      "Iteration 2594, loss = 0.63026722\n",
      "Iteration 2595, loss = 0.63020214\n",
      "Iteration 2596, loss = 0.63013699\n",
      "Iteration 2597, loss = 0.63007177\n",
      "Iteration 2598, loss = 0.63000649\n",
      "Iteration 2599, loss = 0.62994115\n",
      "Iteration 2600, loss = 0.62987574\n",
      "Iteration 2601, loss = 0.62981027\n",
      "Iteration 2602, loss = 0.62974474\n",
      "Iteration 2603, loss = 0.62967914\n",
      "Iteration 2604, loss = 0.62961347\n",
      "Iteration 2605, loss = 0.62954775\n",
      "Iteration 2606, loss = 0.62948196\n",
      "Iteration 2607, loss = 0.62941610\n",
      "Iteration 2608, loss = 0.62935018\n",
      "Iteration 2609, loss = 0.62928420\n",
      "Iteration 2610, loss = 0.62921815\n",
      "Iteration 2611, loss = 0.62915204\n",
      "Iteration 2612, loss = 0.62908586\n",
      "Iteration 2613, loss = 0.62901962\n",
      "Iteration 2614, loss = 0.62895331\n",
      "Iteration 2615, loss = 0.62888694\n",
      "Iteration 2616, loss = 0.62882050\n",
      "Iteration 2617, loss = 0.62875400\n",
      "Iteration 2618, loss = 0.62868743\n",
      "Iteration 2619, loss = 0.62862080\n",
      "Iteration 2620, loss = 0.62855411\n",
      "Iteration 2621, loss = 0.62848735\n",
      "Iteration 2622, loss = 0.62842052\n",
      "Iteration 2623, loss = 0.62835363\n",
      "Iteration 2624, loss = 0.62828667\n",
      "Iteration 2625, loss = 0.62821965\n",
      "Iteration 2626, loss = 0.62815257\n",
      "Iteration 2627, loss = 0.62808541\n",
      "Iteration 2628, loss = 0.62801820\n",
      "Iteration 2629, loss = 0.62795091\n",
      "Iteration 2630, loss = 0.62788356\n",
      "Iteration 2631, loss = 0.62781615\n",
      "Iteration 2632, loss = 0.62774867\n",
      "Iteration 2633, loss = 0.62768113\n",
      "Iteration 2634, loss = 0.62761352\n",
      "Iteration 2635, loss = 0.62754584\n",
      "Iteration 2636, loss = 0.62747810\n",
      "Iteration 2637, loss = 0.62741029\n",
      "Iteration 2638, loss = 0.62734241\n",
      "Iteration 2639, loss = 0.62727447\n",
      "Iteration 2640, loss = 0.62720647\n",
      "Iteration 2641, loss = 0.62713840\n",
      "Iteration 2642, loss = 0.62707026\n",
      "Iteration 2643, loss = 0.62700205\n",
      "Iteration 2644, loss = 0.62693378\n",
      "Iteration 2645, loss = 0.62686545\n",
      "Iteration 2646, loss = 0.62679704\n",
      "Iteration 2647, loss = 0.62672857\n",
      "Iteration 2648, loss = 0.62666004\n",
      "Iteration 2649, loss = 0.62659143\n",
      "Iteration 2650, loss = 0.62652276\n",
      "Iteration 2651, loss = 0.62645403\n",
      "Iteration 2652, loss = 0.62638523\n",
      "Iteration 2653, loss = 0.62631636\n",
      "Iteration 2654, loss = 0.62624742\n",
      "Iteration 2655, loss = 0.62617842\n",
      "Iteration 2656, loss = 0.62610935\n",
      "Iteration 2657, loss = 0.62604021\n",
      "Iteration 2658, loss = 0.62597101\n",
      "Iteration 2659, loss = 0.62590174\n",
      "Iteration 2660, loss = 0.62583240\n",
      "Iteration 2661, loss = 0.62576300\n",
      "Iteration 2662, loss = 0.62569352\n",
      "Iteration 2663, loss = 0.62562399\n",
      "Iteration 2664, loss = 0.62555438\n",
      "Iteration 2665, loss = 0.62548471\n",
      "Iteration 2666, loss = 0.62541496\n",
      "Iteration 2667, loss = 0.62534516\n",
      "Iteration 2668, loss = 0.62527528\n",
      "Iteration 2669, loss = 0.62520534\n",
      "Iteration 2670, loss = 0.62513533\n",
      "Iteration 2671, loss = 0.62506525\n",
      "Iteration 2672, loss = 0.62499510\n",
      "Iteration 2673, loss = 0.62492489\n",
      "Iteration 2674, loss = 0.62485460\n",
      "Iteration 2675, loss = 0.62478425\n",
      "Iteration 2676, loss = 0.62471384\n",
      "Iteration 2677, loss = 0.62464335\n",
      "Iteration 2678, loss = 0.62457280\n",
      "Iteration 2679, loss = 0.62450217\n",
      "Iteration 2680, loss = 0.62443148\n",
      "Iteration 2681, loss = 0.62436073\n",
      "Iteration 2682, loss = 0.62428990\n",
      "Iteration 2683, loss = 0.62421900\n",
      "Iteration 2684, loss = 0.62414804\n",
      "Iteration 2685, loss = 0.62407701\n",
      "Iteration 2686, loss = 0.62400591\n",
      "Iteration 2687, loss = 0.62393474\n",
      "Iteration 2688, loss = 0.62386351\n",
      "Iteration 2689, loss = 0.62379220\n",
      "Iteration 2690, loss = 0.62372083\n",
      "Iteration 2691, loss = 0.62364938\n",
      "Iteration 2692, loss = 0.62357787\n",
      "Iteration 2693, loss = 0.62350629\n",
      "Iteration 2694, loss = 0.62343464\n",
      "Iteration 2695, loss = 0.62336292\n",
      "Iteration 2696, loss = 0.62329114\n",
      "Iteration 2697, loss = 0.62321928\n",
      "Iteration 2698, loss = 0.62314736\n",
      "Iteration 2699, loss = 0.62307536\n",
      "Iteration 2700, loss = 0.62300330\n",
      "Iteration 2701, loss = 0.62293116\n",
      "Iteration 2702, loss = 0.62285896\n",
      "Iteration 2703, loss = 0.62278669\n",
      "Iteration 2704, loss = 0.62271435\n",
      "Iteration 2705, loss = 0.62264194\n",
      "Iteration 2706, loss = 0.62256946\n",
      "Iteration 2707, loss = 0.62249691\n",
      "Iteration 2708, loss = 0.62242429\n",
      "Iteration 2709, loss = 0.62235161\n",
      "Iteration 2710, loss = 0.62227885\n",
      "Iteration 2711, loss = 0.62220602\n",
      "Iteration 2712, loss = 0.62213312\n",
      "Iteration 2713, loss = 0.62206016\n",
      "Iteration 2714, loss = 0.62198712\n",
      "Iteration 2715, loss = 0.62191401\n",
      "Iteration 2716, loss = 0.62184083\n",
      "Iteration 2717, loss = 0.62176759\n",
      "Iteration 2718, loss = 0.62169427\n",
      "Iteration 2719, loss = 0.62162088\n",
      "Iteration 2720, loss = 0.62154743\n",
      "Iteration 2721, loss = 0.62147390\n",
      "Iteration 2722, loss = 0.62140030\n",
      "Iteration 2723, loss = 0.62132663\n",
      "Iteration 2724, loss = 0.62125289\n",
      "Iteration 2725, loss = 0.62117908\n",
      "Iteration 2726, loss = 0.62110520\n",
      "Iteration 2727, loss = 0.62103125\n",
      "Iteration 2728, loss = 0.62095723\n",
      "Iteration 2729, loss = 0.62088314\n",
      "Iteration 2730, loss = 0.62080898\n",
      "Iteration 2731, loss = 0.62073474\n",
      "Iteration 2732, loss = 0.62066044\n",
      "Iteration 2733, loss = 0.62058607\n",
      "Iteration 2734, loss = 0.62051162\n",
      "Iteration 2735, loss = 0.62043710\n",
      "Iteration 2736, loss = 0.62036252\n",
      "Iteration 2737, loss = 0.62028786\n",
      "Iteration 2738, loss = 0.62021313\n",
      "Iteration 2739, loss = 0.62013833\n",
      "Iteration 2740, loss = 0.62006345\n",
      "Iteration 2741, loss = 0.61998851\n",
      "Iteration 2742, loss = 0.61991350\n",
      "Iteration 2743, loss = 0.61983841\n",
      "Iteration 2744, loss = 0.61976325\n",
      "Iteration 2745, loss = 0.61968802\n",
      "Iteration 2746, loss = 0.61961272\n",
      "Iteration 2747, loss = 0.61953735\n",
      "Iteration 2748, loss = 0.61946191\n",
      "Iteration 2749, loss = 0.61938639\n",
      "Iteration 2750, loss = 0.61931081\n",
      "Iteration 2751, loss = 0.61923515\n",
      "Iteration 2752, loss = 0.61915942\n",
      "Iteration 2753, loss = 0.61908361\n",
      "Iteration 2754, loss = 0.61900774\n",
      "Iteration 2755, loss = 0.61893179\n",
      "Iteration 2756, loss = 0.61885577\n",
      "Iteration 2757, loss = 0.61877968\n",
      "Iteration 2758, loss = 0.61870352\n",
      "Iteration 2759, loss = 0.61862729\n",
      "Iteration 2760, loss = 0.61855098\n",
      "Iteration 2761, loss = 0.61847460\n",
      "Iteration 2762, loss = 0.61839815\n",
      "Iteration 2763, loss = 0.61832162\n",
      "Iteration 2764, loss = 0.61824503\n",
      "Iteration 2765, loss = 0.61816836\n",
      "Iteration 2766, loss = 0.61809162\n",
      "Iteration 2767, loss = 0.61801480\n",
      "Iteration 2768, loss = 0.61793792\n",
      "Iteration 2769, loss = 0.61786096\n",
      "Iteration 2770, loss = 0.61778392\n",
      "Iteration 2771, loss = 0.61770682\n",
      "Iteration 2772, loss = 0.61762964\n",
      "Iteration 2773, loss = 0.61755239\n",
      "Iteration 2774, loss = 0.61747507\n",
      "Iteration 2775, loss = 0.61739767\n",
      "Iteration 2776, loss = 0.61732020\n",
      "Iteration 2777, loss = 0.61724266\n",
      "Iteration 2778, loss = 0.61716504\n",
      "Iteration 2779, loss = 0.61708735\n",
      "Iteration 2780, loss = 0.61700959\n",
      "Iteration 2781, loss = 0.61693176\n",
      "Iteration 2782, loss = 0.61685385\n",
      "Iteration 2783, loss = 0.61677587\n",
      "Iteration 2784, loss = 0.61669781\n",
      "Iteration 2785, loss = 0.61661968\n",
      "Iteration 2786, loss = 0.61654148\n",
      "Iteration 2787, loss = 0.61646320\n",
      "Iteration 2788, loss = 0.61638485\n",
      "Iteration 2789, loss = 0.61630643\n",
      "Iteration 2790, loss = 0.61622793\n",
      "Iteration 2791, loss = 0.61614936\n",
      "Iteration 2792, loss = 0.61607072\n",
      "Iteration 2793, loss = 0.61599200\n",
      "Iteration 2794, loss = 0.61591321\n",
      "Iteration 2795, loss = 0.61583434\n",
      "Iteration 2796, loss = 0.61575540\n",
      "Iteration 2797, loss = 0.61567639\n",
      "Iteration 2798, loss = 0.61559730\n",
      "Iteration 2799, loss = 0.61551814\n",
      "Iteration 2800, loss = 0.61543890\n",
      "Iteration 2801, loss = 0.61535959\n",
      "Iteration 2802, loss = 0.61528020\n",
      "Iteration 2803, loss = 0.61520074\n",
      "Iteration 2804, loss = 0.61512121\n",
      "Iteration 2805, loss = 0.61504160\n",
      "Iteration 2806, loss = 0.61496192\n",
      "Iteration 2807, loss = 0.61488216\n",
      "Iteration 2808, loss = 0.61480233\n",
      "Iteration 2809, loss = 0.61472242\n",
      "Iteration 2810, loss = 0.61464244\n",
      "Iteration 2811, loss = 0.61456238\n",
      "Iteration 2812, loss = 0.61448225\n",
      "Iteration 2813, loss = 0.61440205\n",
      "Iteration 2814, loss = 0.61432177\n",
      "Iteration 2815, loss = 0.61424141\n",
      "Iteration 2816, loss = 0.61416098\n",
      "Iteration 2817, loss = 0.61408048\n",
      "Iteration 2818, loss = 0.61399990\n",
      "Iteration 2819, loss = 0.61391924\n",
      "Iteration 2820, loss = 0.61383851\n",
      "Iteration 2821, loss = 0.61375770\n",
      "Iteration 2822, loss = 0.61367682\n",
      "Iteration 2823, loss = 0.61359587\n",
      "Iteration 2824, loss = 0.61351483\n",
      "Iteration 2825, loss = 0.61343373\n",
      "Iteration 2826, loss = 0.61335254\n",
      "Iteration 2827, loss = 0.61327128\n",
      "Iteration 2828, loss = 0.61318995\n",
      "Iteration 2829, loss = 0.61310854\n",
      "Iteration 2830, loss = 0.61302706\n",
      "Iteration 2831, loss = 0.61294550\n",
      "Iteration 2832, loss = 0.61286386\n",
      "Iteration 2833, loss = 0.61278215\n",
      "Iteration 2834, loss = 0.61270036\n",
      "Iteration 2835, loss = 0.61261850\n",
      "Iteration 2836, loss = 0.61253656\n",
      "Iteration 2837, loss = 0.61245454\n",
      "Iteration 2838, loss = 0.61237245\n",
      "Iteration 2839, loss = 0.61229028\n",
      "Iteration 2840, loss = 0.61220804\n",
      "Iteration 2841, loss = 0.61212572\n",
      "Iteration 2842, loss = 0.61204332\n",
      "Iteration 2843, loss = 0.61196085\n",
      "Iteration 2844, loss = 0.61187830\n",
      "Iteration 2845, loss = 0.61179567\n",
      "Iteration 2846, loss = 0.61171297\n",
      "Iteration 2847, loss = 0.61163019\n",
      "Iteration 2848, loss = 0.61154734\n",
      "Iteration 2849, loss = 0.61146441\n",
      "Iteration 2850, loss = 0.61138140\n",
      "Iteration 2851, loss = 0.61129832\n",
      "Iteration 2852, loss = 0.61121516\n",
      "Iteration 2853, loss = 0.61113192\n",
      "Iteration 2854, loss = 0.61104860\n",
      "Iteration 2855, loss = 0.61096521\n",
      "Iteration 2856, loss = 0.61088175\n",
      "Iteration 2857, loss = 0.61079820\n",
      "Iteration 2858, loss = 0.61071458\n",
      "Iteration 2859, loss = 0.61063088\n",
      "Iteration 2860, loss = 0.61054711\n",
      "Iteration 2861, loss = 0.61046325\n",
      "Iteration 2862, loss = 0.61037932\n",
      "Iteration 2863, loss = 0.61029532\n",
      "Iteration 2864, loss = 0.61021123\n",
      "Iteration 2865, loss = 0.61012707\n",
      "Iteration 2866, loss = 0.61004283\n",
      "Iteration 2867, loss = 0.60995851\n",
      "Iteration 2868, loss = 0.60987412\n",
      "Iteration 2869, loss = 0.60978965\n",
      "Iteration 2870, loss = 0.60970510\n",
      "Iteration 2871, loss = 0.60962048\n",
      "Iteration 2872, loss = 0.60953577\n",
      "Iteration 2873, loss = 0.60945099\n",
      "Iteration 2874, loss = 0.60936613\n",
      "Iteration 2875, loss = 0.60928120\n",
      "Iteration 2876, loss = 0.60919618\n",
      "Iteration 2877, loss = 0.60911109\n",
      "Iteration 2878, loss = 0.60902592\n",
      "Iteration 2879, loss = 0.60894067\n",
      "Iteration 2880, loss = 0.60885535\n",
      "Iteration 2881, loss = 0.60876994\n",
      "Iteration 2882, loss = 0.60868446\n",
      "Iteration 2883, loss = 0.60859890\n",
      "Iteration 2884, loss = 0.60851326\n",
      "Iteration 2885, loss = 0.60842755\n",
      "Iteration 2886, loss = 0.60834175\n",
      "Iteration 2887, loss = 0.60825588\n",
      "Iteration 2888, loss = 0.60816993\n",
      "Iteration 2889, loss = 0.60808390\n",
      "Iteration 2890, loss = 0.60799780\n",
      "Iteration 2891, loss = 0.60791161\n",
      "Iteration 2892, loss = 0.60782535\n",
      "Iteration 2893, loss = 0.60773900\n",
      "Iteration 2894, loss = 0.60765258\n",
      "Iteration 2895, loss = 0.60756608\n",
      "Iteration 2896, loss = 0.60747950\n",
      "Iteration 2897, loss = 0.60739285\n",
      "Iteration 2898, loss = 0.60730611\n",
      "Iteration 2899, loss = 0.60721930\n",
      "Iteration 2900, loss = 0.60713240\n",
      "Iteration 2901, loss = 0.60704543\n",
      "Iteration 2902, loss = 0.60695838\n",
      "Iteration 2903, loss = 0.60687125\n",
      "Iteration 2904, loss = 0.60678404\n",
      "Iteration 2905, loss = 0.60669675\n",
      "Iteration 2906, loss = 0.60660939\n",
      "Iteration 2907, loss = 0.60652194\n",
      "Iteration 2908, loss = 0.60643442\n",
      "Iteration 2909, loss = 0.60634681\n",
      "Iteration 2910, loss = 0.60625913\n",
      "Iteration 2911, loss = 0.60617136\n",
      "Iteration 2912, loss = 0.60608352\n",
      "Iteration 2913, loss = 0.60599560\n",
      "Iteration 2914, loss = 0.60590760\n",
      "Iteration 2915, loss = 0.60581952\n",
      "Iteration 2916, loss = 0.60573136\n",
      "Iteration 2917, loss = 0.60564312\n",
      "Iteration 2918, loss = 0.60555480\n",
      "Iteration 2919, loss = 0.60546640\n",
      "Iteration 2920, loss = 0.60537792\n",
      "Iteration 2921, loss = 0.60528937\n",
      "Iteration 2922, loss = 0.60520073\n",
      "Iteration 2923, loss = 0.60511201\n",
      "Iteration 2924, loss = 0.60502321\n",
      "Iteration 2925, loss = 0.60493434\n",
      "Iteration 2926, loss = 0.60484538\n",
      "Iteration 2927, loss = 0.60475634\n",
      "Iteration 2928, loss = 0.60466723\n",
      "Iteration 2929, loss = 0.60457803\n",
      "Iteration 2930, loss = 0.60448875\n",
      "Iteration 2931, loss = 0.60439940\n",
      "Iteration 2932, loss = 0.60430996\n",
      "Iteration 2933, loss = 0.60422044\n",
      "Iteration 2934, loss = 0.60413084\n",
      "Iteration 2935, loss = 0.60404117\n",
      "Iteration 2936, loss = 0.60395141\n",
      "Iteration 2937, loss = 0.60386157\n",
      "Iteration 2938, loss = 0.60377165\n",
      "Iteration 2939, loss = 0.60368165\n",
      "Iteration 2940, loss = 0.60359157\n",
      "Iteration 2941, loss = 0.60350141\n",
      "Iteration 2942, loss = 0.60341117\n",
      "Iteration 2943, loss = 0.60332085\n",
      "Iteration 2944, loss = 0.60323045\n",
      "Iteration 2945, loss = 0.60313996\n",
      "Iteration 2946, loss = 0.60304940\n",
      "Iteration 2947, loss = 0.60295876\n",
      "Iteration 2948, loss = 0.60286803\n",
      "Iteration 2949, loss = 0.60277723\n",
      "Iteration 2950, loss = 0.60268634\n",
      "Iteration 2951, loss = 0.60259537\n",
      "Iteration 2952, loss = 0.60250432\n",
      "Iteration 2953, loss = 0.60241319\n",
      "Iteration 2954, loss = 0.60232198\n",
      "Iteration 2955, loss = 0.60223069\n",
      "Iteration 2956, loss = 0.60213932\n",
      "Iteration 2957, loss = 0.60204787\n",
      "Iteration 2958, loss = 0.60195633\n",
      "Iteration 2959, loss = 0.60186471\n",
      "Iteration 2960, loss = 0.60177302\n",
      "Iteration 2961, loss = 0.60168124\n",
      "Iteration 2962, loss = 0.60158938\n",
      "Iteration 2963, loss = 0.60149744\n",
      "Iteration 2964, loss = 0.60140541\n",
      "Iteration 2965, loss = 0.60131331\n",
      "Iteration 2966, loss = 0.60122112\n",
      "Iteration 2967, loss = 0.60112886\n",
      "Iteration 2968, loss = 0.60103651\n",
      "Iteration 2969, loss = 0.60094408\n",
      "Iteration 2970, loss = 0.60085156\n",
      "Iteration 2971, loss = 0.60075897\n",
      "Iteration 2972, loss = 0.60066629\n",
      "Iteration 2973, loss = 0.60057354\n",
      "Iteration 2974, loss = 0.60048070\n",
      "Iteration 2975, loss = 0.60038778\n",
      "Iteration 2976, loss = 0.60029477\n",
      "Iteration 2977, loss = 0.60020169\n",
      "Iteration 2978, loss = 0.60010852\n",
      "Iteration 2979, loss = 0.60001527\n",
      "Iteration 2980, loss = 0.59992194\n",
      "Iteration 2981, loss = 0.59982853\n",
      "Iteration 2982, loss = 0.59973504\n",
      "Iteration 2983, loss = 0.59964146\n",
      "Iteration 2984, loss = 0.59954780\n",
      "Iteration 2985, loss = 0.59945406\n",
      "Iteration 2986, loss = 0.59936023\n",
      "Iteration 2987, loss = 0.59926633\n",
      "Iteration 2988, loss = 0.59917234\n",
      "Iteration 2989, loss = 0.59907827\n",
      "Iteration 2990, loss = 0.59898412\n",
      "Iteration 2991, loss = 0.59888988\n",
      "Iteration 2992, loss = 0.59879556\n",
      "Iteration 2993, loss = 0.59870116\n",
      "Iteration 2994, loss = 0.59860668\n",
      "Iteration 2995, loss = 0.59851212\n",
      "Iteration 2996, loss = 0.59841747\n",
      "Iteration 2997, loss = 0.59832274\n",
      "Iteration 2998, loss = 0.59822792\n",
      "Iteration 2999, loss = 0.59813303\n",
      "Iteration 3000, loss = 0.59803805\n",
      "Iteration 3001, loss = 0.59794299\n",
      "Iteration 3002, loss = 0.59784784\n",
      "Iteration 3003, loss = 0.59775261\n",
      "Iteration 3004, loss = 0.59765730\n",
      "Iteration 3005, loss = 0.59756191\n",
      "Iteration 3006, loss = 0.59746644\n",
      "Iteration 3007, loss = 0.59737088\n",
      "Iteration 3008, loss = 0.59727523\n",
      "Iteration 3009, loss = 0.59717951\n",
      "Iteration 3010, loss = 0.59708370\n",
      "Iteration 3011, loss = 0.59698781\n",
      "Iteration 3012, loss = 0.59689183\n",
      "Iteration 3013, loss = 0.59679578\n",
      "Iteration 3014, loss = 0.59669964\n",
      "Iteration 3015, loss = 0.59660341\n",
      "Iteration 3016, loss = 0.59650710\n",
      "Iteration 3017, loss = 0.59641071\n",
      "Iteration 3018, loss = 0.59631424\n",
      "Iteration 3019, loss = 0.59621768\n",
      "Iteration 3020, loss = 0.59612104\n",
      "Iteration 3021, loss = 0.59602432\n",
      "Iteration 3022, loss = 0.59592751\n",
      "Iteration 3023, loss = 0.59583062\n",
      "Iteration 3024, loss = 0.59573364\n",
      "Iteration 3025, loss = 0.59563658\n",
      "Iteration 3026, loss = 0.59553944\n",
      "Iteration 3027, loss = 0.59544221\n",
      "Iteration 3028, loss = 0.59534490\n",
      "Iteration 3029, loss = 0.59524751\n",
      "Iteration 3030, loss = 0.59515003\n",
      "Iteration 3031, loss = 0.59505247\n",
      "Iteration 3032, loss = 0.59495483\n",
      "Iteration 3033, loss = 0.59485710\n",
      "Iteration 3034, loss = 0.59475928\n",
      "Iteration 3035, loss = 0.59466139\n",
      "Iteration 3036, loss = 0.59456341\n",
      "Iteration 3037, loss = 0.59446534\n",
      "Iteration 3038, loss = 0.59436719\n",
      "Iteration 3039, loss = 0.59426896\n",
      "Iteration 3040, loss = 0.59417064\n",
      "Iteration 3041, loss = 0.59407224\n",
      "Iteration 3042, loss = 0.59397376\n",
      "Iteration 3043, loss = 0.59387519\n",
      "Iteration 3044, loss = 0.59377653\n",
      "Iteration 3045, loss = 0.59367779\n",
      "Iteration 3046, loss = 0.59357897\n",
      "Iteration 3047, loss = 0.59348007\n",
      "Iteration 3048, loss = 0.59338107\n",
      "Iteration 3049, loss = 0.59328200\n",
      "Iteration 3050, loss = 0.59318284\n",
      "Iteration 3051, loss = 0.59308359\n",
      "Iteration 3052, loss = 0.59298427\n",
      "Iteration 3053, loss = 0.59288485\n",
      "Iteration 3054, loss = 0.59278535\n",
      "Iteration 3055, loss = 0.59268577\n",
      "Iteration 3056, loss = 0.59258611\n",
      "Iteration 3057, loss = 0.59248635\n",
      "Iteration 3058, loss = 0.59238652\n",
      "Iteration 3059, loss = 0.59228660\n",
      "Iteration 3060, loss = 0.59218659\n",
      "Iteration 3061, loss = 0.59208650\n",
      "Iteration 3062, loss = 0.59198633\n",
      "Iteration 3063, loss = 0.59188606\n",
      "Iteration 3064, loss = 0.59178572\n",
      "Iteration 3065, loss = 0.59168529\n",
      "Iteration 3066, loss = 0.59158478\n",
      "Iteration 3067, loss = 0.59148418\n",
      "Iteration 3068, loss = 0.59138349\n",
      "Iteration 3069, loss = 0.59128272\n",
      "Iteration 3070, loss = 0.59118187\n",
      "Iteration 3071, loss = 0.59108093\n",
      "Iteration 3072, loss = 0.59097990\n",
      "Iteration 3073, loss = 0.59087879\n",
      "Iteration 3074, loss = 0.59077760\n",
      "Iteration 3075, loss = 0.59067632\n",
      "Iteration 3076, loss = 0.59057495\n",
      "Iteration 3077, loss = 0.59047350\n",
      "Iteration 3078, loss = 0.59037197\n",
      "Iteration 3079, loss = 0.59027034\n",
      "Iteration 3080, loss = 0.59016864\n",
      "Iteration 3081, loss = 0.59006685\n",
      "Iteration 3082, loss = 0.58996497\n",
      "Iteration 3083, loss = 0.58986301\n",
      "Iteration 3084, loss = 0.58976096\n",
      "Iteration 3085, loss = 0.58965883\n",
      "Iteration 3086, loss = 0.58955661\n",
      "Iteration 3087, loss = 0.58945430\n",
      "Iteration 3088, loss = 0.58935191\n",
      "Iteration 3089, loss = 0.58924944\n",
      "Iteration 3090, loss = 0.58914688\n",
      "Iteration 3091, loss = 0.58904423\n",
      "Iteration 3092, loss = 0.58894150\n",
      "Iteration 3093, loss = 0.58883868\n",
      "Iteration 3094, loss = 0.58873577\n",
      "Iteration 3095, loss = 0.58863278\n",
      "Iteration 3096, loss = 0.58852971\n",
      "Iteration 3097, loss = 0.58842655\n",
      "Iteration 3098, loss = 0.58832330\n",
      "Iteration 3099, loss = 0.58821997\n",
      "Iteration 3100, loss = 0.58811655\n",
      "Iteration 3101, loss = 0.58801305\n",
      "Iteration 3102, loss = 0.58790946\n",
      "Iteration 3103, loss = 0.58780578\n",
      "Iteration 3104, loss = 0.58770202\n",
      "Iteration 3105, loss = 0.58759817\n",
      "Iteration 3106, loss = 0.58749424\n",
      "Iteration 3107, loss = 0.58739022\n",
      "Iteration 3108, loss = 0.58728611\n",
      "Iteration 3109, loss = 0.58718192\n",
      "Iteration 3110, loss = 0.58707764\n",
      "Iteration 3111, loss = 0.58697327\n",
      "Iteration 3112, loss = 0.58686882\n",
      "Iteration 3113, loss = 0.58676429\n",
      "Iteration 3114, loss = 0.58665966\n",
      "Iteration 3115, loss = 0.58655495\n",
      "Iteration 3116, loss = 0.58645016\n",
      "Iteration 3117, loss = 0.58634528\n",
      "Iteration 3118, loss = 0.58624031\n",
      "Iteration 3119, loss = 0.58613525\n",
      "Iteration 3120, loss = 0.58603011\n",
      "Iteration 3121, loss = 0.58592488\n",
      "Iteration 3122, loss = 0.58581957\n",
      "Iteration 3123, loss = 0.58571417\n",
      "Iteration 3124, loss = 0.58560868\n",
      "Iteration 3125, loss = 0.58550311\n",
      "Iteration 3126, loss = 0.58539745\n",
      "Iteration 3127, loss = 0.58529170\n",
      "Iteration 3128, loss = 0.58518587\n",
      "Iteration 3129, loss = 0.58507995\n",
      "Iteration 3130, loss = 0.58497395\n",
      "Iteration 3131, loss = 0.58486785\n",
      "Iteration 3132, loss = 0.58476167\n",
      "Iteration 3133, loss = 0.58465541\n",
      "Iteration 3134, loss = 0.58454906\n",
      "Iteration 3135, loss = 0.58444262\n",
      "Iteration 3136, loss = 0.58433609\n",
      "Iteration 3137, loss = 0.58422948\n",
      "Iteration 3138, loss = 0.58412278\n",
      "Iteration 3139, loss = 0.58401599\n",
      "Iteration 3140, loss = 0.58390912\n",
      "Iteration 3141, loss = 0.58380216\n",
      "Iteration 3142, loss = 0.58369511\n",
      "Iteration 3143, loss = 0.58358798\n",
      "Iteration 3144, loss = 0.58348076\n",
      "Iteration 3145, loss = 0.58337345\n",
      "Iteration 3146, loss = 0.58326606\n",
      "Iteration 3147, loss = 0.58315857\n",
      "Iteration 3148, loss = 0.58305101\n",
      "Iteration 3149, loss = 0.58294335\n",
      "Iteration 3150, loss = 0.58283561\n",
      "Iteration 3151, loss = 0.58272778\n",
      "Iteration 3152, loss = 0.58261986\n",
      "Iteration 3153, loss = 0.58251186\n",
      "Iteration 3154, loss = 0.58240377\n",
      "Iteration 3155, loss = 0.58229559\n",
      "Iteration 3156, loss = 0.58218732\n",
      "Iteration 3157, loss = 0.58207897\n",
      "Iteration 3158, loss = 0.58197053\n",
      "Iteration 3159, loss = 0.58186200\n",
      "Iteration 3160, loss = 0.58175339\n",
      "Iteration 3161, loss = 0.58164469\n",
      "Iteration 3162, loss = 0.58153590\n",
      "Iteration 3163, loss = 0.58142702\n",
      "Iteration 3164, loss = 0.58131806\n",
      "Iteration 3165, loss = 0.58120901\n",
      "Iteration 3166, loss = 0.58109987\n",
      "Iteration 3167, loss = 0.58099065\n",
      "Iteration 3168, loss = 0.58088133\n",
      "Iteration 3169, loss = 0.58077193\n",
      "Iteration 3170, loss = 0.58066245\n",
      "Iteration 3171, loss = 0.58055287\n",
      "Iteration 3172, loss = 0.58044321\n",
      "Iteration 3173, loss = 0.58033346\n",
      "Iteration 3174, loss = 0.58022362\n",
      "Iteration 3175, loss = 0.58011369\n",
      "Iteration 3176, loss = 0.58000368\n",
      "Iteration 3177, loss = 0.57989358\n",
      "Iteration 3178, loss = 0.57978339\n",
      "Iteration 3179, loss = 0.57967312\n",
      "Iteration 3180, loss = 0.57956276\n",
      "Iteration 3181, loss = 0.57945230\n",
      "Iteration 3182, loss = 0.57934177\n",
      "Iteration 3183, loss = 0.57923114\n",
      "Iteration 3184, loss = 0.57912043\n",
      "Iteration 3185, loss = 0.57900963\n",
      "Iteration 3186, loss = 0.57889874\n",
      "Iteration 3187, loss = 0.57878776\n",
      "Iteration 3188, loss = 0.57867669\n",
      "Iteration 3189, loss = 0.57856554\n",
      "Iteration 3190, loss = 0.57845430\n",
      "Iteration 3191, loss = 0.57834297\n",
      "Iteration 3192, loss = 0.57823156\n",
      "Iteration 3193, loss = 0.57812005\n",
      "Iteration 3194, loss = 0.57800846\n",
      "Iteration 3195, loss = 0.57789678\n",
      "Iteration 3196, loss = 0.57778502\n",
      "Iteration 3197, loss = 0.57767316\n",
      "Iteration 3198, loss = 0.57756122\n",
      "Iteration 3199, loss = 0.57744919\n",
      "Iteration 3200, loss = 0.57733707\n",
      "Iteration 3201, loss = 0.57722486\n",
      "Iteration 3202, loss = 0.57711256\n",
      "Iteration 3203, loss = 0.57700018\n",
      "Iteration 3204, loss = 0.57688771\n",
      "Iteration 3205, loss = 0.57677515\n",
      "Iteration 3206, loss = 0.57666250\n",
      "Iteration 3207, loss = 0.57654977\n",
      "Iteration 3208, loss = 0.57643695\n",
      "Iteration 3209, loss = 0.57632403\n",
      "Iteration 3210, loss = 0.57621104\n",
      "Iteration 3211, loss = 0.57609795\n",
      "Iteration 3212, loss = 0.57598477\n",
      "Iteration 3213, loss = 0.57587151\n",
      "Iteration 3214, loss = 0.57575816\n",
      "Iteration 3215, loss = 0.57564472\n",
      "Iteration 3216, loss = 0.57553119\n",
      "Iteration 3217, loss = 0.57541757\n",
      "Iteration 3218, loss = 0.57530387\n",
      "Iteration 3219, loss = 0.57519007\n",
      "Iteration 3220, loss = 0.57507619\n",
      "Iteration 3221, loss = 0.57496222\n",
      "Iteration 3222, loss = 0.57484817\n",
      "Iteration 3223, loss = 0.57473402\n",
      "Iteration 3224, loss = 0.57461979\n",
      "Iteration 3225, loss = 0.57450546\n",
      "Iteration 3226, loss = 0.57439105\n",
      "Iteration 3227, loss = 0.57427655\n",
      "Iteration 3228, loss = 0.57416196\n",
      "Iteration 3229, loss = 0.57404729\n",
      "Iteration 3230, loss = 0.57393252\n",
      "Iteration 3231, loss = 0.57381767\n",
      "Iteration 3232, loss = 0.57370273\n",
      "Iteration 3233, loss = 0.57358770\n",
      "Iteration 3234, loss = 0.57347258\n",
      "Iteration 3235, loss = 0.57335738\n",
      "Iteration 3236, loss = 0.57324208\n",
      "Iteration 3237, loss = 0.57312670\n",
      "Iteration 3238, loss = 0.57301123\n",
      "Iteration 3239, loss = 0.57289567\n",
      "Iteration 3240, loss = 0.57278002\n",
      "Iteration 3241, loss = 0.57266428\n",
      "Iteration 3242, loss = 0.57254846\n",
      "Iteration 3243, loss = 0.57243254\n",
      "Iteration 3244, loss = 0.57231654\n",
      "Iteration 3245, loss = 0.57220045\n",
      "Iteration 3246, loss = 0.57208427\n",
      "Iteration 3247, loss = 0.57196800\n",
      "Iteration 3248, loss = 0.57185164\n",
      "Iteration 3249, loss = 0.57173520\n",
      "Iteration 3250, loss = 0.57161866\n",
      "Iteration 3251, loss = 0.57150204\n",
      "Iteration 3252, loss = 0.57138533\n",
      "Iteration 3253, loss = 0.57126853\n",
      "Iteration 3254, loss = 0.57115164\n",
      "Iteration 3255, loss = 0.57103467\n",
      "Iteration 3256, loss = 0.57091760\n",
      "Iteration 3257, loss = 0.57080045\n",
      "Iteration 3258, loss = 0.57068320\n",
      "Iteration 3259, loss = 0.57056587\n",
      "Iteration 3260, loss = 0.57044845\n",
      "Iteration 3261, loss = 0.57033094\n",
      "Iteration 3262, loss = 0.57021335\n",
      "Iteration 3263, loss = 0.57009566\n",
      "Iteration 3264, loss = 0.56997788\n",
      "Iteration 3265, loss = 0.56986002\n",
      "Iteration 3266, loss = 0.56974207\n",
      "Iteration 3267, loss = 0.56962403\n",
      "Iteration 3268, loss = 0.56950590\n",
      "Iteration 3269, loss = 0.56938768\n",
      "Iteration 3270, loss = 0.56926937\n",
      "Iteration 3271, loss = 0.56915098\n",
      "Iteration 3272, loss = 0.56903249\n",
      "Iteration 3273, loss = 0.56891392\n",
      "Iteration 3274, loss = 0.56879526\n",
      "Iteration 3275, loss = 0.56867650\n",
      "Iteration 3276, loss = 0.56855766\n",
      "Iteration 3277, loss = 0.56843874\n",
      "Iteration 3278, loss = 0.56831972\n",
      "Iteration 3279, loss = 0.56820061\n",
      "Iteration 3280, loss = 0.56808142\n",
      "Iteration 3281, loss = 0.56796213\n",
      "Iteration 3282, loss = 0.56784276\n",
      "Iteration 3283, loss = 0.56772330\n",
      "Iteration 3284, loss = 0.56760375\n",
      "Iteration 3285, loss = 0.56748411\n",
      "Iteration 3286, loss = 0.56736438\n",
      "Iteration 3287, loss = 0.56724456\n",
      "Iteration 3288, loss = 0.56712466\n",
      "Iteration 3289, loss = 0.56700466\n",
      "Iteration 3290, loss = 0.56688458\n",
      "Iteration 3291, loss = 0.56676441\n",
      "Iteration 3292, loss = 0.56664415\n",
      "Iteration 3293, loss = 0.56652380\n",
      "Iteration 3294, loss = 0.56640336\n",
      "Iteration 3295, loss = 0.56628283\n",
      "Iteration 3296, loss = 0.56616221\n",
      "Iteration 3297, loss = 0.56604151\n",
      "Iteration 3298, loss = 0.56592071\n",
      "Iteration 3299, loss = 0.56579983\n",
      "Iteration 3300, loss = 0.56567886\n",
      "Iteration 3301, loss = 0.56555780\n",
      "Iteration 3302, loss = 0.56543665\n",
      "Iteration 3303, loss = 0.56531541\n",
      "Iteration 3304, loss = 0.56519408\n",
      "Iteration 3305, loss = 0.56507266\n",
      "Iteration 3306, loss = 0.56495116\n",
      "Iteration 3307, loss = 0.56482956\n",
      "Iteration 3308, loss = 0.56470788\n",
      "Iteration 3309, loss = 0.56458611\n",
      "Iteration 3310, loss = 0.56446425\n",
      "Iteration 3311, loss = 0.56434230\n",
      "Iteration 3312, loss = 0.56422026\n",
      "Iteration 3313, loss = 0.56409813\n",
      "Iteration 3314, loss = 0.56397591\n",
      "Iteration 3315, loss = 0.56385361\n",
      "Iteration 3316, loss = 0.56373121\n",
      "Iteration 3317, loss = 0.56360873\n",
      "Iteration 3318, loss = 0.56348615\n",
      "Iteration 3319, loss = 0.56336349\n",
      "Iteration 3320, loss = 0.56324074\n",
      "Iteration 3321, loss = 0.56311790\n",
      "Iteration 3322, loss = 0.56299497\n",
      "Iteration 3323, loss = 0.56287196\n",
      "Iteration 3324, loss = 0.56274885\n",
      "Iteration 3325, loss = 0.56262565\n",
      "Iteration 3326, loss = 0.56250237\n",
      "Iteration 3327, loss = 0.56237900\n",
      "Iteration 3328, loss = 0.56225553\n",
      "Iteration 3329, loss = 0.56213198\n",
      "Iteration 3330, loss = 0.56200834\n",
      "Iteration 3331, loss = 0.56188461\n",
      "Iteration 3332, loss = 0.56176080\n",
      "Iteration 3333, loss = 0.56163689\n",
      "Iteration 3334, loss = 0.56151289\n",
      "Iteration 3335, loss = 0.56138881\n",
      "Iteration 3336, loss = 0.56126464\n",
      "Iteration 3337, loss = 0.56114037\n",
      "Iteration 3338, loss = 0.56101602\n",
      "Iteration 3339, loss = 0.56089158\n",
      "Iteration 3340, loss = 0.56076705\n",
      "Iteration 3341, loss = 0.56064243\n",
      "Iteration 3342, loss = 0.56051773\n",
      "Iteration 3343, loss = 0.56039293\n",
      "Iteration 3344, loss = 0.56026805\n",
      "Iteration 3345, loss = 0.56014307\n",
      "Iteration 3346, loss = 0.56001801\n",
      "Iteration 3347, loss = 0.55989286\n",
      "Iteration 3348, loss = 0.55976762\n",
      "Iteration 3349, loss = 0.55964229\n",
      "Iteration 3350, loss = 0.55951687\n",
      "Iteration 3351, loss = 0.55939136\n",
      "Iteration 3352, loss = 0.55926577\n",
      "Iteration 3353, loss = 0.55914008\n",
      "Iteration 3354, loss = 0.55901431\n",
      "Iteration 3355, loss = 0.55888844\n",
      "Iteration 3356, loss = 0.55876249\n",
      "Iteration 3357, loss = 0.55863645\n",
      "Iteration 3358, loss = 0.55851032\n",
      "Iteration 3359, loss = 0.55838410\n",
      "Iteration 3360, loss = 0.55825780\n",
      "Iteration 3361, loss = 0.55813140\n",
      "Iteration 3362, loss = 0.55800492\n",
      "Iteration 3363, loss = 0.55787834\n",
      "Iteration 3364, loss = 0.55775168\n",
      "Iteration 3365, loss = 0.55762493\n",
      "Iteration 3366, loss = 0.55749809\n",
      "Iteration 3367, loss = 0.55737116\n",
      "Iteration 3368, loss = 0.55724414\n",
      "Iteration 3369, loss = 0.55711703\n",
      "Iteration 3370, loss = 0.55698984\n",
      "Iteration 3371, loss = 0.55686256\n",
      "Iteration 3372, loss = 0.55673518\n",
      "Iteration 3373, loss = 0.55660772\n",
      "Iteration 3374, loss = 0.55648017\n",
      "Iteration 3375, loss = 0.55635253\n",
      "Iteration 3376, loss = 0.55622480\n",
      "Iteration 3377, loss = 0.55609699\n",
      "Iteration 3378, loss = 0.55596908\n",
      "Iteration 3379, loss = 0.55584109\n",
      "Iteration 3380, loss = 0.55571300\n",
      "Iteration 3381, loss = 0.55558483\n",
      "Iteration 3382, loss = 0.55545657\n",
      "Iteration 3383, loss = 0.55532822\n",
      "Iteration 3384, loss = 0.55519978\n",
      "Iteration 3385, loss = 0.55507126\n",
      "Iteration 3386, loss = 0.55494264\n",
      "Iteration 3387, loss = 0.55481394\n",
      "Iteration 3388, loss = 0.55468515\n",
      "Iteration 3389, loss = 0.55455627\n",
      "Iteration 3390, loss = 0.55442730\n",
      "Iteration 3391, loss = 0.55429824\n",
      "Iteration 3392, loss = 0.55416909\n",
      "Iteration 3393, loss = 0.55403985\n",
      "Iteration 3394, loss = 0.55391053\n",
      "Iteration 3395, loss = 0.55378112\n",
      "Iteration 3396, loss = 0.55365162\n",
      "Iteration 3397, loss = 0.55352203\n",
      "Iteration 3398, loss = 0.55339235\n",
      "Iteration 3399, loss = 0.55326258\n",
      "Iteration 3400, loss = 0.55313272\n",
      "Iteration 3401, loss = 0.55300278\n",
      "Iteration 3402, loss = 0.55287275\n",
      "Iteration 3403, loss = 0.55274263\n",
      "Iteration 3404, loss = 0.55261242\n",
      "Iteration 3405, loss = 0.55248212\n",
      "Iteration 3406, loss = 0.55235173\n",
      "Iteration 3407, loss = 0.55222126\n",
      "Iteration 3408, loss = 0.55209069\n",
      "Iteration 3409, loss = 0.55196004\n",
      "Iteration 3410, loss = 0.55182930\n",
      "Iteration 3411, loss = 0.55169847\n",
      "Iteration 3412, loss = 0.55156755\n",
      "Iteration 3413, loss = 0.55143655\n",
      "Iteration 3414, loss = 0.55130545\n",
      "Iteration 3415, loss = 0.55117427\n",
      "Iteration 3416, loss = 0.55104300\n",
      "Iteration 3417, loss = 0.55091164\n",
      "Iteration 3418, loss = 0.55078019\n",
      "Iteration 3419, loss = 0.55064866\n",
      "Iteration 3420, loss = 0.55051703\n",
      "Iteration 3421, loss = 0.55038532\n",
      "Iteration 3422, loss = 0.55025352\n",
      "Iteration 3423, loss = 0.55012163\n",
      "Iteration 3424, loss = 0.54998965\n",
      "Iteration 3425, loss = 0.54985759\n",
      "Iteration 3426, loss = 0.54972543\n",
      "Iteration 3427, loss = 0.54959319\n",
      "Iteration 3428, loss = 0.54946086\n",
      "Iteration 3429, loss = 0.54932844\n",
      "Iteration 3430, loss = 0.54919594\n",
      "Iteration 3431, loss = 0.54906334\n",
      "Iteration 3432, loss = 0.54893066\n",
      "Iteration 3433, loss = 0.54879789\n",
      "Iteration 3434, loss = 0.54866503\n",
      "Iteration 3435, loss = 0.54853208\n",
      "Iteration 3436, loss = 0.54839905\n",
      "Iteration 3437, loss = 0.54826592\n",
      "Iteration 3438, loss = 0.54813271\n",
      "Iteration 3439, loss = 0.54799941\n",
      "Iteration 3440, loss = 0.54786603\n",
      "Iteration 3441, loss = 0.54773255\n",
      "Iteration 3442, loss = 0.54759899\n",
      "Iteration 3443, loss = 0.54746534\n",
      "Iteration 3444, loss = 0.54733160\n",
      "Iteration 3445, loss = 0.54719777\n",
      "Iteration 3446, loss = 0.54706386\n",
      "Iteration 3447, loss = 0.54692985\n",
      "Iteration 3448, loss = 0.54679576\n",
      "Iteration 3449, loss = 0.54666158\n",
      "Iteration 3450, loss = 0.54652732\n",
      "Iteration 3451, loss = 0.54639296\n",
      "Iteration 3452, loss = 0.54625852\n",
      "Iteration 3453, loss = 0.54612399\n",
      "Iteration 3454, loss = 0.54598937\n",
      "Iteration 3455, loss = 0.54585467\n",
      "Iteration 3456, loss = 0.54571988\n",
      "Iteration 3457, loss = 0.54558500\n",
      "Iteration 3458, loss = 0.54545003\n",
      "Iteration 3459, loss = 0.54531497\n",
      "Iteration 3460, loss = 0.54517983\n",
      "Iteration 3461, loss = 0.54504460\n",
      "Iteration 3462, loss = 0.54490928\n",
      "Iteration 3463, loss = 0.54477387\n",
      "Iteration 3464, loss = 0.54463838\n",
      "Iteration 3465, loss = 0.54450280\n",
      "Iteration 3466, loss = 0.54436713\n",
      "Iteration 3467, loss = 0.54423137\n",
      "Iteration 3468, loss = 0.54409553\n",
      "Iteration 3469, loss = 0.54395960\n",
      "Iteration 3470, loss = 0.54382358\n",
      "Iteration 3471, loss = 0.54368747\n",
      "Iteration 3472, loss = 0.54355128\n",
      "Iteration 3473, loss = 0.54341500\n",
      "Iteration 3474, loss = 0.54327863\n",
      "Iteration 3475, loss = 0.54314217\n",
      "Iteration 3476, loss = 0.54300563\n",
      "Iteration 3477, loss = 0.54286900\n",
      "Iteration 3478, loss = 0.54273228\n",
      "Iteration 3479, loss = 0.54259548\n",
      "Iteration 3480, loss = 0.54245858\n",
      "Iteration 3481, loss = 0.54232160\n",
      "Iteration 3482, loss = 0.54218454\n",
      "Iteration 3483, loss = 0.54204738\n",
      "Iteration 3484, loss = 0.54191014\n",
      "Iteration 3485, loss = 0.54177282\n",
      "Iteration 3486, loss = 0.54163540\n",
      "Iteration 3487, loss = 0.54149790\n",
      "Iteration 3488, loss = 0.54136031\n",
      "Iteration 3489, loss = 0.54122264\n",
      "Iteration 3490, loss = 0.54108487\n",
      "Iteration 3491, loss = 0.54094702\n",
      "Iteration 3492, loss = 0.54080909\n",
      "Iteration 3493, loss = 0.54067106\n",
      "Iteration 3494, loss = 0.54053295\n",
      "Iteration 3495, loss = 0.54039475\n",
      "Iteration 3496, loss = 0.54025647\n",
      "Iteration 3497, loss = 0.54011810\n",
      "Iteration 3498, loss = 0.53997964\n",
      "Iteration 3499, loss = 0.53984110\n",
      "Iteration 3500, loss = 0.53970247\n",
      "Iteration 3501, loss = 0.53956375\n",
      "Iteration 3502, loss = 0.53942494\n",
      "Iteration 3503, loss = 0.53928605\n",
      "Iteration 3504, loss = 0.53914708\n",
      "Iteration 3505, loss = 0.53900801\n",
      "Iteration 3506, loss = 0.53886886\n",
      "Iteration 3507, loss = 0.53872962\n",
      "Iteration 3508, loss = 0.53859030\n",
      "Iteration 3509, loss = 0.53845089\n",
      "Iteration 3510, loss = 0.53831139\n",
      "Iteration 3511, loss = 0.53817181\n",
      "Iteration 3512, loss = 0.53803214\n",
      "Iteration 3513, loss = 0.53789238\n",
      "Iteration 3514, loss = 0.53775254\n",
      "Iteration 3515, loss = 0.53761261\n",
      "Iteration 3516, loss = 0.53747260\n",
      "Iteration 3517, loss = 0.53733250\n",
      "Iteration 3518, loss = 0.53719231\n",
      "Iteration 3519, loss = 0.53705204\n",
      "Iteration 3520, loss = 0.53691168\n",
      "Iteration 3521, loss = 0.53677123\n",
      "Iteration 3522, loss = 0.53663070\n",
      "Iteration 3523, loss = 0.53649008\n",
      "Iteration 3524, loss = 0.53634938\n",
      "Iteration 3525, loss = 0.53620859\n",
      "Iteration 3526, loss = 0.53606771\n",
      "Iteration 3527, loss = 0.53592675\n",
      "Iteration 3528, loss = 0.53578570\n",
      "Iteration 3529, loss = 0.53564457\n",
      "Iteration 3530, loss = 0.53550335\n",
      "Iteration 3531, loss = 0.53536204\n",
      "Iteration 3532, loss = 0.53522065\n",
      "Iteration 3533, loss = 0.53507918\n",
      "Iteration 3534, loss = 0.53493761\n",
      "Iteration 3535, loss = 0.53479597\n",
      "Iteration 3536, loss = 0.53465423\n",
      "Iteration 3537, loss = 0.53451241\n",
      "Iteration 3538, loss = 0.53437051\n",
      "Iteration 3539, loss = 0.53422852\n",
      "Iteration 3540, loss = 0.53408644\n",
      "Iteration 3541, loss = 0.53394428\n",
      "Iteration 3542, loss = 0.53380203\n",
      "Iteration 3543, loss = 0.53365970\n",
      "Iteration 3544, loss = 0.53351728\n",
      "Iteration 3545, loss = 0.53337478\n",
      "Iteration 3546, loss = 0.53323219\n",
      "Iteration 3547, loss = 0.53308952\n",
      "Iteration 3548, loss = 0.53294676\n",
      "Iteration 3549, loss = 0.53280392\n",
      "Iteration 3550, loss = 0.53266099\n",
      "Iteration 3551, loss = 0.53251797\n",
      "Iteration 3552, loss = 0.53237487\n",
      "Iteration 3553, loss = 0.53223169\n",
      "Iteration 3554, loss = 0.53208842\n",
      "Iteration 3555, loss = 0.53194506\n",
      "Iteration 3556, loss = 0.53180162\n",
      "Iteration 3557, loss = 0.53165810\n",
      "Iteration 3558, loss = 0.53151449\n",
      "Iteration 3559, loss = 0.53137080\n",
      "Iteration 3560, loss = 0.53122702\n",
      "Iteration 3561, loss = 0.53108315\n",
      "Iteration 3562, loss = 0.53093921\n",
      "Iteration 3563, loss = 0.53079517\n",
      "Iteration 3564, loss = 0.53065105\n",
      "Iteration 3565, loss = 0.53050685\n",
      "Iteration 3566, loss = 0.53036256\n",
      "Iteration 3567, loss = 0.53021819\n",
      "Iteration 3568, loss = 0.53007374\n",
      "Iteration 3569, loss = 0.52992920\n",
      "Iteration 3570, loss = 0.52978457\n",
      "Iteration 3571, loss = 0.52963986\n",
      "Iteration 3572, loss = 0.52949507\n",
      "Iteration 3573, loss = 0.52935019\n",
      "Iteration 3574, loss = 0.52920523\n",
      "Iteration 3575, loss = 0.52906018\n",
      "Iteration 3576, loss = 0.52891505\n",
      "Iteration 3577, loss = 0.52876984\n",
      "Iteration 3578, loss = 0.52862454\n",
      "Iteration 3579, loss = 0.52847916\n",
      "Iteration 3580, loss = 0.52833369\n",
      "Iteration 3581, loss = 0.52818814\n",
      "Iteration 3582, loss = 0.52804250\n",
      "Iteration 3583, loss = 0.52789678\n",
      "Iteration 3584, loss = 0.52775098\n",
      "Iteration 3585, loss = 0.52760509\n",
      "Iteration 3586, loss = 0.52745912\n",
      "Iteration 3587, loss = 0.52731307\n",
      "Iteration 3588, loss = 0.52716693\n",
      "Iteration 3589, loss = 0.52702071\n",
      "Iteration 3590, loss = 0.52687441\n",
      "Iteration 3591, loss = 0.52672802\n",
      "Iteration 3592, loss = 0.52658155\n",
      "Iteration 3593, loss = 0.52643499\n",
      "Iteration 3594, loss = 0.52628835\n",
      "Iteration 3595, loss = 0.52614163\n",
      "Iteration 3596, loss = 0.52599482\n",
      "Iteration 3597, loss = 0.52584794\n",
      "Iteration 3598, loss = 0.52570096\n",
      "Iteration 3599, loss = 0.52555391\n",
      "Iteration 3600, loss = 0.52540677\n",
      "Iteration 3601, loss = 0.52525955\n",
      "Iteration 3602, loss = 0.52511224\n",
      "Iteration 3603, loss = 0.52496486\n",
      "Iteration 3604, loss = 0.52481739\n",
      "Iteration 3605, loss = 0.52466983\n",
      "Iteration 3606, loss = 0.52452220\n",
      "Iteration 3607, loss = 0.52437448\n",
      "Iteration 3608, loss = 0.52422668\n",
      "Iteration 3609, loss = 0.52407879\n",
      "Iteration 3610, loss = 0.52393082\n",
      "Iteration 3611, loss = 0.52378277\n",
      "Iteration 3612, loss = 0.52363464\n",
      "Iteration 3613, loss = 0.52348643\n",
      "Iteration 3614, loss = 0.52333813\n",
      "Iteration 3615, loss = 0.52318975\n",
      "Iteration 3616, loss = 0.52304128\n",
      "Iteration 3617, loss = 0.52289274\n",
      "Iteration 3618, loss = 0.52274411\n",
      "Iteration 3619, loss = 0.52259540\n",
      "Iteration 3620, loss = 0.52244661\n",
      "Iteration 3621, loss = 0.52229774\n",
      "Iteration 3622, loss = 0.52214878\n",
      "Iteration 3623, loss = 0.52199974\n",
      "Iteration 3624, loss = 0.52185062\n",
      "Iteration 3625, loss = 0.52170142\n",
      "Iteration 3626, loss = 0.52155214\n",
      "Iteration 3627, loss = 0.52140277\n",
      "Iteration 3628, loss = 0.52125332\n",
      "Iteration 3629, loss = 0.52110379\n",
      "Iteration 3630, loss = 0.52095418\n",
      "Iteration 3631, loss = 0.52080449\n",
      "Iteration 3632, loss = 0.52065472\n",
      "Iteration 3633, loss = 0.52050486\n",
      "Iteration 3634, loss = 0.52035492\n",
      "Iteration 3635, loss = 0.52020490\n",
      "Iteration 3636, loss = 0.52005480\n",
      "Iteration 3637, loss = 0.51990462\n",
      "Iteration 3638, loss = 0.51975436\n",
      "Iteration 3639, loss = 0.51960401\n",
      "Iteration 3640, loss = 0.51945359\n",
      "Iteration 3641, loss = 0.51930308\n",
      "Iteration 3642, loss = 0.51915250\n",
      "Iteration 3643, loss = 0.51900183\n",
      "Iteration 3644, loss = 0.51885108\n",
      "Iteration 3645, loss = 0.51870025\n",
      "Iteration 3646, loss = 0.51854934\n",
      "Iteration 3647, loss = 0.51839834\n",
      "Iteration 3648, loss = 0.51824727\n",
      "Iteration 3649, loss = 0.51809612\n",
      "Iteration 3650, loss = 0.51794488\n",
      "Iteration 3651, loss = 0.51779357\n",
      "Iteration 3652, loss = 0.51764217\n",
      "Iteration 3653, loss = 0.51749070\n",
      "Iteration 3654, loss = 0.51733914\n",
      "Iteration 3655, loss = 0.51718750\n",
      "Iteration 3656, loss = 0.51703579\n",
      "Iteration 3657, loss = 0.51688399\n",
      "Iteration 3658, loss = 0.51673211\n",
      "Iteration 3659, loss = 0.51658016\n",
      "Iteration 3660, loss = 0.51642812\n",
      "Iteration 3661, loss = 0.51627600\n",
      "Iteration 3662, loss = 0.51612380\n",
      "Iteration 3663, loss = 0.51597153\n",
      "Iteration 3664, loss = 0.51581917\n",
      "Iteration 3665, loss = 0.51566673\n",
      "Iteration 3666, loss = 0.51551421\n",
      "Iteration 3667, loss = 0.51536162\n",
      "Iteration 3668, loss = 0.51520894\n",
      "Iteration 3669, loss = 0.51505619\n",
      "Iteration 3670, loss = 0.51490335\n",
      "Iteration 3671, loss = 0.51475044\n",
      "Iteration 3672, loss = 0.51459744\n",
      "Iteration 3673, loss = 0.51444437\n",
      "Iteration 3674, loss = 0.51429122\n",
      "Iteration 3675, loss = 0.51413799\n",
      "Iteration 3676, loss = 0.51398468\n",
      "Iteration 3677, loss = 0.51383129\n",
      "Iteration 3678, loss = 0.51367782\n",
      "Iteration 3679, loss = 0.51352427\n",
      "Iteration 3680, loss = 0.51337064\n",
      "Iteration 3681, loss = 0.51321694\n",
      "Iteration 3682, loss = 0.51306315\n",
      "Iteration 3683, loss = 0.51290929\n",
      "Iteration 3684, loss = 0.51275535\n",
      "Iteration 3685, loss = 0.51260133\n",
      "Iteration 3686, loss = 0.51244723\n",
      "Iteration 3687, loss = 0.51229305\n",
      "Iteration 3688, loss = 0.51213880\n",
      "Iteration 3689, loss = 0.51198446\n",
      "Iteration 3690, loss = 0.51183005\n",
      "Iteration 3691, loss = 0.51167556\n",
      "Iteration 3692, loss = 0.51152099\n",
      "Iteration 3693, loss = 0.51136635\n",
      "Iteration 3694, loss = 0.51121162\n",
      "Iteration 3695, loss = 0.51105682\n",
      "Iteration 3696, loss = 0.51090194\n",
      "Iteration 3697, loss = 0.51074698\n",
      "Iteration 3698, loss = 0.51059195\n",
      "Iteration 3699, loss = 0.51043684\n",
      "Iteration 3700, loss = 0.51028164\n",
      "Iteration 3701, loss = 0.51012638\n",
      "Iteration 3702, loss = 0.50997103\n",
      "Iteration 3703, loss = 0.50981561\n",
      "Iteration 3704, loss = 0.50966011\n",
      "Iteration 3705, loss = 0.50950453\n",
      "Iteration 3706, loss = 0.50934887\n",
      "Iteration 3707, loss = 0.50919314\n",
      "Iteration 3708, loss = 0.50903733\n",
      "Iteration 3709, loss = 0.50888145\n",
      "Iteration 3710, loss = 0.50872548\n",
      "Iteration 3711, loss = 0.50856944\n",
      "Iteration 3712, loss = 0.50841333\n",
      "Iteration 3713, loss = 0.50825714\n",
      "Iteration 3714, loss = 0.50810087\n",
      "Iteration 3715, loss = 0.50794452\n",
      "Iteration 3716, loss = 0.50778810\n",
      "Iteration 3717, loss = 0.50763160\n",
      "Iteration 3718, loss = 0.50747502\n",
      "Iteration 3719, loss = 0.50731837\n",
      "Iteration 3720, loss = 0.50716164\n",
      "Iteration 3721, loss = 0.50700484\n",
      "Iteration 3722, loss = 0.50684796\n",
      "Iteration 3723, loss = 0.50669100\n",
      "Iteration 3724, loss = 0.50653397\n",
      "Iteration 3725, loss = 0.50637686\n",
      "Iteration 3726, loss = 0.50621968\n",
      "Iteration 3727, loss = 0.50606242\n",
      "Iteration 3728, loss = 0.50590508\n",
      "Iteration 3729, loss = 0.50574767\n",
      "Iteration 3730, loss = 0.50559018\n",
      "Iteration 3731, loss = 0.50543262\n",
      "Iteration 3732, loss = 0.50527498\n",
      "Iteration 3733, loss = 0.50511727\n",
      "Iteration 3734, loss = 0.50495948\n",
      "Iteration 3735, loss = 0.50480162\n",
      "Iteration 3736, loss = 0.50464368\n",
      "Iteration 3737, loss = 0.50448567\n",
      "Iteration 3738, loss = 0.50432758\n",
      "Iteration 3739, loss = 0.50416942\n",
      "Iteration 3740, loss = 0.50401119\n",
      "Iteration 3741, loss = 0.50385287\n",
      "Iteration 3742, loss = 0.50369449\n",
      "Iteration 3743, loss = 0.50353603\n",
      "Iteration 3744, loss = 0.50337749\n",
      "Iteration 3745, loss = 0.50321888\n",
      "Iteration 3746, loss = 0.50306020\n",
      "Iteration 3747, loss = 0.50290144\n",
      "Iteration 3748, loss = 0.50274261\n",
      "Iteration 3749, loss = 0.50258370\n",
      "Iteration 3750, loss = 0.50242472\n",
      "Iteration 3751, loss = 0.50226567\n",
      "Iteration 3752, loss = 0.50210654\n",
      "Iteration 3753, loss = 0.50194734\n",
      "Iteration 3754, loss = 0.50178806\n",
      "Iteration 3755, loss = 0.50162871\n",
      "Iteration 3756, loss = 0.50146929\n",
      "Iteration 3757, loss = 0.50130979\n",
      "Iteration 3758, loss = 0.50115023\n",
      "Iteration 3759, loss = 0.50099058\n",
      "Iteration 3760, loss = 0.50083087\n",
      "Iteration 3761, loss = 0.50067108\n",
      "Iteration 3762, loss = 0.50051122\n",
      "Iteration 3763, loss = 0.50035128\n",
      "Iteration 3764, loss = 0.50019127\n",
      "Iteration 3765, loss = 0.50003119\n",
      "Iteration 3766, loss = 0.49987104\n",
      "Iteration 3767, loss = 0.49971082\n",
      "Iteration 3768, loss = 0.49955052\n",
      "Iteration 3769, loss = 0.49939015\n",
      "Iteration 3770, loss = 0.49922970\n",
      "Iteration 3771, loss = 0.49906919\n",
      "Iteration 3772, loss = 0.49890860\n",
      "Iteration 3773, loss = 0.49874794\n",
      "Iteration 3774, loss = 0.49858721\n",
      "Iteration 3775, loss = 0.49842641\n",
      "Iteration 3776, loss = 0.49826553\n",
      "Iteration 3777, loss = 0.49810459\n",
      "Iteration 3778, loss = 0.49794357\n",
      "Iteration 3779, loss = 0.49778248\n",
      "Iteration 3780, loss = 0.49762131\n",
      "Iteration 3781, loss = 0.49746008\n",
      "Iteration 3782, loss = 0.49729878\n",
      "Iteration 3783, loss = 0.49713740\n",
      "Iteration 3784, loss = 0.49697595\n",
      "Iteration 3785, loss = 0.49681444\n",
      "Iteration 3786, loss = 0.49665285\n",
      "Iteration 3787, loss = 0.49649119\n",
      "Iteration 3788, loss = 0.49632946\n",
      "Iteration 3789, loss = 0.49616765\n",
      "Iteration 3790, loss = 0.49600578\n",
      "Iteration 3791, loss = 0.49584384\n",
      "Iteration 3792, loss = 0.49568183\n",
      "Iteration 3793, loss = 0.49551974\n",
      "Iteration 3794, loss = 0.49535759\n",
      "Iteration 3795, loss = 0.49519536\n",
      "Iteration 3796, loss = 0.49503307\n",
      "Iteration 3797, loss = 0.49487071\n",
      "Iteration 3798, loss = 0.49470827\n",
      "Iteration 3799, loss = 0.49454577\n",
      "Iteration 3800, loss = 0.49438319\n",
      "Iteration 3801, loss = 0.49422055\n",
      "Iteration 3802, loss = 0.49405784\n",
      "Iteration 3803, loss = 0.49389505\n",
      "Iteration 3804, loss = 0.49373220\n",
      "Iteration 3805, loss = 0.49356928\n",
      "Iteration 3806, loss = 0.49340629\n",
      "Iteration 3807, loss = 0.49324323\n",
      "Iteration 3808, loss = 0.49308010\n",
      "Iteration 3809, loss = 0.49291690\n",
      "Iteration 3810, loss = 0.49275364\n",
      "Iteration 3811, loss = 0.49259030\n",
      "Iteration 3812, loss = 0.49242690\n",
      "Iteration 3813, loss = 0.49226343\n",
      "Iteration 3814, loss = 0.49209988\n",
      "Iteration 3815, loss = 0.49193628\n",
      "Iteration 3816, loss = 0.49177260\n",
      "Iteration 3817, loss = 0.49160885\n",
      "Iteration 3818, loss = 0.49144504\n",
      "Iteration 3819, loss = 0.49128116\n",
      "Iteration 3820, loss = 0.49111721\n",
      "Iteration 3821, loss = 0.49095319\n",
      "Iteration 3822, loss = 0.49078910\n",
      "Iteration 3823, loss = 0.49062495\n",
      "Iteration 3824, loss = 0.49046073\n",
      "Iteration 3825, loss = 0.49029644\n",
      "Iteration 3826, loss = 0.49013209\n",
      "Iteration 3827, loss = 0.48996766\n",
      "Iteration 3828, loss = 0.48980317\n",
      "Iteration 3829, loss = 0.48963862\n",
      "Iteration 3830, loss = 0.48947399\n",
      "Iteration 3831, loss = 0.48930930\n",
      "Iteration 3832, loss = 0.48914455\n",
      "Iteration 3833, loss = 0.48897972\n",
      "Iteration 3834, loss = 0.48881483\n",
      "Iteration 3835, loss = 0.48864987\n",
      "Iteration 3836, loss = 0.48848485\n",
      "Iteration 3837, loss = 0.48831976\n",
      "Iteration 3838, loss = 0.48815461\n",
      "Iteration 3839, loss = 0.48798938\n",
      "Iteration 3840, loss = 0.48782410\n",
      "Iteration 3841, loss = 0.48765874\n",
      "Iteration 3842, loss = 0.48749332\n",
      "Iteration 3843, loss = 0.48732784\n",
      "Iteration 3844, loss = 0.48716229\n",
      "Iteration 3845, loss = 0.48699667\n",
      "Iteration 3846, loss = 0.48683099\n",
      "Iteration 3847, loss = 0.48666524\n",
      "Iteration 3848, loss = 0.48649943\n",
      "Iteration 3849, loss = 0.48633355\n",
      "Iteration 3850, loss = 0.48616761\n",
      "Iteration 3851, loss = 0.48600161\n",
      "Iteration 3852, loss = 0.48583553\n",
      "Iteration 3853, loss = 0.48566940\n",
      "Iteration 3854, loss = 0.48550320\n",
      "Iteration 3855, loss = 0.48533693\n",
      "Iteration 3856, loss = 0.48517060\n",
      "Iteration 3857, loss = 0.48500421\n",
      "Iteration 3858, loss = 0.48483775\n",
      "Iteration 3859, loss = 0.48467123\n",
      "Iteration 3860, loss = 0.48450464\n",
      "Iteration 3861, loss = 0.48433799\n",
      "Iteration 3862, loss = 0.48417128\n",
      "Iteration 3863, loss = 0.48400450\n",
      "Iteration 3864, loss = 0.48383766\n",
      "Iteration 3865, loss = 0.48367076\n",
      "Iteration 3866, loss = 0.48350379\n",
      "Iteration 3867, loss = 0.48333676\n",
      "Iteration 3868, loss = 0.48316967\n",
      "Iteration 3869, loss = 0.48300251\n",
      "Iteration 3870, loss = 0.48283529\n",
      "Iteration 3871, loss = 0.48266801\n",
      "Iteration 3872, loss = 0.48250067\n",
      "Iteration 3873, loss = 0.48233326\n",
      "Iteration 3874, loss = 0.48216579\n",
      "Iteration 3875, loss = 0.48199826\n",
      "Iteration 3876, loss = 0.48183066\n",
      "Iteration 3877, loss = 0.48166301\n",
      "Iteration 3878, loss = 0.48149529\n",
      "Iteration 3879, loss = 0.48132751\n",
      "Iteration 3880, loss = 0.48115967\n",
      "Iteration 3881, loss = 0.48099176\n",
      "Iteration 3882, loss = 0.48082380\n",
      "Iteration 3883, loss = 0.48065577\n",
      "Iteration 3884, loss = 0.48048769\n",
      "Iteration 3885, loss = 0.48031954\n",
      "Iteration 3886, loss = 0.48015133\n",
      "Iteration 3887, loss = 0.47998305\n",
      "Iteration 3888, loss = 0.47981472\n",
      "Iteration 3889, loss = 0.47964633\n",
      "Iteration 3890, loss = 0.47947788\n",
      "Iteration 3891, loss = 0.47930936\n",
      "Iteration 3892, loss = 0.47914079\n",
      "Iteration 3893, loss = 0.47897215\n",
      "Iteration 3894, loss = 0.47880346\n",
      "Iteration 3895, loss = 0.47863470\n",
      "Iteration 3896, loss = 0.47846589\n",
      "Iteration 3897, loss = 0.47829701\n",
      "Iteration 3898, loss = 0.47812807\n",
      "Iteration 3899, loss = 0.47795908\n",
      "Iteration 3900, loss = 0.47779003\n",
      "Iteration 3901, loss = 0.47762091\n",
      "Iteration 3902, loss = 0.47745174\n",
      "Iteration 3903, loss = 0.47728251\n",
      "Iteration 3904, loss = 0.47711321\n",
      "Iteration 3905, loss = 0.47694386\n",
      "Iteration 3906, loss = 0.47677446\n",
      "Iteration 3907, loss = 0.47660499\n",
      "Iteration 3908, loss = 0.47643546\n",
      "Iteration 3909, loss = 0.47626588\n",
      "Iteration 3910, loss = 0.47609623\n",
      "Iteration 3911, loss = 0.47592653\n",
      "Iteration 3912, loss = 0.47575677\n",
      "Iteration 3913, loss = 0.47558695\n",
      "Iteration 3914, loss = 0.47541708\n",
      "Iteration 3915, loss = 0.47524714\n",
      "Iteration 3916, loss = 0.47507715\n",
      "Iteration 3917, loss = 0.47490710\n",
      "Iteration 3918, loss = 0.47473700\n",
      "Iteration 3919, loss = 0.47456683\n",
      "Iteration 3920, loss = 0.47439661\n",
      "Iteration 3921, loss = 0.47422633\n",
      "Iteration 3922, loss = 0.47405600\n",
      "Iteration 3923, loss = 0.47388560\n",
      "Iteration 3924, loss = 0.47371516\n",
      "Iteration 3925, loss = 0.47354465\n",
      "Iteration 3926, loss = 0.47337409\n",
      "Iteration 3927, loss = 0.47320347\n",
      "Iteration 3928, loss = 0.47303280\n",
      "Iteration 3929, loss = 0.47286206\n",
      "Iteration 3930, loss = 0.47269128\n",
      "Iteration 3931, loss = 0.47252043\n",
      "Iteration 3932, loss = 0.47234954\n",
      "Iteration 3933, loss = 0.47217858\n",
      "Iteration 3934, loss = 0.47200757\n",
      "Iteration 3935, loss = 0.47183651\n",
      "Iteration 3936, loss = 0.47166539\n",
      "Iteration 3937, loss = 0.47149421\n",
      "Iteration 3938, loss = 0.47132298\n",
      "Iteration 3939, loss = 0.47115169\n",
      "Iteration 3940, loss = 0.47098035\n",
      "Iteration 3941, loss = 0.47080896\n",
      "Iteration 3942, loss = 0.47063751\n",
      "Iteration 3943, loss = 0.47046600\n",
      "Iteration 3944, loss = 0.47029445\n",
      "Iteration 3945, loss = 0.47012283\n",
      "Iteration 3946, loss = 0.46995117\n",
      "Iteration 3947, loss = 0.46977945\n",
      "Iteration 3948, loss = 0.46960767\n",
      "Iteration 3949, loss = 0.46943584\n",
      "Iteration 3950, loss = 0.46926396\n",
      "Iteration 3951, loss = 0.46909203\n",
      "Iteration 3952, loss = 0.46892004\n",
      "Iteration 3953, loss = 0.46874799\n",
      "Iteration 3954, loss = 0.46857590\n",
      "Iteration 3955, loss = 0.46840375\n",
      "Iteration 3956, loss = 0.46823155\n",
      "Iteration 3957, loss = 0.46805930\n",
      "Iteration 3958, loss = 0.46788699\n",
      "Iteration 3959, loss = 0.46771463\n",
      "Iteration 3960, loss = 0.46754222\n",
      "Iteration 3961, loss = 0.46736976\n",
      "Iteration 3962, loss = 0.46719725\n",
      "Iteration 3963, loss = 0.46702468\n",
      "Iteration 3964, loss = 0.46685206\n",
      "Iteration 3965, loss = 0.46667939\n",
      "Iteration 3966, loss = 0.46650667\n",
      "Iteration 3967, loss = 0.46633389\n",
      "Iteration 3968, loss = 0.46616107\n",
      "Iteration 3969, loss = 0.46598819\n",
      "Iteration 3970, loss = 0.46581527\n",
      "Iteration 3971, loss = 0.46564229\n",
      "Iteration 3972, loss = 0.46546926\n",
      "Iteration 3973, loss = 0.46529618\n",
      "Iteration 3974, loss = 0.46512305\n",
      "Iteration 3975, loss = 0.46494987\n",
      "Iteration 3976, loss = 0.46477664\n",
      "Iteration 3977, loss = 0.46460336\n",
      "Iteration 3978, loss = 0.46443003\n",
      "Iteration 3979, loss = 0.46425665\n",
      "Iteration 3980, loss = 0.46408322\n",
      "Iteration 3981, loss = 0.46390974\n",
      "Iteration 3982, loss = 0.46373621\n",
      "Iteration 3983, loss = 0.46356264\n",
      "Iteration 3984, loss = 0.46338901\n",
      "Iteration 3985, loss = 0.46321533\n",
      "Iteration 3986, loss = 0.46304161\n",
      "Iteration 3987, loss = 0.46286783\n",
      "Iteration 3988, loss = 0.46269401\n",
      "Iteration 3989, loss = 0.46252014\n",
      "Iteration 3990, loss = 0.46234622\n",
      "Iteration 3991, loss = 0.46217225\n",
      "Iteration 3992, loss = 0.46199823\n",
      "Iteration 3993, loss = 0.46182417\n",
      "Iteration 3994, loss = 0.46165006\n",
      "Iteration 3995, loss = 0.46147590\n",
      "Iteration 3996, loss = 0.46130169\n",
      "Iteration 3997, loss = 0.46112744\n",
      "Iteration 3998, loss = 0.46095313\n",
      "Iteration 3999, loss = 0.46077878\n",
      "Iteration 4000, loss = 0.46060439\n",
      "Iteration 4001, loss = 0.46042994\n",
      "Iteration 4002, loss = 0.46025545\n",
      "Iteration 4003, loss = 0.46008092\n",
      "Iteration 4004, loss = 0.45990633\n",
      "Iteration 4005, loss = 0.45973170\n",
      "Iteration 4006, loss = 0.45955703\n",
      "Iteration 4007, loss = 0.45938231\n",
      "Iteration 4008, loss = 0.45920754\n",
      "Iteration 4009, loss = 0.45903272\n",
      "Iteration 4010, loss = 0.45885787\n",
      "Iteration 4011, loss = 0.45868296\n",
      "Iteration 4012, loss = 0.45850801\n",
      "Iteration 4013, loss = 0.45833301\n",
      "Iteration 4014, loss = 0.45815797\n",
      "Iteration 4015, loss = 0.45798289\n",
      "Iteration 4016, loss = 0.45780776\n",
      "Iteration 4017, loss = 0.45763258\n",
      "Iteration 4018, loss = 0.45745736\n",
      "Iteration 4019, loss = 0.45728210\n",
      "Iteration 4020, loss = 0.45710679\n",
      "Iteration 4021, loss = 0.45693144\n",
      "Iteration 4022, loss = 0.45675604\n",
      "Iteration 4023, loss = 0.45658060\n",
      "Iteration 4024, loss = 0.45640511\n",
      "Iteration 4025, loss = 0.45622959\n",
      "Iteration 4026, loss = 0.45605401\n",
      "Iteration 4027, loss = 0.45587840\n",
      "Iteration 4028, loss = 0.45570274\n",
      "Iteration 4029, loss = 0.45552704\n",
      "Iteration 4030, loss = 0.45535130\n",
      "Iteration 4031, loss = 0.45517551\n",
      "Iteration 4032, loss = 0.45499968\n",
      "Iteration 4033, loss = 0.45482381\n",
      "Iteration 4034, loss = 0.45464789\n",
      "Iteration 4035, loss = 0.45447194\n",
      "Iteration 4036, loss = 0.45429594\n",
      "Iteration 4037, loss = 0.45411990\n",
      "Iteration 4038, loss = 0.45394382\n",
      "Iteration 4039, loss = 0.45376769\n",
      "Iteration 4040, loss = 0.45359153\n",
      "Iteration 4041, loss = 0.45341532\n",
      "Iteration 4042, loss = 0.45323907\n",
      "Iteration 4043, loss = 0.45306278\n",
      "Iteration 4044, loss = 0.45288645\n",
      "Iteration 4045, loss = 0.45271008\n",
      "Iteration 4046, loss = 0.45253367\n",
      "Iteration 4047, loss = 0.45235722\n",
      "Iteration 4048, loss = 0.45218073\n",
      "Iteration 4049, loss = 0.45200420\n",
      "Iteration 4050, loss = 0.45182763\n",
      "Iteration 4051, loss = 0.45165101\n",
      "Iteration 4052, loss = 0.45147436\n",
      "Iteration 4053, loss = 0.45129767\n",
      "Iteration 4054, loss = 0.45112094\n",
      "Iteration 4055, loss = 0.45094417\n",
      "Iteration 4056, loss = 0.45076736\n",
      "Iteration 4057, loss = 0.45059051\n",
      "Iteration 4058, loss = 0.45041362\n",
      "Iteration 4059, loss = 0.45023670\n",
      "Iteration 4060, loss = 0.45005973\n",
      "Iteration 4061, loss = 0.44988273\n",
      "Iteration 4062, loss = 0.44970569\n",
      "Iteration 4063, loss = 0.44952861\n",
      "Iteration 4064, loss = 0.44935149\n",
      "Iteration 4065, loss = 0.44917434\n",
      "Iteration 4066, loss = 0.44899715\n",
      "Iteration 4067, loss = 0.44881991\n",
      "Iteration 4068, loss = 0.44864265\n",
      "Iteration 4069, loss = 0.44846534\n",
      "Iteration 4070, loss = 0.44828800\n",
      "Iteration 4071, loss = 0.44811062\n",
      "Iteration 4072, loss = 0.44793320\n",
      "Iteration 4073, loss = 0.44775575\n",
      "Iteration 4074, loss = 0.44757826\n",
      "Iteration 4075, loss = 0.44740074\n",
      "Iteration 4076, loss = 0.44722318\n",
      "Iteration 4077, loss = 0.44704558\n",
      "Iteration 4078, loss = 0.44686794\n",
      "Iteration 4079, loss = 0.44669027\n",
      "Iteration 4080, loss = 0.44651257\n",
      "Iteration 4081, loss = 0.44633483\n",
      "Iteration 4082, loss = 0.44615705\n",
      "Iteration 4083, loss = 0.44597924\n",
      "Iteration 4084, loss = 0.44580140\n",
      "Iteration 4085, loss = 0.44562351\n",
      "Iteration 4086, loss = 0.44544560\n",
      "Iteration 4087, loss = 0.44526765\n",
      "Iteration 4088, loss = 0.44508966\n",
      "Iteration 4089, loss = 0.44491164\n",
      "Iteration 4090, loss = 0.44473359\n",
      "Iteration 4091, loss = 0.44455550\n",
      "Iteration 4092, loss = 0.44437738\n",
      "Iteration 4093, loss = 0.44419923\n",
      "Iteration 4094, loss = 0.44402104\n",
      "Iteration 4095, loss = 0.44384282\n",
      "Iteration 4096, loss = 0.44366456\n",
      "Iteration 4097, loss = 0.44348628\n",
      "Iteration 4098, loss = 0.44330796\n",
      "Iteration 4099, loss = 0.44312960\n",
      "Iteration 4100, loss = 0.44295122\n",
      "Iteration 4101, loss = 0.44277280\n",
      "Iteration 4102, loss = 0.44259435\n",
      "Iteration 4103, loss = 0.44241587\n",
      "Iteration 4104, loss = 0.44223735\n",
      "Iteration 4105, loss = 0.44205880\n",
      "Iteration 4106, loss = 0.44188023\n",
      "Iteration 4107, loss = 0.44170162\n",
      "Iteration 4108, loss = 0.44152297\n",
      "Iteration 4109, loss = 0.44134430\n",
      "Iteration 4110, loss = 0.44116560\n",
      "Iteration 4111, loss = 0.44098686\n",
      "Iteration 4112, loss = 0.44080810\n",
      "Iteration 4113, loss = 0.44062930\n",
      "Iteration 4114, loss = 0.44045048\n",
      "Iteration 4115, loss = 0.44027162\n",
      "Iteration 4116, loss = 0.44009273\n",
      "Iteration 4117, loss = 0.43991382\n",
      "Iteration 4118, loss = 0.43973487\n",
      "Iteration 4119, loss = 0.43955589\n",
      "Iteration 4120, loss = 0.43937689\n",
      "Iteration 4121, loss = 0.43919785\n",
      "Iteration 4122, loss = 0.43901879\n",
      "Iteration 4123, loss = 0.43883970\n",
      "Iteration 4124, loss = 0.43866057\n",
      "Iteration 4125, loss = 0.43848142\n",
      "Iteration 4126, loss = 0.43830224\n",
      "Iteration 4127, loss = 0.43812303\n",
      "Iteration 4128, loss = 0.43794380\n",
      "Iteration 4129, loss = 0.43776453\n",
      "Iteration 4130, loss = 0.43758524\n",
      "Iteration 4131, loss = 0.43740592\n",
      "Iteration 4132, loss = 0.43722657\n",
      "Iteration 4133, loss = 0.43704720\n",
      "Iteration 4134, loss = 0.43686780\n",
      "Iteration 4135, loss = 0.43668837\n",
      "Iteration 4136, loss = 0.43650891\n",
      "Iteration 4137, loss = 0.43632943\n",
      "Iteration 4138, loss = 0.43614991\n",
      "Iteration 4139, loss = 0.43597038\n",
      "Iteration 4140, loss = 0.43579081\n",
      "Iteration 4141, loss = 0.43561122\n",
      "Iteration 4142, loss = 0.43543161\n",
      "Iteration 4143, loss = 0.43525197\n",
      "Iteration 4144, loss = 0.43507230\n",
      "Iteration 4145, loss = 0.43489261\n",
      "Iteration 4146, loss = 0.43471289\n",
      "Iteration 4147, loss = 0.43453314\n",
      "Iteration 4148, loss = 0.43435338\n",
      "Iteration 4149, loss = 0.43417358\n",
      "Iteration 4150, loss = 0.43399376\n",
      "Iteration 4151, loss = 0.43381392\n",
      "Iteration 4152, loss = 0.43363405\n",
      "Iteration 4153, loss = 0.43345416\n",
      "Iteration 4154, loss = 0.43327424\n",
      "Iteration 4155, loss = 0.43309430\n",
      "Iteration 4156, loss = 0.43291434\n",
      "Iteration 4157, loss = 0.43273435\n",
      "Iteration 4158, loss = 0.43255434\n",
      "Iteration 4159, loss = 0.43237430\n",
      "Iteration 4160, loss = 0.43219425\n",
      "Iteration 4161, loss = 0.43201417\n",
      "Iteration 4162, loss = 0.43183406\n",
      "Iteration 4163, loss = 0.43165393\n",
      "Iteration 4164, loss = 0.43147379\n",
      "Iteration 4165, loss = 0.43129361\n",
      "Iteration 4166, loss = 0.43111342\n",
      "Iteration 4167, loss = 0.43093320\n",
      "Iteration 4168, loss = 0.43075297\n",
      "Iteration 4169, loss = 0.43057271\n",
      "Iteration 4170, loss = 0.43039242\n",
      "Iteration 4171, loss = 0.43021212\n",
      "Iteration 4172, loss = 0.43003180\n",
      "Iteration 4173, loss = 0.42985145\n",
      "Iteration 4174, loss = 0.42967108\n",
      "Iteration 4175, loss = 0.42949070\n",
      "Iteration 4176, loss = 0.42931029\n",
      "Iteration 4177, loss = 0.42912986\n",
      "Iteration 4178, loss = 0.42894941\n",
      "Iteration 4179, loss = 0.42876894\n",
      "Iteration 4180, loss = 0.42858845\n",
      "Iteration 4181, loss = 0.42840795\n",
      "Iteration 4182, loss = 0.42822742\n",
      "Iteration 4183, loss = 0.42804687\n",
      "Iteration 4184, loss = 0.42786630\n",
      "Iteration 4185, loss = 0.42768572\n",
      "Iteration 4186, loss = 0.42750511\n",
      "Iteration 4187, loss = 0.42732449\n",
      "Iteration 4188, loss = 0.42714384\n",
      "Iteration 4189, loss = 0.42696318\n",
      "Iteration 4190, loss = 0.42678250\n",
      "Iteration 4191, loss = 0.42660180\n",
      "Iteration 4192, loss = 0.42642109\n",
      "Iteration 4193, loss = 0.42624035\n",
      "Iteration 4194, loss = 0.42605960\n",
      "Iteration 4195, loss = 0.42587883\n",
      "Iteration 4196, loss = 0.42569804\n",
      "Iteration 4197, loss = 0.42551724\n",
      "Iteration 4198, loss = 0.42533642\n",
      "Iteration 4199, loss = 0.42515558\n",
      "Iteration 4200, loss = 0.42497472\n",
      "Iteration 4201, loss = 0.42479385\n",
      "Iteration 4202, loss = 0.42461296\n",
      "Iteration 4203, loss = 0.42443206\n",
      "Iteration 4204, loss = 0.42425114\n",
      "Iteration 4205, loss = 0.42407020\n",
      "Iteration 4206, loss = 0.42388925\n",
      "Iteration 4207, loss = 0.42370828\n",
      "Iteration 4208, loss = 0.42352730\n",
      "Iteration 4209, loss = 0.42334630\n",
      "Iteration 4210, loss = 0.42316528\n",
      "Iteration 4211, loss = 0.42298425\n",
      "Iteration 4212, loss = 0.42280321\n",
      "Iteration 4213, loss = 0.42262215\n",
      "Iteration 4214, loss = 0.42244108\n",
      "Iteration 4215, loss = 0.42225999\n",
      "Iteration 4216, loss = 0.42207889\n",
      "Iteration 4217, loss = 0.42189777\n",
      "Iteration 4218, loss = 0.42171664\n",
      "Iteration 4219, loss = 0.42153550\n",
      "Iteration 4220, loss = 0.42135435\n",
      "Iteration 4221, loss = 0.42117318\n",
      "Iteration 4222, loss = 0.42099199\n",
      "Iteration 4223, loss = 0.42081080\n",
      "Iteration 4224, loss = 0.42062959\n",
      "Iteration 4225, loss = 0.42044837\n",
      "Iteration 4226, loss = 0.42026713\n",
      "Iteration 4227, loss = 0.42008589\n",
      "Iteration 4228, loss = 0.41990463\n",
      "Iteration 4229, loss = 0.41972336\n",
      "Iteration 4230, loss = 0.41954207\n",
      "Iteration 4231, loss = 0.41936078\n",
      "Iteration 4232, loss = 0.41917947\n",
      "Iteration 4233, loss = 0.41899816\n",
      "Iteration 4234, loss = 0.41881683\n",
      "Iteration 4235, loss = 0.41863549\n",
      "Iteration 4236, loss = 0.41845414\n",
      "Iteration 4237, loss = 0.41827278\n",
      "Iteration 4238, loss = 0.41809141\n",
      "Iteration 4239, loss = 0.41791002\n",
      "Iteration 4240, loss = 0.41772863\n",
      "Iteration 4241, loss = 0.41754723\n",
      "Iteration 4242, loss = 0.41736582\n",
      "Iteration 4243, loss = 0.41718439\n",
      "Iteration 4244, loss = 0.41700296\n",
      "Iteration 4245, loss = 0.41682152\n",
      "Iteration 4246, loss = 0.41664007\n",
      "Iteration 4247, loss = 0.41645861\n",
      "Iteration 4248, loss = 0.41627714\n",
      "Iteration 4249, loss = 0.41609567\n",
      "Iteration 4250, loss = 0.41591418\n",
      "Iteration 4251, loss = 0.41573269\n",
      "Iteration 4252, loss = 0.41555119\n",
      "Iteration 4253, loss = 0.41536968\n",
      "Iteration 4254, loss = 0.41518816\n",
      "Iteration 4255, loss = 0.41500663\n",
      "Iteration 4256, loss = 0.41482510\n",
      "Iteration 4257, loss = 0.41464356\n",
      "Iteration 4258, loss = 0.41446201\n",
      "Iteration 4259, loss = 0.41428045\n",
      "Iteration 4260, loss = 0.41409889\n",
      "Iteration 4261, loss = 0.41391732\n",
      "Iteration 4262, loss = 0.41373575\n",
      "Iteration 4263, loss = 0.41355417\n",
      "Iteration 4264, loss = 0.41337258\n",
      "Iteration 4265, loss = 0.41319098\n",
      "Iteration 4266, loss = 0.41300938\n",
      "Iteration 4267, loss = 0.41282778\n",
      "Iteration 4268, loss = 0.41264617\n",
      "Iteration 4269, loss = 0.41246455\n",
      "Iteration 4270, loss = 0.41228293\n",
      "Iteration 4271, loss = 0.41210130\n",
      "Iteration 4272, loss = 0.41191967\n",
      "Iteration 4273, loss = 0.41173803\n",
      "Iteration 4274, loss = 0.41155639\n",
      "Iteration 4275, loss = 0.41137474\n",
      "Iteration 4276, loss = 0.41119309\n",
      "Iteration 4277, loss = 0.41101144\n",
      "Iteration 4278, loss = 0.41082978\n",
      "Iteration 4279, loss = 0.41064811\n",
      "Iteration 4280, loss = 0.41046645\n",
      "Iteration 4281, loss = 0.41028478\n",
      "Iteration 4282, loss = 0.41010311\n",
      "Iteration 4283, loss = 0.40992143\n",
      "Iteration 4284, loss = 0.40973975\n",
      "Iteration 4285, loss = 0.40955807\n",
      "Iteration 4286, loss = 0.40937638\n",
      "Iteration 4287, loss = 0.40919470\n",
      "Iteration 4288, loss = 0.40901301\n",
      "Iteration 4289, loss = 0.40883132\n",
      "Iteration 4290, loss = 0.40864962\n",
      "Iteration 4291, loss = 0.40846793\n",
      "Iteration 4292, loss = 0.40828623\n",
      "Iteration 4293, loss = 0.40810453\n",
      "Iteration 4294, loss = 0.40792283\n",
      "Iteration 4295, loss = 0.40774113\n",
      "Iteration 4296, loss = 0.40755943\n",
      "Iteration 4297, loss = 0.40737773\n",
      "Iteration 4298, loss = 0.40719603\n",
      "Iteration 4299, loss = 0.40701432\n",
      "Iteration 4300, loss = 0.40683262\n",
      "Iteration 4301, loss = 0.40665091\n",
      "Iteration 4302, loss = 0.40646921\n",
      "Iteration 4303, loss = 0.40628751\n",
      "Iteration 4304, loss = 0.40610580\n",
      "Iteration 4305, loss = 0.40592410\n",
      "Iteration 4306, loss = 0.40574240\n",
      "Iteration 4307, loss = 0.40556070\n",
      "Iteration 4308, loss = 0.40537899\n",
      "Iteration 4309, loss = 0.40519729\n",
      "Iteration 4310, loss = 0.40501560\n",
      "Iteration 4311, loss = 0.40483390\n",
      "Iteration 4312, loss = 0.40465220\n",
      "Iteration 4313, loss = 0.40447051\n",
      "Iteration 4314, loss = 0.40428882\n",
      "Iteration 4315, loss = 0.40410713\n",
      "Iteration 4316, loss = 0.40392544\n",
      "Iteration 4317, loss = 0.40374376\n",
      "Iteration 4318, loss = 0.40356208\n",
      "Iteration 4319, loss = 0.40338040\n",
      "Iteration 4320, loss = 0.40319872\n",
      "Iteration 4321, loss = 0.40301705\n",
      "Iteration 4322, loss = 0.40283538\n",
      "Iteration 4323, loss = 0.40265371\n",
      "Iteration 4324, loss = 0.40247205\n",
      "Iteration 4325, loss = 0.40229039\n",
      "Iteration 4326, loss = 0.40210874\n",
      "Iteration 4327, loss = 0.40192709\n",
      "Iteration 4328, loss = 0.40174544\n",
      "Iteration 4329, loss = 0.40156380\n",
      "Iteration 4330, loss = 0.40138216\n",
      "Iteration 4331, loss = 0.40120053\n",
      "Iteration 4332, loss = 0.40101890\n",
      "Iteration 4333, loss = 0.40083728\n",
      "Iteration 4334, loss = 0.40065566\n",
      "Iteration 4335, loss = 0.40047405\n",
      "Iteration 4336, loss = 0.40029244\n",
      "Iteration 4337, loss = 0.40011084\n",
      "Iteration 4338, loss = 0.39992925\n",
      "Iteration 4339, loss = 0.39974766\n",
      "Iteration 4340, loss = 0.39956608\n",
      "Iteration 4341, loss = 0.39938450\n",
      "Iteration 4342, loss = 0.39920293\n",
      "Iteration 4343, loss = 0.39902137\n",
      "Iteration 4344, loss = 0.39883981\n",
      "Iteration 4345, loss = 0.39865826\n",
      "Iteration 4346, loss = 0.39847672\n",
      "Iteration 4347, loss = 0.39829519\n",
      "Iteration 4348, loss = 0.39811366\n",
      "Iteration 4349, loss = 0.39793214\n",
      "Iteration 4350, loss = 0.39775063\n",
      "Iteration 4351, loss = 0.39756913\n",
      "Iteration 4352, loss = 0.39738763\n",
      "Iteration 4353, loss = 0.39720615\n",
      "Iteration 4354, loss = 0.39702467\n",
      "Iteration 4355, loss = 0.39684320\n",
      "Iteration 4356, loss = 0.39666174\n",
      "Iteration 4357, loss = 0.39648029\n",
      "Iteration 4358, loss = 0.39629885\n",
      "Iteration 4359, loss = 0.39611741\n",
      "Iteration 4360, loss = 0.39593599\n",
      "Iteration 4361, loss = 0.39575458\n",
      "Iteration 4362, loss = 0.39557317\n",
      "Iteration 4363, loss = 0.39539178\n",
      "Iteration 4364, loss = 0.39521039\n",
      "Iteration 4365, loss = 0.39502902\n",
      "Iteration 4366, loss = 0.39484766\n",
      "Iteration 4367, loss = 0.39466630\n",
      "Iteration 4368, loss = 0.39448496\n",
      "Iteration 4369, loss = 0.39430363\n",
      "Iteration 4370, loss = 0.39412231\n",
      "Iteration 4371, loss = 0.39394100\n",
      "Iteration 4372, loss = 0.39375971\n",
      "Iteration 4373, loss = 0.39357842\n",
      "Iteration 4374, loss = 0.39339715\n",
      "Iteration 4375, loss = 0.39321589\n",
      "Iteration 4376, loss = 0.39303464\n",
      "Iteration 4377, loss = 0.39285340\n",
      "Iteration 4378, loss = 0.39267218\n",
      "Iteration 4379, loss = 0.39249096\n",
      "Iteration 4380, loss = 0.39230976\n",
      "Iteration 4381, loss = 0.39212858\n",
      "Iteration 4382, loss = 0.39194740\n",
      "Iteration 4383, loss = 0.39176624\n",
      "Iteration 4384, loss = 0.39158509\n",
      "Iteration 4385, loss = 0.39140396\n",
      "Iteration 4386, loss = 0.39122284\n",
      "Iteration 4387, loss = 0.39104173\n",
      "Iteration 4388, loss = 0.39086064\n",
      "Iteration 4389, loss = 0.39067956\n",
      "Iteration 4390, loss = 0.39049850\n",
      "Iteration 4391, loss = 0.39031745\n",
      "Iteration 4392, loss = 0.39013642\n",
      "Iteration 4393, loss = 0.38995540\n",
      "Iteration 4394, loss = 0.38977439\n",
      "Iteration 4395, loss = 0.38959340\n",
      "Iteration 4396, loss = 0.38941243\n",
      "Iteration 4397, loss = 0.38923147\n",
      "Iteration 4398, loss = 0.38905053\n",
      "Iteration 4399, loss = 0.38886960\n",
      "Iteration 4400, loss = 0.38868869\n",
      "Iteration 4401, loss = 0.38850779\n",
      "Iteration 4402, loss = 0.38832691\n",
      "Iteration 4403, loss = 0.38814605\n",
      "Iteration 4404, loss = 0.38796520\n",
      "Iteration 4405, loss = 0.38778437\n",
      "Iteration 4406, loss = 0.38760356\n",
      "Iteration 4407, loss = 0.38742276\n",
      "Iteration 4408, loss = 0.38724199\n",
      "Iteration 4409, loss = 0.38706122\n",
      "Iteration 4410, loss = 0.38688048\n",
      "Iteration 4411, loss = 0.38669975\n",
      "Iteration 4412, loss = 0.38651905\n",
      "Iteration 4413, loss = 0.38633835\n",
      "Iteration 4414, loss = 0.38615768\n",
      "Iteration 4415, loss = 0.38597703\n",
      "Iteration 4416, loss = 0.38579639\n",
      "Iteration 4417, loss = 0.38561578\n",
      "Iteration 4418, loss = 0.38543518\n",
      "Iteration 4419, loss = 0.38525460\n",
      "Iteration 4420, loss = 0.38507404\n",
      "Iteration 4421, loss = 0.38489350\n",
      "Iteration 4422, loss = 0.38471298\n",
      "Iteration 4423, loss = 0.38453247\n",
      "Iteration 4424, loss = 0.38435199\n",
      "Iteration 4425, loss = 0.38417153\n",
      "Iteration 4426, loss = 0.38399109\n",
      "Iteration 4427, loss = 0.38381066\n",
      "Iteration 4428, loss = 0.38363026\n",
      "Iteration 4429, loss = 0.38344988\n",
      "Iteration 4430, loss = 0.38326952\n",
      "Iteration 4431, loss = 0.38308918\n",
      "Iteration 4432, loss = 0.38290886\n",
      "Iteration 4433, loss = 0.38272856\n",
      "Iteration 4434, loss = 0.38254828\n",
      "Iteration 4435, loss = 0.38236803\n",
      "Iteration 4436, loss = 0.38218779\n",
      "Iteration 4437, loss = 0.38200758\n",
      "Iteration 4438, loss = 0.38182739\n",
      "Iteration 4439, loss = 0.38164722\n",
      "Iteration 4440, loss = 0.38146708\n",
      "Iteration 4441, loss = 0.38128695\n",
      "Iteration 4442, loss = 0.38110685\n",
      "Iteration 4443, loss = 0.38092677\n",
      "Iteration 4444, loss = 0.38074671\n",
      "Iteration 4445, loss = 0.38056668\n",
      "Iteration 4446, loss = 0.38038667\n",
      "Iteration 4447, loss = 0.38020668\n",
      "Iteration 4448, loss = 0.38002672\n",
      "Iteration 4449, loss = 0.37984678\n",
      "Iteration 4450, loss = 0.37966687\n",
      "Iteration 4451, loss = 0.37948697\n",
      "Iteration 4452, loss = 0.37930710\n",
      "Iteration 4453, loss = 0.37912726\n",
      "Iteration 4454, loss = 0.37894744\n",
      "Iteration 4455, loss = 0.37876765\n",
      "Iteration 4456, loss = 0.37858787\n",
      "Iteration 4457, loss = 0.37840813\n",
      "Iteration 4458, loss = 0.37822841\n",
      "Iteration 4459, loss = 0.37804871\n",
      "Iteration 4460, loss = 0.37786904\n",
      "Iteration 4461, loss = 0.37768940\n",
      "Iteration 4462, loss = 0.37750978\n",
      "Iteration 4463, loss = 0.37733018\n",
      "Iteration 4464, loss = 0.37715061\n",
      "Iteration 4465, loss = 0.37697107\n",
      "Iteration 4466, loss = 0.37679155\n",
      "Iteration 4467, loss = 0.37661206\n",
      "Iteration 4468, loss = 0.37643260\n",
      "Iteration 4469, loss = 0.37625316\n",
      "Iteration 4470, loss = 0.37607375\n",
      "Iteration 4471, loss = 0.37589437\n",
      "Iteration 4472, loss = 0.37571501\n",
      "Iteration 4473, loss = 0.37553569\n",
      "Iteration 4474, loss = 0.37535638\n",
      "Iteration 4475, loss = 0.37517711\n",
      "Iteration 4476, loss = 0.37499786\n",
      "Iteration 4477, loss = 0.37481864\n",
      "Iteration 4478, loss = 0.37463945\n",
      "Iteration 4479, loss = 0.37446029\n",
      "Iteration 4480, loss = 0.37428116\n",
      "Iteration 4481, loss = 0.37410205\n",
      "Iteration 4482, loss = 0.37392297\n",
      "Iteration 4483, loss = 0.37374392\n",
      "Iteration 4484, loss = 0.37356490\n",
      "Iteration 4485, loss = 0.37338591\n",
      "Iteration 4486, loss = 0.37320695\n",
      "Iteration 4487, loss = 0.37302802\n",
      "Iteration 4488, loss = 0.37284911\n",
      "Iteration 4489, loss = 0.37267024\n",
      "Iteration 4490, loss = 0.37249139\n",
      "Iteration 4491, loss = 0.37231258\n",
      "Iteration 4492, loss = 0.37213379\n",
      "Iteration 4493, loss = 0.37195504\n",
      "Iteration 4494, loss = 0.37177632\n",
      "Iteration 4495, loss = 0.37159762\n",
      "Iteration 4496, loss = 0.37141896\n",
      "Iteration 4497, loss = 0.37124033\n",
      "Iteration 4498, loss = 0.37106173\n",
      "Iteration 4499, loss = 0.37088316\n",
      "Iteration 4500, loss = 0.37070462\n",
      "Iteration 4501, loss = 0.37052611\n",
      "Iteration 4502, loss = 0.37034763\n",
      "Iteration 4503, loss = 0.37016919\n",
      "Iteration 4504, loss = 0.36999077\n",
      "Iteration 4505, loss = 0.36981239\n",
      "Iteration 4506, loss = 0.36963404\n",
      "Iteration 4507, loss = 0.36945573\n",
      "Iteration 4508, loss = 0.36927744\n",
      "Iteration 4509, loss = 0.36909919\n",
      "Iteration 4510, loss = 0.36892097\n",
      "Iteration 4511, loss = 0.36874278\n",
      "Iteration 4512, loss = 0.36856463\n",
      "Iteration 4513, loss = 0.36838651\n",
      "Iteration 4514, loss = 0.36820842\n",
      "Iteration 4515, loss = 0.36803037\n",
      "Iteration 4516, loss = 0.36785234\n",
      "Iteration 4517, loss = 0.36767436\n",
      "Iteration 4518, loss = 0.36749640\n",
      "Iteration 4519, loss = 0.36731848\n",
      "Iteration 4520, loss = 0.36714060\n",
      "Iteration 4521, loss = 0.36696275\n",
      "Iteration 4522, loss = 0.36678493\n",
      "Iteration 4523, loss = 0.36660715\n",
      "Iteration 4524, loss = 0.36642940\n",
      "Iteration 4525, loss = 0.36625168\n",
      "Iteration 4526, loss = 0.36607401\n",
      "Iteration 4527, loss = 0.36589636\n",
      "Iteration 4528, loss = 0.36571875\n",
      "Iteration 4529, loss = 0.36554118\n",
      "Iteration 4530, loss = 0.36536364\n",
      "Iteration 4531, loss = 0.36518614\n",
      "Iteration 4532, loss = 0.36500867\n",
      "Iteration 4533, loss = 0.36483124\n",
      "Iteration 4534, loss = 0.36465385\n",
      "Iteration 4535, loss = 0.36447649\n",
      "Iteration 4536, loss = 0.36429917\n",
      "Iteration 4537, loss = 0.36412188\n",
      "Iteration 4538, loss = 0.36394463\n",
      "Iteration 4539, loss = 0.36376742\n",
      "Iteration 4540, loss = 0.36359024\n",
      "Iteration 4541, loss = 0.36341311\n",
      "Iteration 4542, loss = 0.36323600\n",
      "Iteration 4543, loss = 0.36305894\n",
      "Iteration 4544, loss = 0.36288191\n",
      "Iteration 4545, loss = 0.36270492\n",
      "Iteration 4546, loss = 0.36252797\n",
      "Iteration 4547, loss = 0.36235106\n",
      "Iteration 4548, loss = 0.36217418\n",
      "Iteration 4549, loss = 0.36199734\n",
      "Iteration 4550, loss = 0.36182054\n",
      "Iteration 4551, loss = 0.36164378\n",
      "Iteration 4552, loss = 0.36146706\n",
      "Iteration 4553, loss = 0.36129037\n",
      "Iteration 4554, loss = 0.36111373\n",
      "Iteration 4555, loss = 0.36093712\n",
      "Iteration 4556, loss = 0.36076055\n",
      "Iteration 4557, loss = 0.36058403\n",
      "Iteration 4558, loss = 0.36040754\n",
      "Iteration 4559, loss = 0.36023109\n",
      "Iteration 4560, loss = 0.36005468\n",
      "Iteration 4561, loss = 0.35987831\n",
      "Iteration 4562, loss = 0.35970197\n",
      "Iteration 4563, loss = 0.35952568\n",
      "Iteration 4564, loss = 0.35934943\n",
      "Iteration 4565, loss = 0.35917322\n",
      "Iteration 4566, loss = 0.35899705\n",
      "Iteration 4567, loss = 0.35882092\n",
      "Iteration 4568, loss = 0.35864483\n",
      "Iteration 4569, loss = 0.35846878\n",
      "Iteration 4570, loss = 0.35829278\n",
      "Iteration 4571, loss = 0.35811681\n",
      "Iteration 4572, loss = 0.35794088\n",
      "Iteration 4573, loss = 0.35776500\n",
      "Iteration 4574, loss = 0.35758916\n",
      "Iteration 4575, loss = 0.35741336\n",
      "Iteration 4576, loss = 0.35723760\n",
      "Iteration 4577, loss = 0.35706188\n",
      "Iteration 4578, loss = 0.35688620\n",
      "Iteration 4579, loss = 0.35671057\n",
      "Iteration 4580, loss = 0.35653498\n",
      "Iteration 4581, loss = 0.35635943\n",
      "Iteration 4582, loss = 0.35618392\n",
      "Iteration 4583, loss = 0.35600846\n",
      "Iteration 4584, loss = 0.35583304\n",
      "Iteration 4585, loss = 0.35565766\n",
      "Iteration 4586, loss = 0.35548232\n",
      "Iteration 4587, loss = 0.35530703\n",
      "Iteration 4588, loss = 0.35513178\n",
      "Iteration 4589, loss = 0.35495658\n",
      "Iteration 4590, loss = 0.35478141\n",
      "Iteration 4591, loss = 0.35460629\n",
      "Iteration 4592, loss = 0.35443122\n",
      "Iteration 4593, loss = 0.35425619\n",
      "Iteration 4594, loss = 0.35408120\n",
      "Iteration 4595, loss = 0.35390626\n",
      "Iteration 4596, loss = 0.35373136\n",
      "Iteration 4597, loss = 0.35355650\n",
      "Iteration 4598, loss = 0.35338169\n",
      "Iteration 4599, loss = 0.35320693\n",
      "Iteration 4600, loss = 0.35303221\n",
      "Iteration 4601, loss = 0.35285753\n",
      "Iteration 4602, loss = 0.35268290\n",
      "Iteration 4603, loss = 0.35250832\n",
      "Iteration 4604, loss = 0.35233378\n",
      "Iteration 4605, loss = 0.35215928\n",
      "Iteration 4606, loss = 0.35198483\n",
      "Iteration 4607, loss = 0.35181043\n",
      "Iteration 4608, loss = 0.35163607\n",
      "Iteration 4609, loss = 0.35146176\n",
      "Iteration 4610, loss = 0.35128749\n",
      "Iteration 4611, loss = 0.35111327\n",
      "Iteration 4612, loss = 0.35093910\n",
      "Iteration 4613, loss = 0.35076497\n",
      "Iteration 4614, loss = 0.35059089\n",
      "Iteration 4615, loss = 0.35041686\n",
      "Iteration 4616, loss = 0.35024287\n",
      "Iteration 4617, loss = 0.35006893\n",
      "Iteration 4618, loss = 0.34989504\n",
      "Iteration 4619, loss = 0.34972119\n",
      "Iteration 4620, loss = 0.34954739\n",
      "Iteration 4621, loss = 0.34937364\n",
      "Iteration 4622, loss = 0.34919993\n",
      "Iteration 4623, loss = 0.34902628\n",
      "Iteration 4624, loss = 0.34885267\n",
      "Iteration 4625, loss = 0.34867911\n",
      "Iteration 4626, loss = 0.34850559\n",
      "Iteration 4627, loss = 0.34833213\n",
      "Iteration 4628, loss = 0.34815871\n",
      "Iteration 4629, loss = 0.34798534\n",
      "Iteration 4630, loss = 0.34781202\n",
      "Iteration 4631, loss = 0.34763875\n",
      "Iteration 4632, loss = 0.34746553\n",
      "Iteration 4633, loss = 0.34729235\n",
      "Iteration 4634, loss = 0.34711923\n",
      "Iteration 4635, loss = 0.34694615\n",
      "Iteration 4636, loss = 0.34677313\n",
      "Iteration 4637, loss = 0.34660015\n",
      "Iteration 4638, loss = 0.34642722\n",
      "Iteration 4639, loss = 0.34625434\n",
      "Iteration 4640, loss = 0.34608151\n",
      "Iteration 4641, loss = 0.34590873\n",
      "Iteration 4642, loss = 0.34573600\n",
      "Iteration 4643, loss = 0.34556332\n",
      "Iteration 4644, loss = 0.34539069\n",
      "Iteration 4645, loss = 0.34521811\n",
      "Iteration 4646, loss = 0.34504558\n",
      "Iteration 4647, loss = 0.34487311\n",
      "Iteration 4648, loss = 0.34470068\n",
      "Iteration 4649, loss = 0.34452830\n",
      "Iteration 4650, loss = 0.34435597\n",
      "Iteration 4651, loss = 0.34418370\n",
      "Iteration 4652, loss = 0.34401147\n",
      "Iteration 4653, loss = 0.34383930\n",
      "Iteration 4654, loss = 0.34366718\n",
      "Iteration 4655, loss = 0.34349511\n",
      "Iteration 4656, loss = 0.34332309\n",
      "Iteration 4657, loss = 0.34315112\n",
      "Iteration 4658, loss = 0.34297920\n",
      "Iteration 4659, loss = 0.34280734\n",
      "Iteration 4660, loss = 0.34263552\n",
      "Iteration 4661, loss = 0.34246376\n",
      "Iteration 4662, loss = 0.34229206\n",
      "Iteration 4663, loss = 0.34212040\n",
      "Iteration 4664, loss = 0.34194880\n",
      "Iteration 4665, loss = 0.34177724\n",
      "Iteration 4666, loss = 0.34160575\n",
      "Iteration 4667, loss = 0.34143430\n",
      "Iteration 4668, loss = 0.34126291\n",
      "Iteration 4669, loss = 0.34109157\n",
      "Iteration 4670, loss = 0.34092028\n",
      "Iteration 4671, loss = 0.34074905\n",
      "Iteration 4672, loss = 0.34057786\n",
      "Iteration 4673, loss = 0.34040674\n",
      "Iteration 4674, loss = 0.34023566\n",
      "Iteration 4675, loss = 0.34006464\n",
      "Iteration 4676, loss = 0.33989368\n",
      "Iteration 4677, loss = 0.33972276\n",
      "Iteration 4678, loss = 0.33955190\n",
      "Iteration 4679, loss = 0.33938110\n",
      "Iteration 4680, loss = 0.33921035\n",
      "Iteration 4681, loss = 0.33903965\n",
      "Iteration 4682, loss = 0.33886901\n",
      "Iteration 4683, loss = 0.33869842\n",
      "Iteration 4684, loss = 0.33852789\n",
      "Iteration 4685, loss = 0.33835741\n",
      "Iteration 4686, loss = 0.33818699\n",
      "Iteration 4687, loss = 0.33801662\n",
      "Iteration 4688, loss = 0.33784630\n",
      "Iteration 4689, loss = 0.33767605\n",
      "Iteration 4690, loss = 0.33750584\n",
      "Iteration 4691, loss = 0.33733569\n",
      "Iteration 4692, loss = 0.33716560\n",
      "Iteration 4693, loss = 0.33699556\n",
      "Iteration 4694, loss = 0.33682558\n",
      "Iteration 4695, loss = 0.33665565\n",
      "Iteration 4696, loss = 0.33648578\n",
      "Iteration 4697, loss = 0.33631597\n",
      "Iteration 4698, loss = 0.33614621\n",
      "Iteration 4699, loss = 0.33597651\n",
      "Iteration 4700, loss = 0.33580686\n",
      "Iteration 4701, loss = 0.33563727\n",
      "Iteration 4702, loss = 0.33546774\n",
      "Iteration 4703, loss = 0.33529826\n",
      "Iteration 4704, loss = 0.33512884\n",
      "Iteration 4705, loss = 0.33495948\n",
      "Iteration 4706, loss = 0.33479017\n",
      "Iteration 4707, loss = 0.33462093\n",
      "Iteration 4708, loss = 0.33445173\n",
      "Iteration 4709, loss = 0.33428260\n",
      "Iteration 4710, loss = 0.33411352\n",
      "Iteration 4711, loss = 0.33394450\n",
      "Iteration 4712, loss = 0.33377554\n",
      "Iteration 4713, loss = 0.33360663\n",
      "Iteration 4714, loss = 0.33343778\n",
      "Iteration 4715, loss = 0.33326899\n",
      "Iteration 4716, loss = 0.33310026\n",
      "Iteration 4717, loss = 0.33293159\n",
      "Iteration 4718, loss = 0.33276297\n",
      "Iteration 4719, loss = 0.33259441\n",
      "Iteration 4720, loss = 0.33242591\n",
      "Iteration 4721, loss = 0.33225747\n",
      "Iteration 4722, loss = 0.33208909\n",
      "Iteration 4723, loss = 0.33192076\n",
      "Iteration 4724, loss = 0.33175250\n",
      "Iteration 4725, loss = 0.33158429\n",
      "Iteration 4726, loss = 0.33141614\n",
      "Iteration 4727, loss = 0.33124805\n",
      "Iteration 4728, loss = 0.33108002\n",
      "Iteration 4729, loss = 0.33091205\n",
      "Iteration 4730, loss = 0.33074414\n",
      "Iteration 4731, loss = 0.33057629\n",
      "Iteration 4732, loss = 0.33040849\n",
      "Iteration 4733, loss = 0.33024076\n",
      "Iteration 4734, loss = 0.33007308\n",
      "Iteration 4735, loss = 0.32990547\n",
      "Iteration 4736, loss = 0.32973792\n",
      "Iteration 4737, loss = 0.32957042\n",
      "Iteration 4738, loss = 0.32940299\n",
      "Iteration 4739, loss = 0.32923561\n",
      "Iteration 4740, loss = 0.32906830\n",
      "Iteration 4741, loss = 0.32890104\n",
      "Iteration 4742, loss = 0.32873385\n",
      "Iteration 4743, loss = 0.32856671\n",
      "Iteration 4744, loss = 0.32839964\n",
      "Iteration 4745, loss = 0.32823263\n",
      "Iteration 4746, loss = 0.32806568\n",
      "Iteration 4747, loss = 0.32789879\n",
      "Iteration 4748, loss = 0.32773196\n",
      "Iteration 4749, loss = 0.32756519\n",
      "Iteration 4750, loss = 0.32739848\n",
      "Iteration 4751, loss = 0.32723183\n",
      "Iteration 4752, loss = 0.32706525\n",
      "Iteration 4753, loss = 0.32689873\n",
      "Iteration 4754, loss = 0.32673226\n",
      "Iteration 4755, loss = 0.32656586\n",
      "Iteration 4756, loss = 0.32639952\n",
      "Iteration 4757, loss = 0.32623325\n",
      "Iteration 4758, loss = 0.32606703\n",
      "Iteration 4759, loss = 0.32590088\n",
      "Iteration 4760, loss = 0.32573478\n",
      "Iteration 4761, loss = 0.32556875\n",
      "Iteration 4762, loss = 0.32540279\n",
      "Iteration 4763, loss = 0.32523688\n",
      "Iteration 4764, loss = 0.32507104\n",
      "Iteration 4765, loss = 0.32490526\n",
      "Iteration 4766, loss = 0.32473954\n",
      "Iteration 4767, loss = 0.32457389\n",
      "Iteration 4768, loss = 0.32440829\n",
      "Iteration 4769, loss = 0.32424276\n",
      "Iteration 4770, loss = 0.32407730\n",
      "Iteration 4771, loss = 0.32391189\n",
      "Iteration 4772, loss = 0.32374655\n",
      "Iteration 4773, loss = 0.32358127\n",
      "Iteration 4774, loss = 0.32341606\n",
      "Iteration 4775, loss = 0.32325091\n",
      "Iteration 4776, loss = 0.32308582\n",
      "Iteration 4777, loss = 0.32292079\n",
      "Iteration 4778, loss = 0.32275583\n",
      "Iteration 4779, loss = 0.32259094\n",
      "Iteration 4780, loss = 0.32242610\n",
      "Iteration 4781, loss = 0.32226133\n",
      "Iteration 4782, loss = 0.32209663\n",
      "Iteration 4783, loss = 0.32193198\n",
      "Iteration 4784, loss = 0.32176741\n",
      "Iteration 4785, loss = 0.32160289\n",
      "Iteration 4786, loss = 0.32143844\n",
      "Iteration 4787, loss = 0.32127406\n",
      "Iteration 4788, loss = 0.32110974\n",
      "Iteration 4789, loss = 0.32094548\n",
      "Iteration 4790, loss = 0.32078129\n",
      "Iteration 4791, loss = 0.32061716\n",
      "Iteration 4792, loss = 0.32045310\n",
      "Iteration 4793, loss = 0.32028910\n",
      "Iteration 4794, loss = 0.32012517\n",
      "Iteration 4795, loss = 0.31996130\n",
      "Iteration 4796, loss = 0.31979750\n",
      "Iteration 4797, loss = 0.31963376\n",
      "Iteration 4798, loss = 0.31947009\n",
      "Iteration 4799, loss = 0.31930648\n",
      "Iteration 4800, loss = 0.31914294\n",
      "Iteration 4801, loss = 0.31897946\n",
      "Iteration 4802, loss = 0.31881605\n",
      "Iteration 4803, loss = 0.31865271\n",
      "Iteration 4804, loss = 0.31848943\n",
      "Iteration 4805, loss = 0.31832621\n",
      "Iteration 4806, loss = 0.31816306\n",
      "Iteration 4807, loss = 0.31799998\n",
      "Iteration 4808, loss = 0.31783696\n",
      "Iteration 4809, loss = 0.31767401\n",
      "Iteration 4810, loss = 0.31751113\n",
      "Iteration 4811, loss = 0.31734831\n",
      "Iteration 4812, loss = 0.31718556\n",
      "Iteration 4813, loss = 0.31702287\n",
      "Iteration 4814, loss = 0.31686026\n",
      "Iteration 4815, loss = 0.31669770\n",
      "Iteration 4816, loss = 0.31653522\n",
      "Iteration 4817, loss = 0.31637280\n",
      "Iteration 4818, loss = 0.31621044\n",
      "Iteration 4819, loss = 0.31604816\n",
      "Iteration 4820, loss = 0.31588594\n",
      "Iteration 4821, loss = 0.31572379\n",
      "Iteration 4822, loss = 0.31556170\n",
      "Iteration 4823, loss = 0.31539969\n",
      "Iteration 4824, loss = 0.31523774\n",
      "Iteration 4825, loss = 0.31507585\n",
      "Iteration 4826, loss = 0.31491404\n",
      "Iteration 4827, loss = 0.31475229\n",
      "Iteration 4828, loss = 0.31459061\n",
      "Iteration 4829, loss = 0.31442899\n",
      "Iteration 4830, loss = 0.31426745\n",
      "Iteration 4831, loss = 0.31410597\n",
      "Iteration 4832, loss = 0.31394456\n",
      "Iteration 4833, loss = 0.31378321\n",
      "Iteration 4834, loss = 0.31362194\n",
      "Iteration 4835, loss = 0.31346073\n",
      "Iteration 4836, loss = 0.31329959\n",
      "Iteration 4837, loss = 0.31313852\n",
      "Iteration 4838, loss = 0.31297752\n",
      "Iteration 4839, loss = 0.31281658\n",
      "Iteration 4840, loss = 0.31265572\n",
      "Iteration 4841, loss = 0.31249492\n",
      "Iteration 4842, loss = 0.31233419\n",
      "Iteration 4843, loss = 0.31217353\n",
      "Iteration 4844, loss = 0.31201294\n",
      "Iteration 4845, loss = 0.31185241\n",
      "Iteration 4846, loss = 0.31169196\n",
      "Iteration 4847, loss = 0.31153157\n",
      "Iteration 4848, loss = 0.31137126\n",
      "Iteration 4849, loss = 0.31121101\n",
      "Iteration 4850, loss = 0.31105083\n",
      "Iteration 4851, loss = 0.31089072\n",
      "Iteration 4852, loss = 0.31073068\n",
      "Iteration 4853, loss = 0.31057071\n",
      "Iteration 4854, loss = 0.31041080\n",
      "Iteration 4855, loss = 0.31025097\n",
      "Iteration 4856, loss = 0.31009121\n",
      "Iteration 4857, loss = 0.30993151\n",
      "Iteration 4858, loss = 0.30977189\n",
      "Iteration 4859, loss = 0.30961233\n",
      "Iteration 4860, loss = 0.30945284\n",
      "Iteration 4861, loss = 0.30929343\n",
      "Iteration 4862, loss = 0.30913408\n",
      "Iteration 4863, loss = 0.30897480\n",
      "Iteration 4864, loss = 0.30881560\n",
      "Iteration 4865, loss = 0.30865646\n",
      "Iteration 4866, loss = 0.30849740\n",
      "Iteration 4867, loss = 0.30833840\n",
      "Iteration 4868, loss = 0.30817947\n",
      "Iteration 4869, loss = 0.30802062\n",
      "Iteration 4870, loss = 0.30786183\n",
      "Iteration 4871, loss = 0.30770311\n",
      "Iteration 4872, loss = 0.30754447\n",
      "Iteration 4873, loss = 0.30738589\n",
      "Iteration 4874, loss = 0.30722739\n",
      "Iteration 4875, loss = 0.30706896\n",
      "Iteration 4876, loss = 0.30691059\n",
      "Iteration 4877, loss = 0.30675230\n",
      "Iteration 4878, loss = 0.30659408\n",
      "Iteration 4879, loss = 0.30643593\n",
      "Iteration 4880, loss = 0.30627785\n",
      "Iteration 4881, loss = 0.30611984\n",
      "Iteration 4882, loss = 0.30596190\n",
      "Iteration 4883, loss = 0.30580404\n",
      "Iteration 4884, loss = 0.30564624\n",
      "Iteration 4885, loss = 0.30548852\n",
      "Iteration 4886, loss = 0.30533086\n",
      "Iteration 4887, loss = 0.30517328\n",
      "Iteration 4888, loss = 0.30501577\n",
      "Iteration 4889, loss = 0.30485833\n",
      "Iteration 4890, loss = 0.30470096\n",
      "Iteration 4891, loss = 0.30454367\n",
      "Iteration 4892, loss = 0.30438644\n",
      "Iteration 4893, loss = 0.30422929\n",
      "Iteration 4894, loss = 0.30407221\n",
      "Iteration 4895, loss = 0.30391520\n",
      "Iteration 4896, loss = 0.30375826\n",
      "Iteration 4897, loss = 0.30360139\n",
      "Iteration 4898, loss = 0.30344460\n",
      "Iteration 4899, loss = 0.30328788\n",
      "Iteration 4900, loss = 0.30313123\n",
      "Iteration 4901, loss = 0.30297465\n",
      "Iteration 4902, loss = 0.30281815\n",
      "Iteration 4903, loss = 0.30266171\n",
      "Iteration 4904, loss = 0.30250535\n",
      "Iteration 4905, loss = 0.30234906\n",
      "Iteration 4906, loss = 0.30219284\n",
      "Iteration 4907, loss = 0.30203670\n",
      "Iteration 4908, loss = 0.30188063\n",
      "Iteration 4909, loss = 0.30172463\n",
      "Iteration 4910, loss = 0.30156870\n",
      "Iteration 4911, loss = 0.30141285\n",
      "Iteration 4912, loss = 0.30125707\n",
      "Iteration 4913, loss = 0.30110136\n",
      "Iteration 4914, loss = 0.30094572\n",
      "Iteration 4915, loss = 0.30079016\n",
      "Iteration 4916, loss = 0.30063467\n",
      "Iteration 4917, loss = 0.30047925\n",
      "Iteration 4918, loss = 0.30032391\n",
      "Iteration 4919, loss = 0.30016864\n",
      "Iteration 4920, loss = 0.30001344\n",
      "Iteration 4921, loss = 0.29985831\n",
      "Iteration 4922, loss = 0.29970326\n",
      "Iteration 4923, loss = 0.29954828\n",
      "Iteration 4924, loss = 0.29939338\n",
      "Iteration 4925, loss = 0.29923855\n",
      "Iteration 4926, loss = 0.29908379\n",
      "Iteration 4927, loss = 0.29892910\n",
      "Iteration 4928, loss = 0.29877449\n",
      "Iteration 4929, loss = 0.29861995\n",
      "Iteration 4930, loss = 0.29846549\n",
      "Iteration 4931, loss = 0.29831110\n",
      "Iteration 4932, loss = 0.29815678\n",
      "Iteration 4933, loss = 0.29800254\n",
      "Iteration 4934, loss = 0.29784837\n",
      "Iteration 4935, loss = 0.29769427\n",
      "Iteration 4936, loss = 0.29754025\n",
      "Iteration 4937, loss = 0.29738630\n",
      "Iteration 4938, loss = 0.29723243\n",
      "Iteration 4939, loss = 0.29707863\n",
      "Iteration 4940, loss = 0.29692490\n",
      "Iteration 4941, loss = 0.29677125\n",
      "Iteration 4942, loss = 0.29661767\n",
      "Iteration 4943, loss = 0.29646417\n",
      "Iteration 4944, loss = 0.29631074\n",
      "Iteration 4945, loss = 0.29615738\n",
      "Iteration 4946, loss = 0.29600410\n",
      "Iteration 4947, loss = 0.29585090\n",
      "Iteration 4948, loss = 0.29569776\n",
      "Iteration 4949, loss = 0.29554471\n",
      "Iteration 4950, loss = 0.29539172\n",
      "Iteration 4951, loss = 0.29523881\n",
      "Iteration 4952, loss = 0.29508598\n",
      "Iteration 4953, loss = 0.29493322\n",
      "Iteration 4954, loss = 0.29478054\n",
      "Iteration 4955, loss = 0.29462793\n",
      "Iteration 4956, loss = 0.29447539\n",
      "Iteration 4957, loss = 0.29432293\n",
      "Iteration 4958, loss = 0.29417054\n",
      "Iteration 4959, loss = 0.29401823\n",
      "Iteration 4960, loss = 0.29386600\n",
      "Iteration 4961, loss = 0.29371384\n",
      "Iteration 4962, loss = 0.29356175\n",
      "Iteration 4963, loss = 0.29340974\n",
      "Iteration 4964, loss = 0.29325780\n",
      "Iteration 4965, loss = 0.29310594\n",
      "Iteration 4966, loss = 0.29295416\n",
      "Iteration 4967, loss = 0.29280245\n",
      "Iteration 4968, loss = 0.29265081\n",
      "Iteration 4969, loss = 0.29249925\n",
      "Iteration 4970, loss = 0.29234777\n",
      "Iteration 4971, loss = 0.29219636\n",
      "Iteration 4972, loss = 0.29204503\n",
      "Iteration 4973, loss = 0.29189377\n",
      "Iteration 4974, loss = 0.29174258\n",
      "Iteration 4975, loss = 0.29159148\n",
      "Iteration 4976, loss = 0.29144045\n",
      "Iteration 4977, loss = 0.29128949\n",
      "Iteration 4978, loss = 0.29113861\n",
      "Iteration 4979, loss = 0.29098780\n",
      "Iteration 4980, loss = 0.29083708\n",
      "Iteration 4981, loss = 0.29068642\n",
      "Iteration 4982, loss = 0.29053584\n",
      "Iteration 4983, loss = 0.29038534\n",
      "Iteration 4984, loss = 0.29023492\n",
      "Iteration 4985, loss = 0.29008457\n",
      "Iteration 4986, loss = 0.28993429\n",
      "Iteration 4987, loss = 0.28978409\n",
      "Iteration 4988, loss = 0.28963397\n",
      "Iteration 4989, loss = 0.28948393\n",
      "Iteration 4990, loss = 0.28933396\n",
      "Iteration 4991, loss = 0.28918406\n",
      "Iteration 4992, loss = 0.28903425\n",
      "Iteration 4993, loss = 0.28888450\n",
      "Iteration 4994, loss = 0.28873484\n",
      "Iteration 4995, loss = 0.28858525\n",
      "Iteration 4996, loss = 0.28843574\n",
      "Iteration 4997, loss = 0.28828630\n",
      "Iteration 4998, loss = 0.28813694\n",
      "Iteration 4999, loss = 0.28798766\n",
      "Iteration 5000, loss = 0.28783845\n",
      "Iteration 5001, loss = 0.28768932\n",
      "Iteration 5002, loss = 0.28754026\n",
      "Iteration 5003, loss = 0.28739129\n",
      "Iteration 5004, loss = 0.28724238\n",
      "Iteration 5005, loss = 0.28709356\n",
      "Iteration 5006, loss = 0.28694481\n",
      "Iteration 5007, loss = 0.28679614\n",
      "Iteration 5008, loss = 0.28664754\n",
      "Iteration 5009, loss = 0.28649903\n",
      "Iteration 5010, loss = 0.28635058\n",
      "Iteration 5011, loss = 0.28620222\n",
      "Iteration 5012, loss = 0.28605393\n",
      "Iteration 5013, loss = 0.28590572\n",
      "Iteration 5014, loss = 0.28575759\n",
      "Iteration 5015, loss = 0.28560953\n",
      "Iteration 5016, loss = 0.28546155\n",
      "Iteration 5017, loss = 0.28531364\n",
      "Iteration 5018, loss = 0.28516582\n",
      "Iteration 5019, loss = 0.28501807\n",
      "Iteration 5020, loss = 0.28487040\n",
      "Iteration 5021, loss = 0.28472280\n",
      "Iteration 5022, loss = 0.28457528\n",
      "Iteration 5023, loss = 0.28442784\n",
      "Iteration 5024, loss = 0.28428048\n",
      "Iteration 5025, loss = 0.28413319\n",
      "Iteration 5026, loss = 0.28398598\n",
      "Iteration 5027, loss = 0.28383885\n",
      "Iteration 5028, loss = 0.28369179\n",
      "Iteration 5029, loss = 0.28354481\n",
      "Iteration 5030, loss = 0.28339791\n",
      "Iteration 5031, loss = 0.28325109\n",
      "Iteration 5032, loss = 0.28310434\n",
      "Iteration 5033, loss = 0.28295767\n",
      "Iteration 5034, loss = 0.28281108\n",
      "Iteration 5035, loss = 0.28266457\n",
      "Iteration 5036, loss = 0.28251813\n",
      "Iteration 5037, loss = 0.28237177\n",
      "Iteration 5038, loss = 0.28222549\n",
      "Iteration 5039, loss = 0.28207929\n",
      "Iteration 5040, loss = 0.28193316\n",
      "Iteration 5041, loss = 0.28178711\n",
      "Iteration 5042, loss = 0.28164114\n",
      "Iteration 5043, loss = 0.28149525\n",
      "Iteration 5044, loss = 0.28134943\n",
      "Iteration 5045, loss = 0.28120370\n",
      "Iteration 5046, loss = 0.28105804\n",
      "Iteration 5047, loss = 0.28091245\n",
      "Iteration 5048, loss = 0.28076695\n",
      "Iteration 5049, loss = 0.28062152\n",
      "Iteration 5050, loss = 0.28047617\n",
      "Iteration 5051, loss = 0.28033090\n",
      "Iteration 5052, loss = 0.28018571\n",
      "Iteration 5053, loss = 0.28004059\n",
      "Iteration 5054, loss = 0.27989555\n",
      "Iteration 5055, loss = 0.27975059\n",
      "Iteration 5056, loss = 0.27960571\n",
      "Iteration 5057, loss = 0.27946091\n",
      "Iteration 5058, loss = 0.27931618\n",
      "Iteration 5059, loss = 0.27917153\n",
      "Iteration 5060, loss = 0.27902696\n",
      "Iteration 5061, loss = 0.27888247\n",
      "Iteration 5062, loss = 0.27873806\n",
      "Iteration 5063, loss = 0.27859372\n",
      "Iteration 5064, loss = 0.27844946\n",
      "Iteration 5065, loss = 0.27830528\n",
      "Iteration 5066, loss = 0.27816118\n",
      "Iteration 5067, loss = 0.27801716\n",
      "Iteration 5068, loss = 0.27787321\n",
      "Iteration 5069, loss = 0.27772935\n",
      "Iteration 5070, loss = 0.27758556\n",
      "Iteration 5071, loss = 0.27744185\n",
      "Iteration 5072, loss = 0.27729821\n",
      "Iteration 5073, loss = 0.27715466\n",
      "Iteration 5074, loss = 0.27701118\n",
      "Iteration 5075, loss = 0.27686779\n",
      "Iteration 5076, loss = 0.27672447\n",
      "Iteration 5077, loss = 0.27658123\n",
      "Iteration 5078, loss = 0.27643807\n",
      "Iteration 5079, loss = 0.27629498\n",
      "Iteration 5080, loss = 0.27615198\n",
      "Iteration 5081, loss = 0.27600905\n",
      "Iteration 5082, loss = 0.27586620\n",
      "Iteration 5083, loss = 0.27572343\n",
      "Iteration 5084, loss = 0.27558074\n",
      "Iteration 5085, loss = 0.27543813\n",
      "Iteration 5086, loss = 0.27529559\n",
      "Iteration 5087, loss = 0.27515313\n",
      "Iteration 5088, loss = 0.27501076\n",
      "Iteration 5089, loss = 0.27486846\n",
      "Iteration 5090, loss = 0.27472624\n",
      "Iteration 5091, loss = 0.27458410\n",
      "Iteration 5092, loss = 0.27444203\n",
      "Iteration 5093, loss = 0.27430005\n",
      "Iteration 5094, loss = 0.27415814\n",
      "Iteration 5095, loss = 0.27401631\n",
      "Iteration 5096, loss = 0.27387456\n",
      "Iteration 5097, loss = 0.27373289\n",
      "Iteration 5098, loss = 0.27359130\n",
      "Iteration 5099, loss = 0.27344979\n",
      "Iteration 5100, loss = 0.27330836\n",
      "Iteration 5101, loss = 0.27316700\n",
      "Iteration 5102, loss = 0.27302572\n",
      "Iteration 5103, loss = 0.27288453\n",
      "Iteration 5104, loss = 0.27274341\n",
      "Iteration 5105, loss = 0.27260237\n",
      "Iteration 5106, loss = 0.27246141\n",
      "Iteration 5107, loss = 0.27232052\n",
      "Iteration 5108, loss = 0.27217972\n",
      "Iteration 5109, loss = 0.27203899\n",
      "Iteration 5110, loss = 0.27189835\n",
      "Iteration 5111, loss = 0.27175778\n",
      "Iteration 5112, loss = 0.27161729\n",
      "Iteration 5113, loss = 0.27147688\n",
      "Iteration 5114, loss = 0.27133655\n",
      "Iteration 5115, loss = 0.27119630\n",
      "Iteration 5116, loss = 0.27105613\n",
      "Iteration 5117, loss = 0.27091603\n",
      "Iteration 5118, loss = 0.27077602\n",
      "Iteration 5119, loss = 0.27063608\n",
      "Iteration 5120, loss = 0.27049622\n",
      "Iteration 5121, loss = 0.27035645\n",
      "Iteration 5122, loss = 0.27021675\n",
      "Iteration 5123, loss = 0.27007713\n",
      "Iteration 5124, loss = 0.26993759\n",
      "Iteration 5125, loss = 0.26979812\n",
      "Iteration 5126, loss = 0.26965874\n",
      "Iteration 5127, loss = 0.26951944\n",
      "Iteration 5128, loss = 0.26938021\n",
      "Iteration 5129, loss = 0.26924106\n",
      "Iteration 5130, loss = 0.26910200\n",
      "Iteration 5131, loss = 0.26896301\n",
      "Iteration 5132, loss = 0.26882410\n",
      "Iteration 5133, loss = 0.26868527\n",
      "Iteration 5134, loss = 0.26854652\n",
      "Iteration 5135, loss = 0.26840785\n",
      "Iteration 5136, loss = 0.26826926\n",
      "Iteration 5137, loss = 0.26813074\n",
      "Iteration 5138, loss = 0.26799231\n",
      "Iteration 5139, loss = 0.26785395\n",
      "Iteration 5140, loss = 0.26771568\n",
      "Iteration 5141, loss = 0.26757748\n",
      "Iteration 5142, loss = 0.26743936\n",
      "Iteration 5143, loss = 0.26730132\n",
      "Iteration 5144, loss = 0.26716336\n",
      "Iteration 5145, loss = 0.26702548\n",
      "Iteration 5146, loss = 0.26688768\n",
      "Iteration 5147, loss = 0.26674996\n",
      "Iteration 5148, loss = 0.26661232\n",
      "Iteration 5149, loss = 0.26647475\n",
      "Iteration 5150, loss = 0.26633727\n",
      "Iteration 5151, loss = 0.26619986\n",
      "Iteration 5152, loss = 0.26606253\n",
      "Iteration 5153, loss = 0.26592529\n",
      "Iteration 5154, loss = 0.26578812\n",
      "Iteration 5155, loss = 0.26565103\n",
      "Iteration 5156, loss = 0.26551402\n",
      "Iteration 5157, loss = 0.26537709\n",
      "Iteration 5158, loss = 0.26524024\n",
      "Iteration 5159, loss = 0.26510347\n",
      "Iteration 5160, loss = 0.26496678\n",
      "Iteration 5161, loss = 0.26483016\n",
      "Iteration 5162, loss = 0.26469363\n",
      "Iteration 5163, loss = 0.26455717\n",
      "Iteration 5164, loss = 0.26442080\n",
      "Iteration 5165, loss = 0.26428450\n",
      "Iteration 5166, loss = 0.26414828\n",
      "Iteration 5167, loss = 0.26401215\n",
      "Iteration 5168, loss = 0.26387609\n",
      "Iteration 5169, loss = 0.26374011\n",
      "Iteration 5170, loss = 0.26360421\n",
      "Iteration 5171, loss = 0.26346839\n",
      "Iteration 5172, loss = 0.26333265\n",
      "Iteration 5173, loss = 0.26319698\n",
      "Iteration 5174, loss = 0.26306140\n",
      "Iteration 5175, loss = 0.26292590\n",
      "Iteration 5176, loss = 0.26279047\n",
      "Iteration 5177, loss = 0.26265513\n",
      "Iteration 5178, loss = 0.26251986\n",
      "Iteration 5179, loss = 0.26238467\n",
      "Iteration 5180, loss = 0.26224957\n",
      "Iteration 5181, loss = 0.26211454\n",
      "Iteration 5182, loss = 0.26197959\n",
      "Iteration 5183, loss = 0.26184472\n",
      "Iteration 5184, loss = 0.26170993\n",
      "Iteration 5185, loss = 0.26157522\n",
      "Iteration 5186, loss = 0.26144058\n",
      "Iteration 5187, loss = 0.26130603\n",
      "Iteration 5188, loss = 0.26117156\n",
      "Iteration 5189, loss = 0.26103716\n",
      "Iteration 5190, loss = 0.26090285\n",
      "Iteration 5191, loss = 0.26076861\n",
      "Iteration 5192, loss = 0.26063446\n",
      "Iteration 5193, loss = 0.26050038\n",
      "Iteration 5194, loss = 0.26036638\n",
      "Iteration 5195, loss = 0.26023246\n",
      "Iteration 5196, loss = 0.26009862\n",
      "Iteration 5197, loss = 0.25996486\n",
      "Iteration 5198, loss = 0.25983118\n",
      "Iteration 5199, loss = 0.25969758\n",
      "Iteration 5200, loss = 0.25956406\n",
      "Iteration 5201, loss = 0.25943062\n",
      "Iteration 5202, loss = 0.25929725\n",
      "Iteration 5203, loss = 0.25916397\n",
      "Iteration 5204, loss = 0.25903076\n",
      "Iteration 5205, loss = 0.25889764\n",
      "Iteration 5206, loss = 0.25876459\n",
      "Iteration 5207, loss = 0.25863162\n",
      "Iteration 5208, loss = 0.25849873\n",
      "Iteration 5209, loss = 0.25836592\n",
      "Iteration 5210, loss = 0.25823319\n",
      "Iteration 5211, loss = 0.25810054\n",
      "Iteration 5212, loss = 0.25796797\n",
      "Iteration 5213, loss = 0.25783548\n",
      "Iteration 5214, loss = 0.25770307\n",
      "Iteration 5215, loss = 0.25757073\n",
      "Iteration 5216, loss = 0.25743848\n",
      "Iteration 5217, loss = 0.25730630\n",
      "Iteration 5218, loss = 0.25717420\n",
      "Iteration 5219, loss = 0.25704219\n",
      "Iteration 5220, loss = 0.25691025\n",
      "Iteration 5221, loss = 0.25677839\n",
      "Iteration 5222, loss = 0.25664661\n",
      "Iteration 5223, loss = 0.25651491\n",
      "Iteration 5224, loss = 0.25638329\n",
      "Iteration 5225, loss = 0.25625175\n",
      "Iteration 5226, loss = 0.25612028\n",
      "Iteration 5227, loss = 0.25598890\n",
      "Iteration 5228, loss = 0.25585759\n",
      "Iteration 5229, loss = 0.25572637\n",
      "Iteration 5230, loss = 0.25559522\n",
      "Iteration 5231, loss = 0.25546415\n",
      "Iteration 5232, loss = 0.25533316\n",
      "Iteration 5233, loss = 0.25520225\n",
      "Iteration 5234, loss = 0.25507142\n",
      "Iteration 5235, loss = 0.25494067\n",
      "Iteration 5236, loss = 0.25481000\n",
      "Iteration 5237, loss = 0.25467941\n",
      "Iteration 5238, loss = 0.25454889\n",
      "Iteration 5239, loss = 0.25441846\n",
      "Iteration 5240, loss = 0.25428810\n",
      "Iteration 5241, loss = 0.25415783\n",
      "Iteration 5242, loss = 0.25402763\n",
      "Iteration 5243, loss = 0.25389751\n",
      "Iteration 5244, loss = 0.25376747\n",
      "Iteration 5245, loss = 0.25363751\n",
      "Iteration 5246, loss = 0.25350763\n",
      "Iteration 5247, loss = 0.25337782\n",
      "Iteration 5248, loss = 0.25324810\n",
      "Iteration 5249, loss = 0.25311845\n",
      "Iteration 5250, loss = 0.25298889\n",
      "Iteration 5251, loss = 0.25285940\n",
      "Iteration 5252, loss = 0.25272999\n",
      "Iteration 5253, loss = 0.25260066\n",
      "Iteration 5254, loss = 0.25247141\n",
      "Iteration 5255, loss = 0.25234224\n",
      "Iteration 5256, loss = 0.25221315\n",
      "Iteration 5257, loss = 0.25208413\n",
      "Iteration 5258, loss = 0.25195520\n",
      "Iteration 5259, loss = 0.25182634\n",
      "Iteration 5260, loss = 0.25169757\n",
      "Iteration 5261, loss = 0.25156887\n",
      "Iteration 5262, loss = 0.25144025\n",
      "Iteration 5263, loss = 0.25131171\n",
      "Iteration 5264, loss = 0.25118325\n",
      "Iteration 5265, loss = 0.25105486\n",
      "Iteration 5266, loss = 0.25092656\n",
      "Iteration 5267, loss = 0.25079833\n",
      "Iteration 5268, loss = 0.25067019\n",
      "Iteration 5269, loss = 0.25054212\n",
      "Iteration 5270, loss = 0.25041413\n",
      "Iteration 5271, loss = 0.25028622\n",
      "Iteration 5272, loss = 0.25015839\n",
      "Iteration 5273, loss = 0.25003063\n",
      "Iteration 5274, loss = 0.24990296\n",
      "Iteration 5275, loss = 0.24977536\n",
      "Iteration 5276, loss = 0.24964785\n",
      "Iteration 5277, loss = 0.24952041\n",
      "Iteration 5278, loss = 0.24939305\n",
      "Iteration 5279, loss = 0.24926577\n",
      "Iteration 5280, loss = 0.24913857\n",
      "Iteration 5281, loss = 0.24901144\n",
      "Iteration 5282, loss = 0.24888440\n",
      "Iteration 5283, loss = 0.24875743\n",
      "Iteration 5284, loss = 0.24863054\n",
      "Iteration 5285, loss = 0.24850373\n",
      "Iteration 5286, loss = 0.24837700\n",
      "Iteration 5287, loss = 0.24825035\n",
      "Iteration 5288, loss = 0.24812377\n",
      "Iteration 5289, loss = 0.24799728\n",
      "Iteration 5290, loss = 0.24787086\n",
      "Iteration 5291, loss = 0.24774452\n",
      "Iteration 5292, loss = 0.24761826\n",
      "Iteration 5293, loss = 0.24749208\n",
      "Iteration 5294, loss = 0.24736598\n",
      "Iteration 5295, loss = 0.24723995\n",
      "Iteration 5296, loss = 0.24711401\n",
      "Iteration 5297, loss = 0.24698814\n",
      "Iteration 5298, loss = 0.24686235\n",
      "Iteration 5299, loss = 0.24673664\n",
      "Iteration 5300, loss = 0.24661101\n",
      "Iteration 5301, loss = 0.24648545\n",
      "Iteration 5302, loss = 0.24635998\n",
      "Iteration 5303, loss = 0.24623458\n",
      "Iteration 5304, loss = 0.24610926\n",
      "Iteration 5305, loss = 0.24598402\n",
      "Iteration 5306, loss = 0.24585885\n",
      "Iteration 5307, loss = 0.24573377\n",
      "Iteration 5308, loss = 0.24560876\n",
      "Iteration 5309, loss = 0.24548383\n",
      "Iteration 5310, loss = 0.24535898\n",
      "Iteration 5311, loss = 0.24523421\n",
      "Iteration 5312, loss = 0.24510952\n",
      "Iteration 5313, loss = 0.24498490\n",
      "Iteration 5314, loss = 0.24486036\n",
      "Iteration 5315, loss = 0.24473590\n",
      "Iteration 5316, loss = 0.24461152\n",
      "Iteration 5317, loss = 0.24448722\n",
      "Iteration 5318, loss = 0.24436299\n",
      "Iteration 5319, loss = 0.24423884\n",
      "Iteration 5320, loss = 0.24411477\n",
      "Iteration 5321, loss = 0.24399078\n",
      "Iteration 5322, loss = 0.24386687\n",
      "Iteration 5323, loss = 0.24374303\n",
      "Iteration 5324, loss = 0.24361928\n",
      "Iteration 5325, loss = 0.24349560\n",
      "Iteration 5326, loss = 0.24337199\n",
      "Iteration 5327, loss = 0.24324847\n",
      "Iteration 5328, loss = 0.24312502\n",
      "Iteration 5329, loss = 0.24300166\n",
      "Iteration 5330, loss = 0.24287837\n",
      "Iteration 5331, loss = 0.24275515\n",
      "Iteration 5332, loss = 0.24263202\n",
      "Iteration 5333, loss = 0.24250896\n",
      "Iteration 5334, loss = 0.24238598\n",
      "Iteration 5335, loss = 0.24226308\n",
      "Iteration 5336, loss = 0.24214026\n",
      "Iteration 5337, loss = 0.24201751\n",
      "Iteration 5338, loss = 0.24189484\n",
      "Iteration 5339, loss = 0.24177225\n",
      "Iteration 5340, loss = 0.24164974\n",
      "Iteration 5341, loss = 0.24152730\n",
      "Iteration 5342, loss = 0.24140495\n",
      "Iteration 5343, loss = 0.24128267\n",
      "Iteration 5344, loss = 0.24116046\n",
      "Iteration 5345, loss = 0.24103834\n",
      "Iteration 5346, loss = 0.24091629\n",
      "Iteration 5347, loss = 0.24079432\n",
      "Iteration 5348, loss = 0.24067243\n",
      "Iteration 5349, loss = 0.24055061\n",
      "Iteration 5350, loss = 0.24042887\n",
      "Iteration 5351, loss = 0.24030721\n",
      "Iteration 5352, loss = 0.24018563\n",
      "Iteration 5353, loss = 0.24006413\n",
      "Iteration 5354, loss = 0.23994270\n",
      "Iteration 5355, loss = 0.23982135\n",
      "Iteration 5356, loss = 0.23970007\n",
      "Iteration 5357, loss = 0.23957888\n",
      "Iteration 5358, loss = 0.23945776\n",
      "Iteration 5359, loss = 0.23933672\n",
      "Iteration 5360, loss = 0.23921575\n",
      "Iteration 5361, loss = 0.23909486\n",
      "Iteration 5362, loss = 0.23897405\n",
      "Iteration 5363, loss = 0.23885332\n",
      "Iteration 5364, loss = 0.23873267\n",
      "Iteration 5365, loss = 0.23861209\n",
      "Iteration 5366, loss = 0.23849159\n",
      "Iteration 5367, loss = 0.23837116\n",
      "Iteration 5368, loss = 0.23825081\n",
      "Iteration 5369, loss = 0.23813054\n",
      "Iteration 5370, loss = 0.23801035\n",
      "Iteration 5371, loss = 0.23789023\n",
      "Iteration 5372, loss = 0.23777020\n",
      "Iteration 5373, loss = 0.23765023\n",
      "Iteration 5374, loss = 0.23753035\n",
      "Iteration 5375, loss = 0.23741054\n",
      "Iteration 5376, loss = 0.23729081\n",
      "Iteration 5377, loss = 0.23717115\n",
      "Iteration 5378, loss = 0.23705158\n",
      "Iteration 5379, loss = 0.23693207\n",
      "Iteration 5380, loss = 0.23681265\n",
      "Iteration 5381, loss = 0.23669330\n",
      "Iteration 5382, loss = 0.23657403\n",
      "Iteration 5383, loss = 0.23645484\n",
      "Iteration 5384, loss = 0.23633572\n",
      "Iteration 5385, loss = 0.23621668\n",
      "Iteration 5386, loss = 0.23609772\n",
      "Iteration 5387, loss = 0.23597883\n",
      "Iteration 5388, loss = 0.23586002\n",
      "Iteration 5389, loss = 0.23574128\n",
      "Iteration 5390, loss = 0.23562263\n",
      "Iteration 5391, loss = 0.23550404\n",
      "Iteration 5392, loss = 0.23538554\n",
      "Iteration 5393, loss = 0.23526711\n",
      "Iteration 5394, loss = 0.23514876\n",
      "Iteration 5395, loss = 0.23503048\n",
      "Iteration 5396, loss = 0.23491229\n",
      "Iteration 5397, loss = 0.23479416\n",
      "Iteration 5398, loss = 0.23467612\n",
      "Iteration 5399, loss = 0.23455815\n",
      "Iteration 5400, loss = 0.23444025\n",
      "Iteration 5401, loss = 0.23432244\n",
      "Iteration 5402, loss = 0.23420470\n",
      "Iteration 5403, loss = 0.23408703\n",
      "Iteration 5404, loss = 0.23396944\n",
      "Iteration 5405, loss = 0.23385193\n",
      "Iteration 5406, loss = 0.23373449\n",
      "Iteration 5407, loss = 0.23361713\n",
      "Iteration 5408, loss = 0.23349985\n",
      "Iteration 5409, loss = 0.23338264\n",
      "Iteration 5410, loss = 0.23326551\n",
      "Iteration 5411, loss = 0.23314845\n",
      "Iteration 5412, loss = 0.23303147\n",
      "Iteration 5413, loss = 0.23291457\n",
      "Iteration 5414, loss = 0.23279774\n",
      "Iteration 5415, loss = 0.23268099\n",
      "Iteration 5416, loss = 0.23256432\n",
      "Iteration 5417, loss = 0.23244772\n",
      "Iteration 5418, loss = 0.23233119\n",
      "Iteration 5419, loss = 0.23221474\n",
      "Iteration 5420, loss = 0.23209837\n",
      "Iteration 5421, loss = 0.23198207\n",
      "Iteration 5422, loss = 0.23186585\n",
      "Iteration 5423, loss = 0.23174971\n",
      "Iteration 5424, loss = 0.23163364\n",
      "Iteration 5425, loss = 0.23151764\n",
      "Iteration 5426, loss = 0.23140172\n",
      "Iteration 5427, loss = 0.23128588\n",
      "Iteration 5428, loss = 0.23117011\n",
      "Iteration 5429, loss = 0.23105442\n",
      "Iteration 5430, loss = 0.23093881\n",
      "Iteration 5431, loss = 0.23082327\n",
      "Iteration 5432, loss = 0.23070780\n",
      "Iteration 5433, loss = 0.23059241\n",
      "Iteration 5434, loss = 0.23047710\n",
      "Iteration 5435, loss = 0.23036186\n",
      "Iteration 5436, loss = 0.23024669\n",
      "Iteration 5437, loss = 0.23013161\n",
      "Iteration 5438, loss = 0.23001659\n",
      "Iteration 5439, loss = 0.22990166\n",
      "Iteration 5440, loss = 0.22978679\n",
      "Iteration 5441, loss = 0.22967201\n",
      "Iteration 5442, loss = 0.22955730\n",
      "Iteration 5443, loss = 0.22944266\n",
      "Iteration 5444, loss = 0.22932810\n",
      "Iteration 5445, loss = 0.22921361\n",
      "Iteration 5446, loss = 0.22909920\n",
      "Iteration 5447, loss = 0.22898486\n",
      "Iteration 5448, loss = 0.22887060\n",
      "Iteration 5449, loss = 0.22875641\n",
      "Iteration 5450, loss = 0.22864230\n",
      "Iteration 5451, loss = 0.22852827\n",
      "Iteration 5452, loss = 0.22841431\n",
      "Iteration 5453, loss = 0.22830042\n",
      "Iteration 5454, loss = 0.22818661\n",
      "Iteration 5455, loss = 0.22807287\n",
      "Iteration 5456, loss = 0.22795921\n",
      "Iteration 5457, loss = 0.22784562\n",
      "Iteration 5458, loss = 0.22773211\n",
      "Iteration 5459, loss = 0.22761867\n",
      "Iteration 5460, loss = 0.22750531\n",
      "Iteration 5461, loss = 0.22739202\n",
      "Iteration 5462, loss = 0.22727880\n",
      "Iteration 5463, loss = 0.22716567\n",
      "Iteration 5464, loss = 0.22705260\n",
      "Iteration 5465, loss = 0.22693961\n",
      "Iteration 5466, loss = 0.22682669\n",
      "Iteration 5467, loss = 0.22671385\n",
      "Iteration 5468, loss = 0.22660109\n",
      "Iteration 5469, loss = 0.22648839\n",
      "Iteration 5470, loss = 0.22637578\n",
      "Iteration 5471, loss = 0.22626323\n",
      "Iteration 5472, loss = 0.22615076\n",
      "Iteration 5473, loss = 0.22603837\n",
      "Iteration 5474, loss = 0.22592605\n",
      "Iteration 5475, loss = 0.22581380\n",
      "Iteration 5476, loss = 0.22570163\n",
      "Iteration 5477, loss = 0.22558953\n",
      "Iteration 5478, loss = 0.22547751\n",
      "Iteration 5479, loss = 0.22536556\n",
      "Iteration 5480, loss = 0.22525368\n",
      "Iteration 5481, loss = 0.22514188\n",
      "Iteration 5482, loss = 0.22503015\n",
      "Iteration 5483, loss = 0.22491850\n",
      "Iteration 5484, loss = 0.22480692\n",
      "Iteration 5485, loss = 0.22469541\n",
      "Iteration 5486, loss = 0.22458398\n",
      "Iteration 5487, loss = 0.22447262\n",
      "Iteration 5488, loss = 0.22436134\n",
      "Iteration 5489, loss = 0.22425013\n",
      "Iteration 5490, loss = 0.22413899\n",
      "Iteration 5491, loss = 0.22402793\n",
      "Iteration 5492, loss = 0.22391694\n",
      "Iteration 5493, loss = 0.22380602\n",
      "Iteration 5494, loss = 0.22369518\n",
      "Iteration 5495, loss = 0.22358441\n",
      "Iteration 5496, loss = 0.22347372\n",
      "Iteration 5497, loss = 0.22336310\n",
      "Iteration 5498, loss = 0.22325255\n",
      "Iteration 5499, loss = 0.22314207\n",
      "Iteration 5500, loss = 0.22303167\n",
      "Iteration 5501, loss = 0.22292135\n",
      "Iteration 5502, loss = 0.22281109\n",
      "Iteration 5503, loss = 0.22270091\n",
      "Iteration 5504, loss = 0.22259080\n",
      "Iteration 5505, loss = 0.22248077\n",
      "Iteration 5506, loss = 0.22237081\n",
      "Iteration 5507, loss = 0.22226092\n",
      "Iteration 5508, loss = 0.22215111\n",
      "Iteration 5509, loss = 0.22204137\n",
      "Iteration 5510, loss = 0.22193170\n",
      "Iteration 5511, loss = 0.22182210\n",
      "Iteration 5512, loss = 0.22171258\n",
      "Iteration 5513, loss = 0.22160313\n",
      "Iteration 5514, loss = 0.22149376\n",
      "Iteration 5515, loss = 0.22138446\n",
      "Iteration 5516, loss = 0.22127523\n",
      "Iteration 5517, loss = 0.22116607\n",
      "Iteration 5518, loss = 0.22105699\n",
      "Iteration 5519, loss = 0.22094797\n",
      "Iteration 5520, loss = 0.22083904\n",
      "Iteration 5521, loss = 0.22073017\n",
      "Iteration 5522, loss = 0.22062138\n",
      "Iteration 5523, loss = 0.22051266\n",
      "Iteration 5524, loss = 0.22040401\n",
      "Iteration 5525, loss = 0.22029544\n",
      "Iteration 5526, loss = 0.22018693\n",
      "Iteration 5527, loss = 0.22007850\n",
      "Iteration 5528, loss = 0.21997015\n",
      "Iteration 5529, loss = 0.21986186\n",
      "Iteration 5530, loss = 0.21975365\n",
      "Iteration 5531, loss = 0.21964551\n",
      "Iteration 5532, loss = 0.21953744\n",
      "Iteration 5533, loss = 0.21942945\n",
      "Iteration 5534, loss = 0.21932153\n",
      "Iteration 5535, loss = 0.21921368\n",
      "Iteration 5536, loss = 0.21910590\n",
      "Iteration 5537, loss = 0.21899819\n",
      "Iteration 5538, loss = 0.21889056\n",
      "Iteration 5539, loss = 0.21878300\n",
      "Iteration 5540, loss = 0.21867551\n",
      "Iteration 5541, loss = 0.21856809\n",
      "Iteration 5542, loss = 0.21846075\n",
      "Iteration 5543, loss = 0.21835348\n",
      "Iteration 5544, loss = 0.21824628\n",
      "Iteration 5545, loss = 0.21813915\n",
      "Iteration 5546, loss = 0.21803209\n",
      "Iteration 5547, loss = 0.21792511\n",
      "Iteration 5548, loss = 0.21781819\n",
      "Iteration 5549, loss = 0.21771135\n",
      "Iteration 5550, loss = 0.21760458\n",
      "Iteration 5551, loss = 0.21749788\n",
      "Iteration 5552, loss = 0.21739126\n",
      "Iteration 5553, loss = 0.21728470\n",
      "Iteration 5554, loss = 0.21717822\n",
      "Iteration 5555, loss = 0.21707181\n",
      "Iteration 5556, loss = 0.21696547\n",
      "Iteration 5557, loss = 0.21685920\n",
      "Iteration 5558, loss = 0.21675301\n",
      "Iteration 5559, loss = 0.21664688\n",
      "Iteration 5560, loss = 0.21654083\n",
      "Iteration 5561, loss = 0.21643485\n",
      "Iteration 5562, loss = 0.21632894\n",
      "Iteration 5563, loss = 0.21622310\n",
      "Iteration 5564, loss = 0.21611733\n",
      "Iteration 5565, loss = 0.21601164\n",
      "Iteration 5566, loss = 0.21590601\n",
      "Iteration 5567, loss = 0.21580046\n",
      "Iteration 5568, loss = 0.21569497\n",
      "Iteration 5569, loss = 0.21558956\n",
      "Iteration 5570, loss = 0.21548422\n",
      "Iteration 5571, loss = 0.21537895\n",
      "Iteration 5572, loss = 0.21527375\n",
      "Iteration 5573, loss = 0.21516863\n",
      "Iteration 5574, loss = 0.21506357\n",
      "Iteration 5575, loss = 0.21495858\n",
      "Iteration 5576, loss = 0.21485367\n",
      "Iteration 5577, loss = 0.21474883\n",
      "Iteration 5578, loss = 0.21464405\n",
      "Iteration 5579, loss = 0.21453935\n",
      "Iteration 5580, loss = 0.21443472\n",
      "Iteration 5581, loss = 0.21433016\n",
      "Iteration 5582, loss = 0.21422567\n",
      "Iteration 5583, loss = 0.21412125\n",
      "Iteration 5584, loss = 0.21401690\n",
      "Iteration 5585, loss = 0.21391262\n",
      "Iteration 5586, loss = 0.21380842\n",
      "Iteration 5587, loss = 0.21370428\n",
      "Iteration 5588, loss = 0.21360021\n",
      "Iteration 5589, loss = 0.21349622\n",
      "Iteration 5590, loss = 0.21339229\n",
      "Iteration 5591, loss = 0.21328844\n",
      "Iteration 5592, loss = 0.21318465\n",
      "Iteration 5593, loss = 0.21308094\n",
      "Iteration 5594, loss = 0.21297729\n",
      "Iteration 5595, loss = 0.21287372\n",
      "Iteration 5596, loss = 0.21277021\n",
      "Iteration 5597, loss = 0.21266678\n",
      "Iteration 5598, loss = 0.21256342\n",
      "Iteration 5599, loss = 0.21246012\n",
      "Iteration 5600, loss = 0.21235690\n",
      "Iteration 5601, loss = 0.21225375\n",
      "Iteration 5602, loss = 0.21215066\n",
      "Iteration 5603, loss = 0.21204765\n",
      "Iteration 5604, loss = 0.21194471\n",
      "Iteration 5605, loss = 0.21184183\n",
      "Iteration 5606, loss = 0.21173903\n",
      "Iteration 5607, loss = 0.21163629\n",
      "Iteration 5608, loss = 0.21153363\n",
      "Iteration 5609, loss = 0.21143103\n",
      "Iteration 5610, loss = 0.21132851\n",
      "Iteration 5611, loss = 0.21122605\n",
      "Iteration 5612, loss = 0.21112367\n",
      "Iteration 5613, loss = 0.21102135\n",
      "Iteration 5614, loss = 0.21091910\n",
      "Iteration 5615, loss = 0.21081693\n",
      "Iteration 5616, loss = 0.21071482\n",
      "Iteration 5617, loss = 0.21061278\n",
      "Iteration 5618, loss = 0.21051081\n",
      "Iteration 5619, loss = 0.21040891\n",
      "Iteration 5620, loss = 0.21030708\n",
      "Iteration 5621, loss = 0.21020532\n",
      "Iteration 5622, loss = 0.21010363\n",
      "Iteration 5623, loss = 0.21000200\n",
      "Iteration 5624, loss = 0.20990045\n",
      "Iteration 5625, loss = 0.20979897\n",
      "Iteration 5626, loss = 0.20969755\n",
      "Iteration 5627, loss = 0.20959621\n",
      "Iteration 5628, loss = 0.20949493\n",
      "Iteration 5629, loss = 0.20939372\n",
      "Iteration 5630, loss = 0.20929258\n",
      "Iteration 5631, loss = 0.20919151\n",
      "Iteration 5632, loss = 0.20909051\n",
      "Iteration 5633, loss = 0.20898958\n",
      "Iteration 5634, loss = 0.20888871\n",
      "Iteration 5635, loss = 0.20878792\n",
      "Iteration 5636, loss = 0.20868719\n",
      "Iteration 5637, loss = 0.20858653\n",
      "Iteration 5638, loss = 0.20848594\n",
      "Iteration 5639, loss = 0.20838542\n",
      "Iteration 5640, loss = 0.20828497\n",
      "Iteration 5641, loss = 0.20818459\n",
      "Iteration 5642, loss = 0.20808427\n",
      "Iteration 5643, loss = 0.20798403\n",
      "Iteration 5644, loss = 0.20788385\n",
      "Iteration 5645, loss = 0.20778374\n",
      "Iteration 5646, loss = 0.20768370\n",
      "Iteration 5647, loss = 0.20758372\n",
      "Iteration 5648, loss = 0.20748382\n",
      "Iteration 5649, loss = 0.20738398\n",
      "Iteration 5650, loss = 0.20728421\n",
      "Iteration 5651, loss = 0.20718451\n",
      "Iteration 5652, loss = 0.20708488\n",
      "Iteration 5653, loss = 0.20698532\n",
      "Iteration 5654, loss = 0.20688582\n",
      "Iteration 5655, loss = 0.20678639\n",
      "Iteration 5656, loss = 0.20668703\n",
      "Iteration 5657, loss = 0.20658774\n",
      "Iteration 5658, loss = 0.20648852\n",
      "Iteration 5659, loss = 0.20638936\n",
      "Iteration 5660, loss = 0.20629027\n",
      "Iteration 5661, loss = 0.20619125\n",
      "Iteration 5662, loss = 0.20609230\n",
      "Iteration 5663, loss = 0.20599341\n",
      "Iteration 5664, loss = 0.20589459\n",
      "Iteration 5665, loss = 0.20579584\n",
      "Iteration 5666, loss = 0.20569716\n",
      "Iteration 5667, loss = 0.20559855\n",
      "Iteration 5668, loss = 0.20550000\n",
      "Iteration 5669, loss = 0.20540152\n",
      "Iteration 5670, loss = 0.20530311\n",
      "Iteration 5671, loss = 0.20520476\n",
      "Iteration 5672, loss = 0.20510648\n",
      "Iteration 5673, loss = 0.20500827\n",
      "Iteration 5674, loss = 0.20491013\n",
      "Iteration 5675, loss = 0.20481205\n",
      "Iteration 5676, loss = 0.20471404\n",
      "Iteration 5677, loss = 0.20461610\n",
      "Iteration 5678, loss = 0.20451823\n",
      "Iteration 5679, loss = 0.20442042\n",
      "Iteration 5680, loss = 0.20432268\n",
      "Iteration 5681, loss = 0.20422501\n",
      "Iteration 5682, loss = 0.20412740\n",
      "Iteration 5683, loss = 0.20402986\n",
      "Iteration 5684, loss = 0.20393239\n",
      "Iteration 5685, loss = 0.20383498\n",
      "Iteration 5686, loss = 0.20373764\n",
      "Iteration 5687, loss = 0.20364037\n",
      "Iteration 5688, loss = 0.20354316\n",
      "Iteration 5689, loss = 0.20344603\n",
      "Iteration 5690, loss = 0.20334895\n",
      "Iteration 5691, loss = 0.20325195\n",
      "Iteration 5692, loss = 0.20315501\n",
      "Iteration 5693, loss = 0.20305814\n",
      "Iteration 5694, loss = 0.20296133\n",
      "Iteration 5695, loss = 0.20286459\n",
      "Iteration 5696, loss = 0.20276792\n",
      "Iteration 5697, loss = 0.20267131\n",
      "Iteration 5698, loss = 0.20257477\n",
      "Iteration 5699, loss = 0.20247830\n",
      "Iteration 5700, loss = 0.20238189\n",
      "Iteration 5701, loss = 0.20228555\n",
      "Iteration 5702, loss = 0.20218927\n",
      "Iteration 5703, loss = 0.20209306\n",
      "Iteration 5704, loss = 0.20199692\n",
      "Iteration 5705, loss = 0.20190084\n",
      "Iteration 5706, loss = 0.20180483\n",
      "Iteration 5707, loss = 0.20170889\n",
      "Iteration 5708, loss = 0.20161301\n",
      "Iteration 5709, loss = 0.20151720\n",
      "Iteration 5710, loss = 0.20142145\n",
      "Iteration 5711, loss = 0.20132577\n",
      "Iteration 5712, loss = 0.20123015\n",
      "Iteration 5713, loss = 0.20113460\n",
      "Iteration 5714, loss = 0.20103912\n",
      "Iteration 5715, loss = 0.20094370\n",
      "Iteration 5716, loss = 0.20084835\n",
      "Iteration 5717, loss = 0.20075306\n",
      "Iteration 5718, loss = 0.20065784\n",
      "Iteration 5719, loss = 0.20056268\n",
      "Iteration 5720, loss = 0.20046759\n",
      "Iteration 5721, loss = 0.20037257\n",
      "Iteration 5722, loss = 0.20027761\n",
      "Iteration 5723, loss = 0.20018272\n",
      "Iteration 5724, loss = 0.20008789\n",
      "Iteration 5725, loss = 0.19999312\n",
      "Iteration 5726, loss = 0.19989842\n",
      "Iteration 5727, loss = 0.19980379\n",
      "Iteration 5728, loss = 0.19970922\n",
      "Iteration 5729, loss = 0.19961472\n",
      "Iteration 5730, loss = 0.19952028\n",
      "Iteration 5731, loss = 0.19942591\n",
      "Iteration 5732, loss = 0.19933160\n",
      "Iteration 5733, loss = 0.19923736\n",
      "Iteration 5734, loss = 0.19914318\n",
      "Iteration 5735, loss = 0.19904907\n",
      "Iteration 5736, loss = 0.19895502\n",
      "Iteration 5737, loss = 0.19886104\n",
      "Iteration 5738, loss = 0.19876712\n",
      "Iteration 5739, loss = 0.19867327\n",
      "Iteration 5740, loss = 0.19857948\n",
      "Iteration 5741, loss = 0.19848575\n",
      "Iteration 5742, loss = 0.19839209\n",
      "Iteration 5743, loss = 0.19829850\n",
      "Iteration 5744, loss = 0.19820497\n",
      "Iteration 5745, loss = 0.19811150\n",
      "Iteration 5746, loss = 0.19801810\n",
      "Iteration 5747, loss = 0.19792476\n",
      "Iteration 5748, loss = 0.19783149\n",
      "Iteration 5749, loss = 0.19773828\n",
      "Iteration 5750, loss = 0.19764514\n",
      "Iteration 5751, loss = 0.19755206\n",
      "Iteration 5752, loss = 0.19745904\n",
      "Iteration 5753, loss = 0.19736609\n",
      "Iteration 5754, loss = 0.19727321\n",
      "Iteration 5755, loss = 0.19718038\n",
      "Iteration 5756, loss = 0.19708762\n",
      "Iteration 5757, loss = 0.19699493\n",
      "Iteration 5758, loss = 0.19690230\n",
      "Iteration 5759, loss = 0.19680973\n",
      "Iteration 5760, loss = 0.19671723\n",
      "Iteration 5761, loss = 0.19662479\n",
      "Iteration 5762, loss = 0.19653241\n",
      "Iteration 5763, loss = 0.19644010\n",
      "Iteration 5764, loss = 0.19634785\n",
      "Iteration 5765, loss = 0.19625567\n",
      "Iteration 5766, loss = 0.19616355\n",
      "Iteration 5767, loss = 0.19607149\n",
      "Iteration 5768, loss = 0.19597950\n",
      "Iteration 5769, loss = 0.19588757\n",
      "Iteration 5770, loss = 0.19579570\n",
      "Iteration 5771, loss = 0.19570390\n",
      "Iteration 5772, loss = 0.19561216\n",
      "Iteration 5773, loss = 0.19552049\n",
      "Iteration 5774, loss = 0.19542887\n",
      "Iteration 5775, loss = 0.19533732\n",
      "Iteration 5776, loss = 0.19524584\n",
      "Iteration 5777, loss = 0.19515442\n",
      "Iteration 5778, loss = 0.19506306\n",
      "Iteration 5779, loss = 0.19497176\n",
      "Iteration 5780, loss = 0.19488053\n",
      "Iteration 5781, loss = 0.19478936\n",
      "Iteration 5782, loss = 0.19469825\n",
      "Iteration 5783, loss = 0.19460721\n",
      "Iteration 5784, loss = 0.19451622\n",
      "Iteration 5785, loss = 0.19442531\n",
      "Iteration 5786, loss = 0.19433445\n",
      "Iteration 5787, loss = 0.19424366\n",
      "Iteration 5788, loss = 0.19415293\n",
      "Iteration 5789, loss = 0.19406226\n",
      "Iteration 5790, loss = 0.19397166\n",
      "Iteration 5791, loss = 0.19388112\n",
      "Iteration 5792, loss = 0.19379064\n",
      "Iteration 5793, loss = 0.19370022\n",
      "Iteration 5794, loss = 0.19360987\n",
      "Iteration 5795, loss = 0.19351958\n",
      "Iteration 5796, loss = 0.19342935\n",
      "Iteration 5797, loss = 0.19333918\n",
      "Iteration 5798, loss = 0.19324908\n",
      "Iteration 5799, loss = 0.19315904\n",
      "Iteration 5800, loss = 0.19306906\n",
      "Iteration 5801, loss = 0.19297914\n",
      "Iteration 5802, loss = 0.19288929\n",
      "Iteration 5803, loss = 0.19279949\n",
      "Iteration 5804, loss = 0.19270976\n",
      "Iteration 5805, loss = 0.19262009\n",
      "Iteration 5806, loss = 0.19253049\n",
      "Iteration 5807, loss = 0.19244094\n",
      "Iteration 5808, loss = 0.19235146\n",
      "Iteration 5809, loss = 0.19226204\n",
      "Iteration 5810, loss = 0.19217268\n",
      "Iteration 5811, loss = 0.19208339\n",
      "Iteration 5812, loss = 0.19199415\n",
      "Iteration 5813, loss = 0.19190498\n",
      "Iteration 5814, loss = 0.19181587\n",
      "Iteration 5815, loss = 0.19172682\n",
      "Iteration 5816, loss = 0.19163783\n",
      "Iteration 5817, loss = 0.19154891\n",
      "Iteration 5818, loss = 0.19146004\n",
      "Iteration 5819, loss = 0.19137124\n",
      "Iteration 5820, loss = 0.19128250\n",
      "Iteration 5821, loss = 0.19119382\n",
      "Iteration 5822, loss = 0.19110520\n",
      "Iteration 5823, loss = 0.19101664\n",
      "Iteration 5824, loss = 0.19092815\n",
      "Iteration 5825, loss = 0.19083971\n",
      "Iteration 5826, loss = 0.19075134\n",
      "Iteration 5827, loss = 0.19066303\n",
      "Iteration 5828, loss = 0.19057478\n",
      "Iteration 5829, loss = 0.19048659\n",
      "Iteration 5830, loss = 0.19039846\n",
      "Iteration 5831, loss = 0.19031039\n",
      "Iteration 5832, loss = 0.19022239\n",
      "Iteration 5833, loss = 0.19013444\n",
      "Iteration 5834, loss = 0.19004656\n",
      "Iteration 5835, loss = 0.18995873\n",
      "Iteration 5836, loss = 0.18987097\n",
      "Iteration 5837, loss = 0.18978327\n",
      "Iteration 5838, loss = 0.18969563\n",
      "Iteration 5839, loss = 0.18960805\n",
      "Iteration 5840, loss = 0.18952053\n",
      "Iteration 5841, loss = 0.18943307\n",
      "Iteration 5842, loss = 0.18934567\n",
      "Iteration 5843, loss = 0.18925834\n",
      "Iteration 5844, loss = 0.18917106\n",
      "Iteration 5845, loss = 0.18908384\n",
      "Iteration 5846, loss = 0.18899669\n",
      "Iteration 5847, loss = 0.18890959\n",
      "Iteration 5848, loss = 0.18882256\n",
      "Iteration 5849, loss = 0.18873558\n",
      "Iteration 5850, loss = 0.18864867\n",
      "Iteration 5851, loss = 0.18856181\n",
      "Iteration 5852, loss = 0.18847502\n",
      "Iteration 5853, loss = 0.18838829\n",
      "Iteration 5854, loss = 0.18830161\n",
      "Iteration 5855, loss = 0.18821500\n",
      "Iteration 5856, loss = 0.18812845\n",
      "Iteration 5857, loss = 0.18804195\n",
      "Iteration 5858, loss = 0.18795552\n",
      "Iteration 5859, loss = 0.18786915\n",
      "Iteration 5860, loss = 0.18778283\n",
      "Iteration 5861, loss = 0.18769658\n",
      "Iteration 5862, loss = 0.18761039\n",
      "Iteration 5863, loss = 0.18752425\n",
      "Iteration 5864, loss = 0.18743818\n",
      "Iteration 5865, loss = 0.18735216\n",
      "Iteration 5866, loss = 0.18726621\n",
      "Iteration 5867, loss = 0.18718031\n",
      "Iteration 5868, loss = 0.18709448\n",
      "Iteration 5869, loss = 0.18700870\n",
      "Iteration 5870, loss = 0.18692299\n",
      "Iteration 5871, loss = 0.18683733\n",
      "Iteration 5872, loss = 0.18675173\n",
      "Iteration 5873, loss = 0.18666619\n",
      "Iteration 5874, loss = 0.18658072\n",
      "Iteration 5875, loss = 0.18649530\n",
      "Iteration 5876, loss = 0.18640994\n",
      "Iteration 5877, loss = 0.18632463\n",
      "Iteration 5878, loss = 0.18623939\n",
      "Iteration 5879, loss = 0.18615421\n",
      "Iteration 5880, loss = 0.18606909\n",
      "Iteration 5881, loss = 0.18598402\n",
      "Iteration 5882, loss = 0.18589902\n",
      "Iteration 5883, loss = 0.18581407\n",
      "Iteration 5884, loss = 0.18572918\n",
      "Iteration 5885, loss = 0.18564436\n",
      "Iteration 5886, loss = 0.18555959\n",
      "Iteration 5887, loss = 0.18547488\n",
      "Iteration 5888, loss = 0.18539022\n",
      "Iteration 5889, loss = 0.18530563\n",
      "Iteration 5890, loss = 0.18522110\n",
      "Iteration 5891, loss = 0.18513662\n",
      "Iteration 5892, loss = 0.18505220\n",
      "Iteration 5893, loss = 0.18496784\n",
      "Iteration 5894, loss = 0.18488354\n",
      "Iteration 5895, loss = 0.18479930\n",
      "Iteration 5896, loss = 0.18471512\n",
      "Iteration 5897, loss = 0.18463099\n",
      "Iteration 5898, loss = 0.18454693\n",
      "Iteration 5899, loss = 0.18446292\n",
      "Iteration 5900, loss = 0.18437897\n",
      "Iteration 5901, loss = 0.18429508\n",
      "Iteration 5902, loss = 0.18421125\n",
      "Iteration 5903, loss = 0.18412747\n",
      "Iteration 5904, loss = 0.18404376\n",
      "Iteration 5905, loss = 0.18396010\n",
      "Iteration 5906, loss = 0.18387650\n",
      "Iteration 5907, loss = 0.18379295\n",
      "Iteration 5908, loss = 0.18370947\n",
      "Iteration 5909, loss = 0.18362604\n",
      "Iteration 5910, loss = 0.18354267\n",
      "Iteration 5911, loss = 0.18345936\n",
      "Iteration 5912, loss = 0.18337611\n",
      "Iteration 5913, loss = 0.18329292\n",
      "Iteration 5914, loss = 0.18320978\n",
      "Iteration 5915, loss = 0.18312670\n",
      "Iteration 5916, loss = 0.18304368\n",
      "Iteration 5917, loss = 0.18296071\n",
      "Iteration 5918, loss = 0.18287781\n",
      "Iteration 5919, loss = 0.18279496\n",
      "Iteration 5920, loss = 0.18271217\n",
      "Iteration 5921, loss = 0.18262943\n",
      "Iteration 5922, loss = 0.18254675\n",
      "Iteration 5923, loss = 0.18246414\n",
      "Iteration 5924, loss = 0.18238157\n",
      "Iteration 5925, loss = 0.18229907\n",
      "Iteration 5926, loss = 0.18221662\n",
      "Iteration 5927, loss = 0.18213423\n",
      "Iteration 5928, loss = 0.18205190\n",
      "Iteration 5929, loss = 0.18196962\n",
      "Iteration 5930, loss = 0.18188740\n",
      "Iteration 5931, loss = 0.18180524\n",
      "Iteration 5932, loss = 0.18172314\n",
      "Iteration 5933, loss = 0.18164109\n",
      "Iteration 5934, loss = 0.18155910\n",
      "Iteration 5935, loss = 0.18147717\n",
      "Iteration 5936, loss = 0.18139529\n",
      "Iteration 5937, loss = 0.18131347\n",
      "Iteration 5938, loss = 0.18123171\n",
      "Iteration 5939, loss = 0.18115000\n",
      "Iteration 5940, loss = 0.18106835\n",
      "Iteration 5941, loss = 0.18098676\n",
      "Iteration 5942, loss = 0.18090522\n",
      "Iteration 5943, loss = 0.18082374\n",
      "Iteration 5944, loss = 0.18074232\n",
      "Iteration 5945, loss = 0.18066095\n",
      "Iteration 5946, loss = 0.18057964\n",
      "Iteration 5947, loss = 0.18049839\n",
      "Iteration 5948, loss = 0.18041719\n",
      "Iteration 5949, loss = 0.18033605\n",
      "Iteration 5950, loss = 0.18025497\n",
      "Iteration 5951, loss = 0.18017394\n",
      "Iteration 5952, loss = 0.18009297\n",
      "Iteration 5953, loss = 0.18001205\n",
      "Iteration 5954, loss = 0.17993119\n",
      "Iteration 5955, loss = 0.17985039\n",
      "Iteration 5956, loss = 0.17976964\n",
      "Iteration 5957, loss = 0.17968895\n",
      "Iteration 5958, loss = 0.17960831\n",
      "Iteration 5959, loss = 0.17952773\n",
      "Iteration 5960, loss = 0.17944721\n",
      "Iteration 5961, loss = 0.17936674\n",
      "Iteration 5962, loss = 0.17928633\n",
      "Iteration 5963, loss = 0.17920598\n",
      "Iteration 5964, loss = 0.17912568\n",
      "Iteration 5965, loss = 0.17904543\n",
      "Iteration 5966, loss = 0.17896524\n",
      "Iteration 5967, loss = 0.17888511\n",
      "Iteration 5968, loss = 0.17880503\n",
      "Iteration 5969, loss = 0.17872501\n",
      "Iteration 5970, loss = 0.17864504\n",
      "Iteration 5971, loss = 0.17856513\n",
      "Iteration 5972, loss = 0.17848528\n",
      "Iteration 5973, loss = 0.17840548\n",
      "Iteration 5974, loss = 0.17832573\n",
      "Iteration 5975, loss = 0.17824604\n",
      "Iteration 5976, loss = 0.17816641\n",
      "Iteration 5977, loss = 0.17808683\n",
      "Iteration 5978, loss = 0.17800731\n",
      "Iteration 5979, loss = 0.17792784\n",
      "Iteration 5980, loss = 0.17784843\n",
      "Iteration 5981, loss = 0.17776907\n",
      "Iteration 5982, loss = 0.17768977\n",
      "Iteration 5983, loss = 0.17761052\n",
      "Iteration 5984, loss = 0.17753133\n",
      "Iteration 5985, loss = 0.17745219\n",
      "Iteration 5986, loss = 0.17737310\n",
      "Iteration 5987, loss = 0.17729408\n",
      "Iteration 5988, loss = 0.17721510\n",
      "Iteration 5989, loss = 0.17713618\n",
      "Iteration 5990, loss = 0.17705732\n",
      "Iteration 5991, loss = 0.17697851\n",
      "Iteration 5992, loss = 0.17689976\n",
      "Iteration 5993, loss = 0.17682106\n",
      "Iteration 5994, loss = 0.17674241\n",
      "Iteration 5995, loss = 0.17666382\n",
      "Iteration 5996, loss = 0.17658529\n",
      "Iteration 5997, loss = 0.17650680\n",
      "Iteration 5998, loss = 0.17642838\n",
      "Iteration 5999, loss = 0.17635000\n",
      "Iteration 6000, loss = 0.17627169\n",
      "Iteration 6001, loss = 0.17619342\n",
      "Iteration 6002, loss = 0.17611521\n",
      "Iteration 6003, loss = 0.17603706\n",
      "Iteration 6004, loss = 0.17595895\n",
      "Iteration 6005, loss = 0.17588091\n",
      "Iteration 6006, loss = 0.17580291\n",
      "Iteration 6007, loss = 0.17572497\n",
      "Iteration 6008, loss = 0.17564709\n",
      "Iteration 6009, loss = 0.17556926\n",
      "Iteration 6010, loss = 0.17549148\n",
      "Iteration 6011, loss = 0.17541376\n",
      "Iteration 6012, loss = 0.17533609\n",
      "Iteration 6013, loss = 0.17525847\n",
      "Iteration 6014, loss = 0.17518091\n",
      "Iteration 6015, loss = 0.17510341\n",
      "Iteration 6016, loss = 0.17502595\n",
      "Iteration 6017, loss = 0.17494855\n",
      "Iteration 6018, loss = 0.17487120\n",
      "Iteration 6019, loss = 0.17479391\n",
      "Iteration 6020, loss = 0.17471667\n",
      "Iteration 6021, loss = 0.17463949\n",
      "Iteration 6022, loss = 0.17456235\n",
      "Iteration 6023, loss = 0.17448528\n",
      "Iteration 6024, loss = 0.17440825\n",
      "Iteration 6025, loss = 0.17433128\n",
      "Iteration 6026, loss = 0.17425436\n",
      "Iteration 6027, loss = 0.17417749\n",
      "Iteration 6028, loss = 0.17410068\n",
      "Iteration 6029, loss = 0.17402392\n",
      "Iteration 6030, loss = 0.17394722\n",
      "Iteration 6031, loss = 0.17387056\n",
      "Iteration 6032, loss = 0.17379396\n",
      "Iteration 6033, loss = 0.17371742\n",
      "Iteration 6034, loss = 0.17364092\n",
      "Iteration 6035, loss = 0.17356448\n",
      "Iteration 6036, loss = 0.17348810\n",
      "Iteration 6037, loss = 0.17341176\n",
      "Iteration 6038, loss = 0.17333548\n",
      "Iteration 6039, loss = 0.17325925\n",
      "Iteration 6040, loss = 0.17318308\n",
      "Iteration 6041, loss = 0.17310695\n",
      "Iteration 6042, loss = 0.17303088\n",
      "Iteration 6043, loss = 0.17295487\n",
      "Iteration 6044, loss = 0.17287890\n",
      "Iteration 6045, loss = 0.17280299\n",
      "Iteration 6046, loss = 0.17272713\n",
      "Iteration 6047, loss = 0.17265132\n",
      "Iteration 6048, loss = 0.17257557\n",
      "Iteration 6049, loss = 0.17249986\n",
      "Iteration 6050, loss = 0.17242421\n",
      "Iteration 6051, loss = 0.17234862\n",
      "Iteration 6052, loss = 0.17227307\n",
      "Iteration 6053, loss = 0.17219758\n",
      "Iteration 6054, loss = 0.17212214\n",
      "Iteration 6055, loss = 0.17204675\n",
      "Iteration 6056, loss = 0.17197141\n",
      "Iteration 6057, loss = 0.17189613\n",
      "Iteration 6058, loss = 0.17182090\n",
      "Iteration 6059, loss = 0.17174572\n",
      "Iteration 6060, loss = 0.17167059\n",
      "Iteration 6061, loss = 0.17159551\n",
      "Iteration 6062, loss = 0.17152049\n",
      "Iteration 6063, loss = 0.17144552\n",
      "Iteration 6064, loss = 0.17137060\n",
      "Iteration 6065, loss = 0.17129573\n",
      "Iteration 6066, loss = 0.17122091\n",
      "Iteration 6067, loss = 0.17114615\n",
      "Iteration 6068, loss = 0.17107144\n",
      "Iteration 6069, loss = 0.17099677\n",
      "Iteration 6070, loss = 0.17092217\n",
      "Iteration 6071, loss = 0.17084761\n",
      "Iteration 6072, loss = 0.17077310\n",
      "Iteration 6073, loss = 0.17069865\n",
      "Iteration 6074, loss = 0.17062424\n",
      "Iteration 6075, loss = 0.17054989\n",
      "Iteration 6076, loss = 0.17047559\n",
      "Iteration 6077, loss = 0.17040134\n",
      "Iteration 6078, loss = 0.17032714\n",
      "Iteration 6079, loss = 0.17025300\n",
      "Iteration 6080, loss = 0.17017890\n",
      "Iteration 6081, loss = 0.17010486\n",
      "Iteration 6082, loss = 0.17003087\n",
      "Iteration 6083, loss = 0.16995693\n",
      "Iteration 6084, loss = 0.16988303\n",
      "Iteration 6085, loss = 0.16980920\n",
      "Iteration 6086, loss = 0.16973541\n",
      "Iteration 6087, loss = 0.16966167\n",
      "Iteration 6088, loss = 0.16958798\n",
      "Iteration 6089, loss = 0.16951435\n",
      "Iteration 6090, loss = 0.16944076\n",
      "Iteration 6091, loss = 0.16936723\n",
      "Iteration 6092, loss = 0.16929375\n",
      "Iteration 6093, loss = 0.16922032\n",
      "Iteration 6094, loss = 0.16914694\n",
      "Iteration 6095, loss = 0.16907361\n",
      "Iteration 6096, loss = 0.16900033\n",
      "Iteration 6097, loss = 0.16892710\n",
      "Iteration 6098, loss = 0.16885392\n",
      "Iteration 6099, loss = 0.16878079\n",
      "Iteration 6100, loss = 0.16870771\n",
      "Iteration 6101, loss = 0.16863469\n",
      "Iteration 6102, loss = 0.16856171\n",
      "Iteration 6103, loss = 0.16848878\n",
      "Iteration 6104, loss = 0.16841591\n",
      "Iteration 6105, loss = 0.16834308\n",
      "Iteration 6106, loss = 0.16827031\n",
      "Iteration 6107, loss = 0.16819758\n",
      "Iteration 6108, loss = 0.16812491\n",
      "Iteration 6109, loss = 0.16805228\n",
      "Iteration 6110, loss = 0.16797971\n",
      "Iteration 6111, loss = 0.16790719\n",
      "Iteration 6112, loss = 0.16783471\n",
      "Iteration 6113, loss = 0.16776229\n",
      "Iteration 6114, loss = 0.16768991\n",
      "Iteration 6115, loss = 0.16761759\n",
      "Iteration 6116, loss = 0.16754532\n",
      "Iteration 6117, loss = 0.16747309\n",
      "Iteration 6118, loss = 0.16740092\n",
      "Iteration 6119, loss = 0.16732879\n",
      "Iteration 6120, loss = 0.16725672\n",
      "Iteration 6121, loss = 0.16718469\n",
      "Iteration 6122, loss = 0.16711272\n",
      "Iteration 6123, loss = 0.16704079\n",
      "Iteration 6124, loss = 0.16696892\n",
      "Iteration 6125, loss = 0.16689709\n",
      "Iteration 6126, loss = 0.16682531\n",
      "Iteration 6127, loss = 0.16675358\n",
      "Iteration 6128, loss = 0.16668191\n",
      "Iteration 6129, loss = 0.16661028\n",
      "Iteration 6130, loss = 0.16653870\n",
      "Iteration 6131, loss = 0.16646717\n",
      "Iteration 6132, loss = 0.16639569\n",
      "Iteration 6133, loss = 0.16632426\n",
      "Iteration 6134, loss = 0.16625288\n",
      "Iteration 6135, loss = 0.16618154\n",
      "Iteration 6136, loss = 0.16611026\n",
      "Iteration 6137, loss = 0.16603903\n",
      "Iteration 6138, loss = 0.16596784\n",
      "Iteration 6139, loss = 0.16589670\n",
      "Iteration 6140, loss = 0.16582562\n",
      "Iteration 6141, loss = 0.16575458\n",
      "Iteration 6142, loss = 0.16568359\n",
      "Iteration 6143, loss = 0.16561265\n",
      "Iteration 6144, loss = 0.16554176\n",
      "Iteration 6145, loss = 0.16547092\n",
      "Iteration 6146, loss = 0.16540012\n",
      "Iteration 6147, loss = 0.16532938\n",
      "Iteration 6148, loss = 0.16525868\n",
      "Iteration 6149, loss = 0.16518803\n",
      "Iteration 6150, loss = 0.16511743\n",
      "Iteration 6151, loss = 0.16504688\n",
      "Iteration 6152, loss = 0.16497638\n",
      "Iteration 6153, loss = 0.16490593\n",
      "Iteration 6154, loss = 0.16483553\n",
      "Iteration 6155, loss = 0.16476517\n",
      "Iteration 6156, loss = 0.16469486\n",
      "Iteration 6157, loss = 0.16462460\n",
      "Iteration 6158, loss = 0.16455439\n",
      "Iteration 6159, loss = 0.16448423\n",
      "Iteration 6160, loss = 0.16441411\n",
      "Iteration 6161, loss = 0.16434405\n",
      "Iteration 6162, loss = 0.16427403\n",
      "Iteration 6163, loss = 0.16420406\n",
      "Iteration 6164, loss = 0.16413414\n",
      "Iteration 6165, loss = 0.16406427\n",
      "Iteration 6166, loss = 0.16399444\n",
      "Iteration 6167, loss = 0.16392466\n",
      "Iteration 6168, loss = 0.16385494\n",
      "Iteration 6169, loss = 0.16378525\n",
      "Iteration 6170, loss = 0.16371562\n",
      "Iteration 6171, loss = 0.16364604\n",
      "Iteration 6172, loss = 0.16357650\n",
      "Iteration 6173, loss = 0.16350701\n",
      "Iteration 6174, loss = 0.16343757\n",
      "Iteration 6175, loss = 0.16336817\n",
      "Iteration 6176, loss = 0.16329883\n",
      "Iteration 6177, loss = 0.16322953\n",
      "Iteration 6178, loss = 0.16316028\n",
      "Iteration 6179, loss = 0.16309107\n",
      "Iteration 6180, loss = 0.16302192\n",
      "Iteration 6181, loss = 0.16295281\n",
      "Iteration 6182, loss = 0.16288375\n",
      "Iteration 6183, loss = 0.16281473\n",
      "Iteration 6184, loss = 0.16274577\n",
      "Iteration 6185, loss = 0.16267685\n",
      "Iteration 6186, loss = 0.16260798\n",
      "Iteration 6187, loss = 0.16253916\n",
      "Iteration 6188, loss = 0.16247038\n",
      "Iteration 6189, loss = 0.16240165\n",
      "Iteration 6190, loss = 0.16233297\n",
      "Iteration 6191, loss = 0.16226433\n",
      "Iteration 6192, loss = 0.16219574\n",
      "Iteration 6193, loss = 0.16212720\n",
      "Iteration 6194, loss = 0.16205871\n",
      "Iteration 6195, loss = 0.16199026\n",
      "Iteration 6196, loss = 0.16192186\n",
      "Iteration 6197, loss = 0.16185351\n",
      "Iteration 6198, loss = 0.16178521\n",
      "Iteration 6199, loss = 0.16171695\n",
      "Iteration 6200, loss = 0.16164873\n",
      "Iteration 6201, loss = 0.16158057\n",
      "Iteration 6202, loss = 0.16151245\n",
      "Iteration 6203, loss = 0.16144438\n",
      "Iteration 6204, loss = 0.16137635\n",
      "Iteration 6205, loss = 0.16130838\n",
      "Iteration 6206, loss = 0.16124044\n",
      "Iteration 6207, loss = 0.16117256\n",
      "Iteration 6208, loss = 0.16110472\n",
      "Iteration 6209, loss = 0.16103693\n",
      "Iteration 6210, loss = 0.16096918\n",
      "Iteration 6211, loss = 0.16090148\n",
      "Iteration 6212, loss = 0.16083383\n",
      "Iteration 6213, loss = 0.16076622\n",
      "Iteration 6214, loss = 0.16069866\n",
      "Iteration 6215, loss = 0.16063115\n",
      "Iteration 6216, loss = 0.16056368\n",
      "Iteration 6217, loss = 0.16049626\n",
      "Iteration 6218, loss = 0.16042889\n",
      "Iteration 6219, loss = 0.16036156\n",
      "Iteration 6220, loss = 0.16029428\n",
      "Iteration 6221, loss = 0.16022704\n",
      "Iteration 6222, loss = 0.16015985\n",
      "Iteration 6223, loss = 0.16009271\n",
      "Iteration 6224, loss = 0.16002561\n",
      "Iteration 6225, loss = 0.15995856\n",
      "Iteration 6226, loss = 0.15989155\n",
      "Iteration 6227, loss = 0.15982459\n",
      "Iteration 6228, loss = 0.15975767\n",
      "Iteration 6229, loss = 0.15969080\n",
      "Iteration 6230, loss = 0.15962398\n",
      "Iteration 6231, loss = 0.15955720\n",
      "Iteration 6232, loss = 0.15949047\n",
      "Iteration 6233, loss = 0.15942379\n",
      "Iteration 6234, loss = 0.15935715\n",
      "Iteration 6235, loss = 0.15929055\n",
      "Iteration 6236, loss = 0.15922400\n",
      "Iteration 6237, loss = 0.15915750\n",
      "Iteration 6238, loss = 0.15909104\n",
      "Iteration 6239, loss = 0.15902463\n",
      "Iteration 6240, loss = 0.15895826\n",
      "Iteration 6241, loss = 0.15889194\n",
      "Iteration 6242, loss = 0.15882566\n",
      "Iteration 6243, loss = 0.15875943\n",
      "Iteration 6244, loss = 0.15869325\n",
      "Iteration 6245, loss = 0.15862711\n",
      "Iteration 6246, loss = 0.15856101\n",
      "Iteration 6247, loss = 0.15849496\n",
      "Iteration 6248, loss = 0.15842896\n",
      "Iteration 6249, loss = 0.15836300\n",
      "Iteration 6250, loss = 0.15829708\n",
      "Iteration 6251, loss = 0.15823121\n",
      "Iteration 6252, loss = 0.15816539\n",
      "Iteration 6253, loss = 0.15809961\n",
      "Iteration 6254, loss = 0.15803387\n",
      "Iteration 6255, loss = 0.15796818\n",
      "Iteration 6256, loss = 0.15790254\n",
      "Iteration 6257, loss = 0.15783694\n",
      "Iteration 6258, loss = 0.15777138\n",
      "Iteration 6259, loss = 0.15770587\n",
      "Iteration 6260, loss = 0.15764041\n",
      "Iteration 6261, loss = 0.15757499\n",
      "Iteration 6262, loss = 0.15750961\n",
      "Iteration 6263, loss = 0.15744428\n",
      "Iteration 6264, loss = 0.15737899\n",
      "Iteration 6265, loss = 0.15731375\n",
      "Iteration 6266, loss = 0.15724855\n",
      "Iteration 6267, loss = 0.15718340\n",
      "Iteration 6268, loss = 0.15711829\n",
      "Iteration 6269, loss = 0.15705322\n",
      "Iteration 6270, loss = 0.15698820\n",
      "Iteration 6271, loss = 0.15692323\n",
      "Iteration 6272, loss = 0.15685830\n",
      "Iteration 6273, loss = 0.15679341\n",
      "Iteration 6274, loss = 0.15672856\n",
      "Iteration 6275, loss = 0.15666377\n",
      "Iteration 6276, loss = 0.15659901\n",
      "Iteration 6277, loss = 0.15653430\n",
      "Iteration 6278, loss = 0.15646963\n",
      "Iteration 6279, loss = 0.15640501\n",
      "Iteration 6280, loss = 0.15634043\n",
      "Iteration 6281, loss = 0.15627590\n",
      "Iteration 6282, loss = 0.15621141\n",
      "Iteration 6283, loss = 0.15614696\n",
      "Iteration 6284, loss = 0.15608256\n",
      "Iteration 6285, loss = 0.15601820\n",
      "Iteration 6286, loss = 0.15595389\n",
      "Iteration 6287, loss = 0.15588962\n",
      "Iteration 6288, loss = 0.15582539\n",
      "Iteration 6289, loss = 0.15576121\n",
      "Iteration 6290, loss = 0.15569707\n",
      "Iteration 6291, loss = 0.15563297\n",
      "Iteration 6292, loss = 0.15556892\n",
      "Iteration 6293, loss = 0.15550491\n",
      "Iteration 6294, loss = 0.15544095\n",
      "Iteration 6295, loss = 0.15537703\n",
      "Iteration 6296, loss = 0.15531315\n",
      "Iteration 6297, loss = 0.15524931\n",
      "Iteration 6298, loss = 0.15518552\n",
      "Iteration 6299, loss = 0.15512177\n",
      "Iteration 6300, loss = 0.15505807\n",
      "Iteration 6301, loss = 0.15499441\n",
      "Iteration 6302, loss = 0.15493079\n",
      "Iteration 6303, loss = 0.15486722\n",
      "Iteration 6304, loss = 0.15480369\n",
      "Iteration 6305, loss = 0.15474020\n",
      "Iteration 6306, loss = 0.15467675\n",
      "Iteration 6307, loss = 0.15461335\n",
      "Iteration 6308, loss = 0.15454999\n",
      "Iteration 6309, loss = 0.15448668\n",
      "Iteration 6310, loss = 0.15442341\n",
      "Iteration 6311, loss = 0.15436018\n",
      "Iteration 6312, loss = 0.15429699\n",
      "Iteration 6313, loss = 0.15423385\n",
      "Iteration 6314, loss = 0.15417075\n",
      "Iteration 6315, loss = 0.15410769\n",
      "Iteration 6316, loss = 0.15404467\n",
      "Iteration 6317, loss = 0.15398170\n",
      "Iteration 6318, loss = 0.15391877\n",
      "Iteration 6319, loss = 0.15385589\n",
      "Iteration 6320, loss = 0.15379304\n",
      "Iteration 6321, loss = 0.15373024\n",
      "Iteration 6322, loss = 0.15366748\n",
      "Iteration 6323, loss = 0.15360477\n",
      "Iteration 6324, loss = 0.15354209\n",
      "Iteration 6325, loss = 0.15347946\n",
      "Iteration 6326, loss = 0.15341687\n",
      "Iteration 6327, loss = 0.15335433\n",
      "Iteration 6328, loss = 0.15329182\n",
      "Iteration 6329, loss = 0.15322936\n",
      "Iteration 6330, loss = 0.15316694\n",
      "Iteration 6331, loss = 0.15310457\n",
      "Iteration 6332, loss = 0.15304223\n",
      "Iteration 6333, loss = 0.15297994\n",
      "Iteration 6334, loss = 0.15291769\n",
      "Iteration 6335, loss = 0.15285549\n",
      "Iteration 6336, loss = 0.15279332\n",
      "Iteration 6337, loss = 0.15273120\n",
      "Iteration 6338, loss = 0.15266912\n",
      "Iteration 6339, loss = 0.15260708\n",
      "Iteration 6340, loss = 0.15254508\n",
      "Iteration 6341, loss = 0.15248313\n",
      "Iteration 6342, loss = 0.15242121\n",
      "Iteration 6343, loss = 0.15235934\n",
      "Iteration 6344, loss = 0.15229751\n",
      "Iteration 6345, loss = 0.15223573\n",
      "Iteration 6346, loss = 0.15217398\n",
      "Iteration 6347, loss = 0.15211228\n",
      "Iteration 6348, loss = 0.15205062\n",
      "Iteration 6349, loss = 0.15198900\n",
      "Iteration 6350, loss = 0.15192742\n",
      "Iteration 6351, loss = 0.15186588\n",
      "Iteration 6352, loss = 0.15180439\n",
      "Iteration 6353, loss = 0.15174294\n",
      "Iteration 6354, loss = 0.15168152\n",
      "Iteration 6355, loss = 0.15162015\n",
      "Iteration 6356, loss = 0.15155883\n",
      "Iteration 6357, loss = 0.15149754\n",
      "Iteration 6358, loss = 0.15143629\n",
      "Iteration 6359, loss = 0.15137509\n",
      "Iteration 6360, loss = 0.15131393\n",
      "Iteration 6361, loss = 0.15125280\n",
      "Iteration 6362, loss = 0.15119172\n",
      "Iteration 6363, loss = 0.15113069\n",
      "Iteration 6364, loss = 0.15106969\n",
      "Iteration 6365, loss = 0.15100873\n",
      "Iteration 6366, loss = 0.15094782\n",
      "Iteration 6367, loss = 0.15088694\n",
      "Iteration 6368, loss = 0.15082611\n",
      "Iteration 6369, loss = 0.15076532\n",
      "Iteration 6370, loss = 0.15070457\n",
      "Iteration 6371, loss = 0.15064386\n",
      "Iteration 6372, loss = 0.15058319\n",
      "Iteration 6373, loss = 0.15052256\n",
      "Iteration 6374, loss = 0.15046198\n",
      "Iteration 6375, loss = 0.15040143\n",
      "Iteration 6376, loss = 0.15034092\n",
      "Iteration 6377, loss = 0.15028046\n",
      "Iteration 6378, loss = 0.15022004\n",
      "Iteration 6379, loss = 0.15015966\n",
      "Iteration 6380, loss = 0.15009931\n",
      "Iteration 6381, loss = 0.15003901\n",
      "Iteration 6382, loss = 0.14997875\n",
      "Iteration 6383, loss = 0.14991853\n",
      "Iteration 6384, loss = 0.14985835\n",
      "Iteration 6385, loss = 0.14979821\n",
      "Iteration 6386, loss = 0.14973812\n",
      "Iteration 6387, loss = 0.14967806\n",
      "Iteration 6388, loss = 0.14961804\n",
      "Iteration 6389, loss = 0.14955807\n",
      "Iteration 6390, loss = 0.14949813\n",
      "Iteration 6391, loss = 0.14943823\n",
      "Iteration 6392, loss = 0.14937838\n",
      "Iteration 6393, loss = 0.14931856\n",
      "Iteration 6394, loss = 0.14925879\n",
      "Iteration 6395, loss = 0.14919905\n",
      "Iteration 6396, loss = 0.14913936\n",
      "Iteration 6397, loss = 0.14907971\n",
      "Iteration 6398, loss = 0.14902009\n",
      "Iteration 6399, loss = 0.14896052\n",
      "Iteration 6400, loss = 0.14890099\n",
      "Iteration 6401, loss = 0.14884149\n",
      "Iteration 6402, loss = 0.14878204\n",
      "Iteration 6403, loss = 0.14872263\n",
      "Iteration 6404, loss = 0.14866325\n",
      "Iteration 6405, loss = 0.14860392\n",
      "Iteration 6406, loss = 0.14854463\n",
      "Iteration 6407, loss = 0.14848537\n",
      "Iteration 6408, loss = 0.14842616\n",
      "Iteration 6409, loss = 0.14836698\n",
      "Iteration 6410, loss = 0.14830785\n",
      "Iteration 6411, loss = 0.14824875\n",
      "Iteration 6412, loss = 0.14818970\n",
      "Iteration 6413, loss = 0.14813069\n",
      "Iteration 6414, loss = 0.14807171\n",
      "Iteration 6415, loss = 0.14801277\n",
      "Iteration 6416, loss = 0.14795388\n",
      "Iteration 6417, loss = 0.14789502\n",
      "Iteration 6418, loss = 0.14783620\n",
      "Iteration 6419, loss = 0.14777743\n",
      "Iteration 6420, loss = 0.14771869\n",
      "Iteration 6421, loss = 0.14765999\n",
      "Iteration 6422, loss = 0.14760133\n",
      "Iteration 6423, loss = 0.14754271\n",
      "Iteration 6424, loss = 0.14748413\n",
      "Iteration 6425, loss = 0.14742559\n",
      "Iteration 6426, loss = 0.14736709\n",
      "Iteration 6427, loss = 0.14730863\n",
      "Iteration 6428, loss = 0.14725020\n",
      "Iteration 6429, loss = 0.14719182\n",
      "Iteration 6430, loss = 0.14713347\n",
      "Iteration 6431, loss = 0.14707517\n",
      "Iteration 6432, loss = 0.14701690\n",
      "Iteration 6433, loss = 0.14695867\n",
      "Iteration 6434, loss = 0.14690049\n",
      "Iteration 6435, loss = 0.14684234\n",
      "Iteration 6436, loss = 0.14678423\n",
      "Iteration 6437, loss = 0.14672615\n",
      "Iteration 6438, loss = 0.14666812\n",
      "Iteration 6439, loss = 0.14661013\n",
      "Iteration 6440, loss = 0.14655217\n",
      "Iteration 6441, loss = 0.14649426\n",
      "Iteration 6442, loss = 0.14643638\n",
      "Iteration 6443, loss = 0.14637854\n",
      "Iteration 6444, loss = 0.14632074\n",
      "Iteration 6445, loss = 0.14626298\n",
      "Iteration 6446, loss = 0.14620526\n",
      "Iteration 6447, loss = 0.14614758\n",
      "Iteration 6448, loss = 0.14608993\n",
      "Iteration 6449, loss = 0.14603232\n",
      "Iteration 6450, loss = 0.14597476\n",
      "Iteration 6451, loss = 0.14591723\n",
      "Iteration 6452, loss = 0.14585974\n",
      "Iteration 6453, loss = 0.14580228\n",
      "Iteration 6454, loss = 0.14574487\n",
      "Iteration 6455, loss = 0.14568749\n",
      "Iteration 6456, loss = 0.14563016\n",
      "Iteration 6457, loss = 0.14557286\n",
      "Iteration 6458, loss = 0.14551560\n",
      "Iteration 6459, loss = 0.14545838\n",
      "Iteration 6460, loss = 0.14540119\n",
      "Iteration 6461, loss = 0.14534405\n",
      "Iteration 6462, loss = 0.14528694\n",
      "Iteration 6463, loss = 0.14522987\n",
      "Iteration 6464, loss = 0.14517284\n",
      "Iteration 6465, loss = 0.14511584\n",
      "Iteration 6466, loss = 0.14505889\n",
      "Iteration 6467, loss = 0.14500197\n",
      "Iteration 6468, loss = 0.14494509\n",
      "Iteration 6469, loss = 0.14488825\n",
      "Iteration 6470, loss = 0.14483145\n",
      "Iteration 6471, loss = 0.14477468\n",
      "Iteration 6472, loss = 0.14471796\n",
      "Iteration 6473, loss = 0.14466127\n",
      "Iteration 6474, loss = 0.14460462\n",
      "Iteration 6475, loss = 0.14454800\n",
      "Iteration 6476, loss = 0.14449143\n",
      "Iteration 6477, loss = 0.14443489\n",
      "Iteration 6478, loss = 0.14437839\n",
      "Iteration 6479, loss = 0.14432192\n",
      "Iteration 6480, loss = 0.14426550\n",
      "Iteration 6481, loss = 0.14420911\n",
      "Iteration 6482, loss = 0.14415276\n",
      "Iteration 6483, loss = 0.14409645\n",
      "Iteration 6484, loss = 0.14404017\n",
      "Iteration 6485, loss = 0.14398394\n",
      "Iteration 6486, loss = 0.14392774\n",
      "Iteration 6487, loss = 0.14387158\n",
      "Iteration 6488, loss = 0.14381545\n",
      "Iteration 6489, loss = 0.14375936\n",
      "Iteration 6490, loss = 0.14370331\n",
      "Iteration 6491, loss = 0.14364730\n",
      "Iteration 6492, loss = 0.14359132\n",
      "Iteration 6493, loss = 0.14353539\n",
      "Iteration 6494, loss = 0.14347948\n",
      "Iteration 6495, loss = 0.14342362\n",
      "Iteration 6496, loss = 0.14336779\n",
      "Iteration 6497, loss = 0.14331200\n",
      "Iteration 6498, loss = 0.14325625\n",
      "Iteration 6499, loss = 0.14320054\n",
      "Iteration 6500, loss = 0.14314486\n",
      "Iteration 6501, loss = 0.14308922\n",
      "Iteration 6502, loss = 0.14303361\n",
      "Iteration 6503, loss = 0.14297804\n",
      "Iteration 6504, loss = 0.14292251\n",
      "Iteration 6505, loss = 0.14286702\n",
      "Iteration 6506, loss = 0.14281156\n",
      "Iteration 6507, loss = 0.14275614\n",
      "Iteration 6508, loss = 0.14270076\n",
      "Iteration 6509, loss = 0.14264542\n",
      "Iteration 6510, loss = 0.14259011\n",
      "Iteration 6511, loss = 0.14253483\n",
      "Iteration 6512, loss = 0.14247960\n",
      "Iteration 6513, loss = 0.14242440\n",
      "Iteration 6514, loss = 0.14236924\n",
      "Iteration 6515, loss = 0.14231411\n",
      "Iteration 6516, loss = 0.14225902\n",
      "Iteration 6517, loss = 0.14220397\n",
      "Iteration 6518, loss = 0.14214895\n",
      "Iteration 6519, loss = 0.14209397\n",
      "Iteration 6520, loss = 0.14203903\n",
      "Iteration 6521, loss = 0.14198412\n",
      "Iteration 6522, loss = 0.14192925\n",
      "Iteration 6523, loss = 0.14187442\n",
      "Iteration 6524, loss = 0.14181962\n",
      "Iteration 6525, loss = 0.14176486\n",
      "Iteration 6526, loss = 0.14171013\n",
      "Iteration 6527, loss = 0.14165545\n",
      "Iteration 6528, loss = 0.14160079\n",
      "Iteration 6529, loss = 0.14154618\n",
      "Iteration 6530, loss = 0.14149160\n",
      "Iteration 6531, loss = 0.14143705\n",
      "Iteration 6532, loss = 0.14138255\n",
      "Iteration 6533, loss = 0.14132807\n",
      "Iteration 6534, loss = 0.14127364\n",
      "Iteration 6535, loss = 0.14121924\n",
      "Iteration 6536, loss = 0.14116488\n",
      "Iteration 6537, loss = 0.14111055\n",
      "Iteration 6538, loss = 0.14105626\n",
      "Iteration 6539, loss = 0.14100200\n",
      "Iteration 6540, loss = 0.14094778\n",
      "Iteration 6541, loss = 0.14089360\n",
      "Iteration 6542, loss = 0.14083945\n",
      "Iteration 6543, loss = 0.14078534\n",
      "Iteration 6544, loss = 0.14073126\n",
      "Iteration 6545, loss = 0.14067722\n",
      "Iteration 6546, loss = 0.14062322\n",
      "Iteration 6547, loss = 0.14056925\n",
      "Iteration 6548, loss = 0.14051531\n",
      "Iteration 6549, loss = 0.14046141\n",
      "Iteration 6550, loss = 0.14040755\n",
      "Iteration 6551, loss = 0.14035373\n",
      "Iteration 6552, loss = 0.14029993\n",
      "Iteration 6553, loss = 0.14024618\n",
      "Iteration 6554, loss = 0.14019246\n",
      "Iteration 6555, loss = 0.14013877\n",
      "Iteration 6556, loss = 0.14008513\n",
      "Iteration 6557, loss = 0.14003151\n",
      "Iteration 6558, loss = 0.13997793\n",
      "Iteration 6559, loss = 0.13992439\n",
      "Iteration 6560, loss = 0.13987088\n",
      "Iteration 6561, loss = 0.13981741\n",
      "Iteration 6562, loss = 0.13976397\n",
      "Iteration 6563, loss = 0.13971057\n",
      "Iteration 6564, loss = 0.13965721\n",
      "Iteration 6565, loss = 0.13960387\n",
      "Iteration 6566, loss = 0.13955058\n",
      "Iteration 6567, loss = 0.13949732\n",
      "Iteration 6568, loss = 0.13944409\n",
      "Iteration 6569, loss = 0.13939090\n",
      "Iteration 6570, loss = 0.13933775\n",
      "Iteration 6571, loss = 0.13928462\n",
      "Iteration 6572, loss = 0.13923154\n",
      "Iteration 6573, loss = 0.13917849\n",
      "Iteration 6574, loss = 0.13912547\n",
      "Iteration 6575, loss = 0.13907249\n",
      "Iteration 6576, loss = 0.13901955\n",
      "Iteration 6577, loss = 0.13896664\n",
      "Iteration 6578, loss = 0.13891376\n",
      "Iteration 6579, loss = 0.13886092\n",
      "Iteration 6580, loss = 0.13880811\n",
      "Iteration 6581, loss = 0.13875534\n",
      "Iteration 6582, loss = 0.13870260\n",
      "Iteration 6583, loss = 0.13864990\n",
      "Iteration 6584, loss = 0.13859723\n",
      "Iteration 6585, loss = 0.13854460\n",
      "Iteration 6586, loss = 0.13849200\n",
      "Iteration 6587, loss = 0.13843944\n",
      "Iteration 6588, loss = 0.13838691\n",
      "Iteration 6589, loss = 0.13833441\n",
      "Iteration 6590, loss = 0.13828195\n",
      "Iteration 6591, loss = 0.13822953\n",
      "Iteration 6592, loss = 0.13817713\n",
      "Iteration 6593, loss = 0.13812478\n",
      "Iteration 6594, loss = 0.13807245\n",
      "Iteration 6595, loss = 0.13802017\n",
      "Iteration 6596, loss = 0.13796791\n",
      "Iteration 6597, loss = 0.13791569\n",
      "Iteration 6598, loss = 0.13786351\n",
      "Iteration 6599, loss = 0.13781136\n",
      "Iteration 6600, loss = 0.13775924\n",
      "Iteration 6601, loss = 0.13770716\n",
      "Iteration 6602, loss = 0.13765511\n",
      "Iteration 6603, loss = 0.13760309\n",
      "Iteration 6604, loss = 0.13755111\n",
      "Iteration 6605, loss = 0.13749917\n",
      "Iteration 6606, loss = 0.13744726\n",
      "Iteration 6607, loss = 0.13739538\n",
      "Iteration 6608, loss = 0.13734354\n",
      "Iteration 6609, loss = 0.13729173\n",
      "Iteration 6610, loss = 0.13723995\n",
      "Iteration 6611, loss = 0.13718821\n",
      "Iteration 6612, loss = 0.13713650\n",
      "Iteration 6613, loss = 0.13708483\n",
      "Iteration 6614, loss = 0.13703318\n",
      "Iteration 6615, loss = 0.13698158\n",
      "Iteration 6616, loss = 0.13693001\n",
      "Iteration 6617, loss = 0.13687847\n",
      "Iteration 6618, loss = 0.13682696\n",
      "Iteration 6619, loss = 0.13677549\n",
      "Iteration 6620, loss = 0.13672405\n",
      "Iteration 6621, loss = 0.13667265\n",
      "Iteration 6622, loss = 0.13662128\n",
      "Iteration 6623, loss = 0.13656994\n",
      "Iteration 6624, loss = 0.13651864\n",
      "Iteration 6625, loss = 0.13646737\n",
      "Iteration 6626, loss = 0.13641613\n",
      "Iteration 6627, loss = 0.13636493\n",
      "Iteration 6628, loss = 0.13631376\n",
      "Iteration 6629, loss = 0.13626262\n",
      "Iteration 6630, loss = 0.13621152\n",
      "Iteration 6631, loss = 0.13616045\n",
      "Iteration 6632, loss = 0.13610942\n",
      "Iteration 6633, loss = 0.13605841\n",
      "Iteration 6634, loss = 0.13600745\n",
      "Iteration 6635, loss = 0.13595651\n",
      "Iteration 6636, loss = 0.13590561\n",
      "Iteration 6637, loss = 0.13585474\n",
      "Iteration 6638, loss = 0.13580390\n",
      "Iteration 6639, loss = 0.13575310\n",
      "Iteration 6640, loss = 0.13570233\n",
      "Iteration 6641, loss = 0.13565159\n",
      "Iteration 6642, loss = 0.13560089\n",
      "Iteration 6643, loss = 0.13555022\n",
      "Iteration 6644, loss = 0.13549958\n",
      "Iteration 6645, loss = 0.13544898\n",
      "Iteration 6646, loss = 0.13539841\n",
      "Iteration 6647, loss = 0.13534787\n",
      "Iteration 6648, loss = 0.13529737\n",
      "Iteration 6649, loss = 0.13524689\n",
      "Iteration 6650, loss = 0.13519645\n",
      "Iteration 6651, loss = 0.13514605\n",
      "Iteration 6652, loss = 0.13509567\n",
      "Iteration 6653, loss = 0.13504533\n",
      "Iteration 6654, loss = 0.13499503\n",
      "Iteration 6655, loss = 0.13494475\n",
      "Iteration 6656, loss = 0.13489451\n",
      "Iteration 6657, loss = 0.13484430\n",
      "Iteration 6658, loss = 0.13479412\n",
      "Iteration 6659, loss = 0.13474398\n",
      "Iteration 6660, loss = 0.13469386\n",
      "Iteration 6661, loss = 0.13464379\n",
      "Iteration 6662, loss = 0.13459374\n",
      "Iteration 6663, loss = 0.13454372\n",
      "Iteration 6664, loss = 0.13449374\n",
      "Iteration 6665, loss = 0.13444379\n",
      "Iteration 6666, loss = 0.13439388\n",
      "Iteration 6667, loss = 0.13434399\n",
      "Iteration 6668, loss = 0.13429414\n",
      "Iteration 6669, loss = 0.13424432\n",
      "Iteration 6670, loss = 0.13419454\n",
      "Iteration 6671, loss = 0.13414478\n",
      "Iteration 6672, loss = 0.13409506\n",
      "Iteration 6673, loss = 0.13404537\n",
      "Iteration 6674, loss = 0.13399571\n",
      "Iteration 6675, loss = 0.13394609\n",
      "Iteration 6676, loss = 0.13389649\n",
      "Iteration 6677, loss = 0.13384693\n",
      "Iteration 6678, loss = 0.13379740\n",
      "Iteration 6679, loss = 0.13374791\n",
      "Iteration 6680, loss = 0.13369844\n",
      "Iteration 6681, loss = 0.13364901\n",
      "Iteration 6682, loss = 0.13359961\n",
      "Iteration 6683, loss = 0.13355024\n",
      "Iteration 6684, loss = 0.13350091\n",
      "Iteration 6685, loss = 0.13345160\n",
      "Iteration 6686, loss = 0.13340233\n",
      "Iteration 6687, loss = 0.13335309\n",
      "Iteration 6688, loss = 0.13330388\n",
      "Iteration 6689, loss = 0.13325470\n",
      "Iteration 6690, loss = 0.13320556\n",
      "Iteration 6691, loss = 0.13315645\n",
      "Iteration 6692, loss = 0.13310737\n",
      "Iteration 6693, loss = 0.13305832\n",
      "Iteration 6694, loss = 0.13300930\n",
      "Iteration 6695, loss = 0.13296031\n",
      "Iteration 6696, loss = 0.13291136\n",
      "Iteration 6697, loss = 0.13286244\n",
      "Iteration 6698, loss = 0.13281355\n",
      "Iteration 6699, loss = 0.13276469\n",
      "Iteration 6700, loss = 0.13271586\n",
      "Iteration 6701, loss = 0.13266707\n",
      "Iteration 6702, loss = 0.13261830\n",
      "Iteration 6703, loss = 0.13256957\n",
      "Iteration 6704, loss = 0.13252087\n",
      "Iteration 6705, loss = 0.13247220\n",
      "Iteration 6706, loss = 0.13242356\n",
      "Iteration 6707, loss = 0.13237495\n",
      "Iteration 6708, loss = 0.13232638\n",
      "Iteration 6709, loss = 0.13227783\n",
      "Iteration 6710, loss = 0.13222932\n",
      "Iteration 6711, loss = 0.13218084\n",
      "Iteration 6712, loss = 0.13213239\n",
      "Iteration 6713, loss = 0.13208397\n",
      "Iteration 6714, loss = 0.13203558\n",
      "Iteration 6715, loss = 0.13198723\n",
      "Iteration 6716, loss = 0.13193890\n",
      "Iteration 6717, loss = 0.13189061\n",
      "Iteration 6718, loss = 0.13184235\n",
      "Iteration 6719, loss = 0.13179412\n",
      "Iteration 6720, loss = 0.13174592\n",
      "Iteration 6721, loss = 0.13169775\n",
      "Iteration 6722, loss = 0.13164961\n",
      "Iteration 6723, loss = 0.13160150\n",
      "Iteration 6724, loss = 0.13155343\n",
      "Iteration 6725, loss = 0.13150538\n",
      "Iteration 6726, loss = 0.13145737\n",
      "Iteration 6727, loss = 0.13140938\n",
      "Iteration 6728, loss = 0.13136143\n",
      "Iteration 6729, loss = 0.13131351\n",
      "Iteration 6730, loss = 0.13126562\n",
      "Iteration 6731, loss = 0.13121776\n",
      "Iteration 6732, loss = 0.13116993\n",
      "Iteration 6733, loss = 0.13112213\n",
      "Iteration 6734, loss = 0.13107437\n",
      "Iteration 6735, loss = 0.13102663\n",
      "Iteration 6736, loss = 0.13097893\n",
      "Iteration 6737, loss = 0.13093125\n",
      "Iteration 6738, loss = 0.13088361\n",
      "Iteration 6739, loss = 0.13083599\n",
      "Iteration 6740, loss = 0.13078841\n",
      "Iteration 6741, loss = 0.13074086\n",
      "Iteration 6742, loss = 0.13069334\n",
      "Iteration 6743, loss = 0.13064584\n",
      "Iteration 6744, loss = 0.13059838\n",
      "Iteration 6745, loss = 0.13055095\n",
      "Iteration 6746, loss = 0.13050355\n",
      "Iteration 6747, loss = 0.13045618\n",
      "Iteration 6748, loss = 0.13040884\n",
      "Iteration 6749, loss = 0.13036154\n",
      "Iteration 6750, loss = 0.13031426\n",
      "Iteration 6751, loss = 0.13026701\n",
      "Iteration 6752, loss = 0.13021979\n",
      "Iteration 6753, loss = 0.13017261\n",
      "Iteration 6754, loss = 0.13012545\n",
      "Iteration 6755, loss = 0.13007832\n",
      "Iteration 6756, loss = 0.13003123\n",
      "Iteration 6757, loss = 0.12998416\n",
      "Iteration 6758, loss = 0.12993712\n",
      "Iteration 6759, loss = 0.12989012\n",
      "Iteration 6760, loss = 0.12984314\n",
      "Iteration 6761, loss = 0.12979620\n",
      "Iteration 6762, loss = 0.12974928\n",
      "Iteration 6763, loss = 0.12970240\n",
      "Iteration 6764, loss = 0.12965554\n",
      "Iteration 6765, loss = 0.12960872\n",
      "Iteration 6766, loss = 0.12956192\n",
      "Iteration 6767, loss = 0.12951516\n",
      "Iteration 6768, loss = 0.12946842\n",
      "Iteration 6769, loss = 0.12942172\n",
      "Iteration 6770, loss = 0.12937504\n",
      "Iteration 6771, loss = 0.12932840\n",
      "Iteration 6772, loss = 0.12928178\n",
      "Iteration 6773, loss = 0.12923519\n",
      "Iteration 6774, loss = 0.12918864\n",
      "Iteration 6775, loss = 0.12914211\n",
      "Iteration 6776, loss = 0.12909562\n",
      "Iteration 6777, loss = 0.12904915\n",
      "Iteration 6778, loss = 0.12900271\n",
      "Iteration 6779, loss = 0.12895630\n",
      "Iteration 6780, loss = 0.12890993\n",
      "Iteration 6781, loss = 0.12886358\n",
      "Iteration 6782, loss = 0.12881726\n",
      "Iteration 6783, loss = 0.12877097\n",
      "Iteration 6784, loss = 0.12872471\n",
      "Iteration 6785, loss = 0.12867848\n",
      "Iteration 6786, loss = 0.12863228\n",
      "Iteration 6787, loss = 0.12858611\n",
      "Iteration 6788, loss = 0.12853997\n",
      "Iteration 6789, loss = 0.12849386\n",
      "Iteration 6790, loss = 0.12844778\n",
      "Iteration 6791, loss = 0.12840172\n",
      "Iteration 6792, loss = 0.12835570\n",
      "Iteration 6793, loss = 0.12830971\n",
      "Iteration 6794, loss = 0.12826374\n",
      "Iteration 6795, loss = 0.12821781\n",
      "Iteration 6796, loss = 0.12817190\n",
      "Iteration 6797, loss = 0.12812602\n",
      "Iteration 6798, loss = 0.12808018\n",
      "Iteration 6799, loss = 0.12803436\n",
      "Iteration 6800, loss = 0.12798857\n",
      "Iteration 6801, loss = 0.12794281\n",
      "Iteration 6802, loss = 0.12789708\n",
      "Iteration 6803, loss = 0.12785138\n",
      "Iteration 6804, loss = 0.12780570\n",
      "Iteration 6805, loss = 0.12776006\n",
      "Iteration 6806, loss = 0.12771445\n",
      "Iteration 6807, loss = 0.12766886\n",
      "Iteration 6808, loss = 0.12762330\n",
      "Iteration 6809, loss = 0.12757778\n",
      "Iteration 6810, loss = 0.12753228\n",
      "Iteration 6811, loss = 0.12748681\n",
      "Iteration 6812, loss = 0.12744137\n",
      "Iteration 6813, loss = 0.12739596\n",
      "Iteration 6814, loss = 0.12735058\n",
      "Iteration 6815, loss = 0.12730522\n",
      "Iteration 6816, loss = 0.12725990\n",
      "Iteration 6817, loss = 0.12721460\n",
      "Iteration 6818, loss = 0.12716933\n",
      "Iteration 6819, loss = 0.12712409\n",
      "Iteration 6820, loss = 0.12707888\n",
      "Iteration 6821, loss = 0.12703370\n",
      "Iteration 6822, loss = 0.12698855\n",
      "Iteration 6823, loss = 0.12694343\n",
      "Iteration 6824, loss = 0.12689833\n",
      "Iteration 6825, loss = 0.12685327\n",
      "Iteration 6826, loss = 0.12680823\n",
      "Iteration 6827, loss = 0.12676322\n",
      "Iteration 6828, loss = 0.12671824\n",
      "Iteration 6829, loss = 0.12667328\n",
      "Iteration 6830, loss = 0.12662836\n",
      "Iteration 6831, loss = 0.12658347\n",
      "Iteration 6832, loss = 0.12653860\n",
      "Iteration 6833, loss = 0.12649376\n",
      "Iteration 6834, loss = 0.12644895\n",
      "Iteration 6835, loss = 0.12640417\n",
      "Iteration 6836, loss = 0.12635942\n",
      "Iteration 6837, loss = 0.12631469\n",
      "Iteration 6838, loss = 0.12627000\n",
      "Iteration 6839, loss = 0.12622533\n",
      "Iteration 6840, loss = 0.12618069\n",
      "Iteration 6841, loss = 0.12613608\n",
      "Iteration 6842, loss = 0.12609149\n",
      "Iteration 6843, loss = 0.12604694\n",
      "Iteration 6844, loss = 0.12600241\n",
      "Iteration 6845, loss = 0.12595791\n",
      "Iteration 6846, loss = 0.12591344\n",
      "Iteration 6847, loss = 0.12586900\n",
      "Iteration 6848, loss = 0.12582459\n",
      "Iteration 6849, loss = 0.12578020\n",
      "Iteration 6850, loss = 0.12573584\n",
      "Iteration 6851, loss = 0.12569151\n",
      "Iteration 6852, loss = 0.12564721\n",
      "Iteration 6853, loss = 0.12560294\n",
      "Iteration 6854, loss = 0.12555869\n",
      "Iteration 6855, loss = 0.12551447\n",
      "Iteration 6856, loss = 0.12547028\n",
      "Iteration 6857, loss = 0.12542612\n",
      "Iteration 6858, loss = 0.12538199\n",
      "Iteration 6859, loss = 0.12533788\n",
      "Iteration 6860, loss = 0.12529380\n",
      "Iteration 6861, loss = 0.12524975\n",
      "Iteration 6862, loss = 0.12520573\n",
      "Iteration 6863, loss = 0.12516173\n",
      "Iteration 6864, loss = 0.12511776\n",
      "Iteration 6865, loss = 0.12507382\n",
      "Iteration 6866, loss = 0.12502991\n",
      "Iteration 6867, loss = 0.12498603\n",
      "Iteration 6868, loss = 0.12494217\n",
      "Iteration 6869, loss = 0.12489834\n",
      "Iteration 6870, loss = 0.12485454\n",
      "Iteration 6871, loss = 0.12481077\n",
      "Iteration 6872, loss = 0.12476702\n",
      "Iteration 6873, loss = 0.12472330\n",
      "Iteration 6874, loss = 0.12467961\n",
      "Iteration 6875, loss = 0.12463595\n",
      "Iteration 6876, loss = 0.12459231\n",
      "Iteration 6877, loss = 0.12454870\n",
      "Iteration 6878, loss = 0.12450512\n",
      "Iteration 6879, loss = 0.12446157\n",
      "Iteration 6880, loss = 0.12441804\n",
      "Iteration 6881, loss = 0.12437454\n",
      "Iteration 6882, loss = 0.12433107\n",
      "Iteration 6883, loss = 0.12428763\n",
      "Iteration 6884, loss = 0.12424421\n",
      "Iteration 6885, loss = 0.12420082\n",
      "Iteration 6886, loss = 0.12415746\n",
      "Iteration 6887, loss = 0.12411413\n",
      "Iteration 6888, loss = 0.12407082\n",
      "Iteration 6889, loss = 0.12402754\n",
      "Iteration 6890, loss = 0.12398428\n",
      "Iteration 6891, loss = 0.12394106\n",
      "Iteration 6892, loss = 0.12389786\n",
      "Iteration 6893, loss = 0.12385469\n",
      "Iteration 6894, loss = 0.12381154\n",
      "Iteration 6895, loss = 0.12376843\n",
      "Iteration 6896, loss = 0.12372534\n",
      "Iteration 6897, loss = 0.12368227\n",
      "Iteration 6898, loss = 0.12363924\n",
      "Iteration 6899, loss = 0.12359623\n",
      "Iteration 6900, loss = 0.12355324\n",
      "Iteration 6901, loss = 0.12351029\n",
      "Iteration 6902, loss = 0.12346736\n",
      "Iteration 6903, loss = 0.12342446\n",
      "Iteration 6904, loss = 0.12338158\n",
      "Iteration 6905, loss = 0.12333874\n",
      "Iteration 6906, loss = 0.12329591\n",
      "Iteration 6907, loss = 0.12325312\n",
      "Iteration 6908, loss = 0.12321035\n",
      "Iteration 6909, loss = 0.12316761\n",
      "Iteration 6910, loss = 0.12312490\n",
      "Iteration 6911, loss = 0.12308221\n",
      "Iteration 6912, loss = 0.12303955\n",
      "Iteration 6913, loss = 0.12299692\n",
      "Iteration 6914, loss = 0.12295431\n",
      "Iteration 6915, loss = 0.12291173\n",
      "Iteration 6916, loss = 0.12286918\n",
      "Iteration 6917, loss = 0.12282665\n",
      "Iteration 6918, loss = 0.12278415\n",
      "Iteration 6919, loss = 0.12274168\n",
      "Iteration 6920, loss = 0.12269923\n",
      "Iteration 6921, loss = 0.12265681\n",
      "Iteration 6922, loss = 0.12261441\n",
      "Iteration 6923, loss = 0.12257205\n",
      "Iteration 6924, loss = 0.12252970\n",
      "Iteration 6925, loss = 0.12248739\n",
      "Iteration 6926, loss = 0.12244510\n",
      "Iteration 6927, loss = 0.12240284\n",
      "Iteration 6928, loss = 0.12236060\n",
      "Iteration 6929, loss = 0.12231840\n",
      "Iteration 6930, loss = 0.12227621\n",
      "Iteration 6931, loss = 0.12223406\n",
      "Iteration 6932, loss = 0.12219193\n",
      "Iteration 6933, loss = 0.12214982\n",
      "Iteration 6934, loss = 0.12210774\n",
      "Iteration 6935, loss = 0.12206569\n",
      "Iteration 6936, loss = 0.12202367\n",
      "Iteration 6937, loss = 0.12198167\n",
      "Iteration 6938, loss = 0.12193970\n",
      "Iteration 6939, loss = 0.12189775\n",
      "Iteration 6940, loss = 0.12185583\n",
      "Iteration 6941, loss = 0.12181394\n",
      "Iteration 6942, loss = 0.12177207\n",
      "Iteration 6943, loss = 0.12173023\n",
      "Iteration 6944, loss = 0.12168841\n",
      "Iteration 6945, loss = 0.12164662\n",
      "Iteration 6946, loss = 0.12160486\n",
      "Iteration 6947, loss = 0.12156312\n",
      "Iteration 6948, loss = 0.12152141\n",
      "Iteration 6949, loss = 0.12147972\n",
      "Iteration 6950, loss = 0.12143806\n",
      "Iteration 6951, loss = 0.12139643\n",
      "Iteration 6952, loss = 0.12135482\n",
      "Iteration 6953, loss = 0.12131323\n",
      "Iteration 6954, loss = 0.12127168\n",
      "Iteration 6955, loss = 0.12123015\n",
      "Iteration 6956, loss = 0.12118864\n",
      "Iteration 6957, loss = 0.12114716\n",
      "Iteration 6958, loss = 0.12110571\n",
      "Iteration 6959, loss = 0.12106428\n",
      "Iteration 6960, loss = 0.12102288\n",
      "Iteration 6961, loss = 0.12098151\n",
      "Iteration 6962, loss = 0.12094015\n",
      "Iteration 6963, loss = 0.12089883\n",
      "Iteration 6964, loss = 0.12085753\n",
      "Iteration 6965, loss = 0.12081626\n",
      "Iteration 6966, loss = 0.12077501\n",
      "Iteration 6967, loss = 0.12073379\n",
      "Iteration 6968, loss = 0.12069259\n",
      "Iteration 6969, loss = 0.12065142\n",
      "Iteration 6970, loss = 0.12061027\n",
      "Iteration 6971, loss = 0.12056915\n",
      "Iteration 6972, loss = 0.12052806\n",
      "Iteration 6973, loss = 0.12048699\n",
      "Iteration 6974, loss = 0.12044595\n",
      "Iteration 6975, loss = 0.12040493\n",
      "Iteration 6976, loss = 0.12036394\n",
      "Iteration 6977, loss = 0.12032297\n",
      "Iteration 6978, loss = 0.12028203\n",
      "Iteration 6979, loss = 0.12024111\n",
      "Iteration 6980, loss = 0.12020022\n",
      "Iteration 6981, loss = 0.12015935\n",
      "Iteration 6982, loss = 0.12011851\n",
      "Iteration 6983, loss = 0.12007770\n",
      "Iteration 6984, loss = 0.12003691\n",
      "Iteration 6985, loss = 0.11999614\n",
      "Iteration 6986, loss = 0.11995540\n",
      "Iteration 6987, loss = 0.11991469\n",
      "Iteration 6988, loss = 0.11987400\n",
      "Iteration 6989, loss = 0.11983334\n",
      "Iteration 6990, loss = 0.11979270\n",
      "Iteration 6991, loss = 0.11975208\n",
      "Iteration 6992, loss = 0.11971149\n",
      "Iteration 6993, loss = 0.11967093\n",
      "Iteration 6994, loss = 0.11963039\n",
      "Iteration 6995, loss = 0.11958988\n",
      "Iteration 6996, loss = 0.11954939\n",
      "Iteration 6997, loss = 0.11950893\n",
      "Iteration 6998, loss = 0.11946849\n",
      "Iteration 6999, loss = 0.11942808\n",
      "Iteration 7000, loss = 0.11938769\n",
      "Iteration 7001, loss = 0.11934732\n",
      "Iteration 7002, loss = 0.11930698\n",
      "Iteration 7003, loss = 0.11926667\n",
      "Iteration 7004, loss = 0.11922638\n",
      "Iteration 7005, loss = 0.11918612\n",
      "Iteration 7006, loss = 0.11914588\n",
      "Iteration 7007, loss = 0.11910566\n",
      "Iteration 7008, loss = 0.11906547\n",
      "Iteration 7009, loss = 0.11902531\n",
      "Iteration 7010, loss = 0.11898517\n",
      "Iteration 7011, loss = 0.11894505\n",
      "Iteration 7012, loss = 0.11890496\n",
      "Iteration 7013, loss = 0.11886490\n",
      "Iteration 7014, loss = 0.11882485\n",
      "Iteration 7015, loss = 0.11878484\n",
      "Iteration 7016, loss = 0.11874485\n",
      "Iteration 7017, loss = 0.11870488\n",
      "Iteration 7018, loss = 0.11866493\n",
      "Iteration 7019, loss = 0.11862502\n",
      "Iteration 7020, loss = 0.11858512\n",
      "Iteration 7021, loss = 0.11854525\n",
      "Iteration 7022, loss = 0.11850541\n",
      "Iteration 7023, loss = 0.11846559\n",
      "Iteration 7024, loss = 0.11842579\n",
      "Iteration 7025, loss = 0.11838602\n",
      "Iteration 7026, loss = 0.11834627\n",
      "Iteration 7027, loss = 0.11830655\n",
      "Iteration 7028, loss = 0.11826685\n",
      "Iteration 7029, loss = 0.11822718\n",
      "Iteration 7030, loss = 0.11818753\n",
      "Iteration 7031, loss = 0.11814791\n",
      "Iteration 7032, loss = 0.11810830\n",
      "Iteration 7033, loss = 0.11806873\n",
      "Iteration 7034, loss = 0.11802918\n",
      "Iteration 7035, loss = 0.11798965\n",
      "Iteration 7036, loss = 0.11795015\n",
      "Iteration 7037, loss = 0.11791067\n",
      "Iteration 7038, loss = 0.11787121\n",
      "Iteration 7039, loss = 0.11783178\n",
      "Iteration 7040, loss = 0.11779237\n",
      "Iteration 7041, loss = 0.11775299\n",
      "Iteration 7042, loss = 0.11771363\n",
      "Iteration 7043, loss = 0.11767430\n",
      "Iteration 7044, loss = 0.11763499\n",
      "Iteration 7045, loss = 0.11759570\n",
      "Iteration 7046, loss = 0.11755644\n",
      "Iteration 7047, loss = 0.11751720\n",
      "Iteration 7048, loss = 0.11747799\n",
      "Iteration 7049, loss = 0.11743880\n",
      "Iteration 7050, loss = 0.11739963\n",
      "Iteration 7051, loss = 0.11736049\n",
      "Iteration 7052, loss = 0.11732137\n",
      "Iteration 7053, loss = 0.11728228\n",
      "Iteration 7054, loss = 0.11724321\n",
      "Iteration 7055, loss = 0.11720416\n",
      "Iteration 7056, loss = 0.11716514\n",
      "Iteration 7057, loss = 0.11712614\n",
      "Iteration 7058, loss = 0.11708716\n",
      "Iteration 7059, loss = 0.11704821\n",
      "Iteration 7060, loss = 0.11700929\n",
      "Iteration 7061, loss = 0.11697038\n",
      "Iteration 7062, loss = 0.11693150\n",
      "Iteration 7063, loss = 0.11689265\n",
      "Iteration 7064, loss = 0.11685382\n",
      "Iteration 7065, loss = 0.11681501\n",
      "Iteration 7066, loss = 0.11677622\n",
      "Iteration 7067, loss = 0.11673746\n",
      "Iteration 7068, loss = 0.11669872\n",
      "Iteration 7069, loss = 0.11666001\n",
      "Iteration 7070, loss = 0.11662132\n",
      "Iteration 7071, loss = 0.11658265\n",
      "Iteration 7072, loss = 0.11654401\n",
      "Iteration 7073, loss = 0.11650539\n",
      "Iteration 7074, loss = 0.11646679\n",
      "Iteration 7075, loss = 0.11642822\n",
      "Iteration 7076, loss = 0.11638967\n",
      "Iteration 7077, loss = 0.11635115\n",
      "Iteration 7078, loss = 0.11631265\n",
      "Iteration 7079, loss = 0.11627417\n",
      "Iteration 7080, loss = 0.11623571\n",
      "Iteration 7081, loss = 0.11619728\n",
      "Iteration 7082, loss = 0.11615887\n",
      "Iteration 7083, loss = 0.11612049\n",
      "Iteration 7084, loss = 0.11608213\n",
      "Iteration 7085, loss = 0.11604379\n",
      "Iteration 7086, loss = 0.11600547\n",
      "Iteration 7087, loss = 0.11596718\n",
      "Iteration 7088, loss = 0.11592891\n",
      "Iteration 7089, loss = 0.11589067\n",
      "Iteration 7090, loss = 0.11585245\n",
      "Iteration 7091, loss = 0.11581425\n",
      "Iteration 7092, loss = 0.11577607\n",
      "Iteration 7093, loss = 0.11573792\n",
      "Iteration 7094, loss = 0.11569979\n",
      "Iteration 7095, loss = 0.11566169\n",
      "Iteration 7096, loss = 0.11562360\n",
      "Iteration 7097, loss = 0.11558554\n",
      "Iteration 7098, loss = 0.11554751\n",
      "Iteration 7099, loss = 0.11550949\n",
      "Iteration 7100, loss = 0.11547150\n",
      "Iteration 7101, loss = 0.11543354\n",
      "Iteration 7102, loss = 0.11539559\n",
      "Iteration 7103, loss = 0.11535767\n",
      "Iteration 7104, loss = 0.11531977\n",
      "Iteration 7105, loss = 0.11528190\n",
      "Iteration 7106, loss = 0.11524405\n",
      "Iteration 7107, loss = 0.11520622\n",
      "Iteration 7108, loss = 0.11516841\n",
      "Iteration 7109, loss = 0.11513063\n",
      "Iteration 7110, loss = 0.11509287\n",
      "Iteration 7111, loss = 0.11505513\n",
      "Iteration 7112, loss = 0.11501741\n",
      "Iteration 7113, loss = 0.11497972\n",
      "Iteration 7114, loss = 0.11494205\n",
      "Iteration 7115, loss = 0.11490441\n",
      "Iteration 7116, loss = 0.11486678\n",
      "Iteration 7117, loss = 0.11482918\n",
      "Iteration 7118, loss = 0.11479161\n",
      "Iteration 7119, loss = 0.11475405\n",
      "Iteration 7120, loss = 0.11471652\n",
      "Iteration 7121, loss = 0.11467901\n",
      "Iteration 7122, loss = 0.11464152\n",
      "Iteration 7123, loss = 0.11460406\n",
      "Iteration 7124, loss = 0.11456661\n",
      "Iteration 7125, loss = 0.11452920\n",
      "Iteration 7126, loss = 0.11449180\n",
      "Iteration 7127, loss = 0.11445442\n",
      "Iteration 7128, loss = 0.11441707\n",
      "Iteration 7129, loss = 0.11437974\n",
      "Iteration 7130, loss = 0.11434244\n",
      "Iteration 7131, loss = 0.11430515\n",
      "Iteration 7132, loss = 0.11426789\n",
      "Iteration 7133, loss = 0.11423065\n",
      "Iteration 7134, loss = 0.11419344\n",
      "Iteration 7135, loss = 0.11415624\n",
      "Iteration 7136, loss = 0.11411907\n",
      "Iteration 7137, loss = 0.11408192\n",
      "Iteration 7138, loss = 0.11404480\n",
      "Iteration 7139, loss = 0.11400769\n",
      "Iteration 7140, loss = 0.11397061\n",
      "Iteration 7141, loss = 0.11393355\n",
      "Iteration 7142, loss = 0.11389651\n",
      "Iteration 7143, loss = 0.11385950\n",
      "Iteration 7144, loss = 0.11382251\n",
      "Iteration 7145, loss = 0.11378554\n",
      "Iteration 7146, loss = 0.11374859\n",
      "Iteration 7147, loss = 0.11371166\n",
      "Iteration 7148, loss = 0.11367476\n",
      "Iteration 7149, loss = 0.11363788\n",
      "Iteration 7150, loss = 0.11360102\n",
      "Iteration 7151, loss = 0.11356418\n",
      "Iteration 7152, loss = 0.11352737\n",
      "Iteration 7153, loss = 0.11349057\n",
      "Iteration 7154, loss = 0.11345380\n",
      "Iteration 7155, loss = 0.11341705\n",
      "Iteration 7156, loss = 0.11338033\n",
      "Iteration 7157, loss = 0.11334362\n",
      "Iteration 7158, loss = 0.11330694\n",
      "Iteration 7159, loss = 0.11327028\n",
      "Iteration 7160, loss = 0.11323364\n",
      "Iteration 7161, loss = 0.11319703\n",
      "Iteration 7162, loss = 0.11316043\n",
      "Iteration 7163, loss = 0.11312386\n",
      "Iteration 7164, loss = 0.11308731\n",
      "Iteration 7165, loss = 0.11305078\n",
      "Iteration 7166, loss = 0.11301428\n",
      "Iteration 7167, loss = 0.11297779\n",
      "Iteration 7168, loss = 0.11294133\n",
      "Iteration 7169, loss = 0.11290489\n",
      "Iteration 7170, loss = 0.11286847\n",
      "Iteration 7171, loss = 0.11283207\n",
      "Iteration 7172, loss = 0.11279570\n",
      "Iteration 7173, loss = 0.11275934\n",
      "Iteration 7174, loss = 0.11272301\n",
      "Iteration 7175, loss = 0.11268670\n",
      "Iteration 7176, loss = 0.11265041\n",
      "Iteration 7177, loss = 0.11261415\n",
      "Iteration 7178, loss = 0.11257790\n",
      "Iteration 7179, loss = 0.11254168\n",
      "Iteration 7180, loss = 0.11250548\n",
      "Iteration 7181, loss = 0.11246930\n",
      "Iteration 7182, loss = 0.11243314\n",
      "Iteration 7183, loss = 0.11239700\n",
      "Iteration 7184, loss = 0.11236089\n",
      "Iteration 7185, loss = 0.11232480\n",
      "Iteration 7186, loss = 0.11228872\n",
      "Iteration 7187, loss = 0.11225267\n",
      "Iteration 7188, loss = 0.11221665\n",
      "Iteration 7189, loss = 0.11218064\n",
      "Iteration 7190, loss = 0.11214465\n",
      "Iteration 7191, loss = 0.11210869\n",
      "Iteration 7192, loss = 0.11207275\n",
      "Iteration 7193, loss = 0.11203683\n",
      "Iteration 7194, loss = 0.11200093\n",
      "Iteration 7195, loss = 0.11196505\n",
      "Iteration 7196, loss = 0.11192919\n",
      "Iteration 7197, loss = 0.11189336\n",
      "Iteration 7198, loss = 0.11185754\n",
      "Iteration 7199, loss = 0.11182175\n",
      "Iteration 7200, loss = 0.11178598\n",
      "Iteration 7201, loss = 0.11175023\n",
      "Iteration 7202, loss = 0.11171450\n",
      "Iteration 7203, loss = 0.11167880\n",
      "Iteration 7204, loss = 0.11164311\n",
      "Iteration 7205, loss = 0.11160744\n",
      "Iteration 7206, loss = 0.11157180\n",
      "Iteration 7207, loss = 0.11153618\n",
      "Iteration 7208, loss = 0.11150058\n",
      "Iteration 7209, loss = 0.11146500\n",
      "Iteration 7210, loss = 0.11142944\n",
      "Iteration 7211, loss = 0.11139390\n",
      "Iteration 7212, loss = 0.11135839\n",
      "Iteration 7213, loss = 0.11132289\n",
      "Iteration 7214, loss = 0.11128742\n",
      "Iteration 7215, loss = 0.11125197\n",
      "Iteration 7216, loss = 0.11121653\n",
      "Iteration 7217, loss = 0.11118112\n",
      "Iteration 7218, loss = 0.11114573\n",
      "Iteration 7219, loss = 0.11111037\n",
      "Iteration 7220, loss = 0.11107502\n",
      "Iteration 7221, loss = 0.11103969\n",
      "Iteration 7222, loss = 0.11100439\n",
      "Iteration 7223, loss = 0.11096910\n",
      "Iteration 7224, loss = 0.11093384\n",
      "Iteration 7225, loss = 0.11089860\n",
      "Iteration 7226, loss = 0.11086337\n",
      "Iteration 7227, loss = 0.11082817\n",
      "Iteration 7228, loss = 0.11079299\n",
      "Iteration 7229, loss = 0.11075783\n",
      "Iteration 7230, loss = 0.11072270\n",
      "Iteration 7231, loss = 0.11068758\n",
      "Iteration 7232, loss = 0.11065248\n",
      "Iteration 7233, loss = 0.11061741\n",
      "Iteration 7234, loss = 0.11058235\n",
      "Iteration 7235, loss = 0.11054732\n",
      "Iteration 7236, loss = 0.11051231\n",
      "Iteration 7237, loss = 0.11047731\n",
      "Iteration 7238, loss = 0.11044234\n",
      "Iteration 7239, loss = 0.11040739\n",
      "Iteration 7240, loss = 0.11037246\n",
      "Iteration 7241, loss = 0.11033755\n",
      "Iteration 7242, loss = 0.11030266\n",
      "Iteration 7243, loss = 0.11026779\n",
      "Iteration 7244, loss = 0.11023295\n",
      "Iteration 7245, loss = 0.11019812\n",
      "Iteration 7246, loss = 0.11016331\n",
      "Iteration 7247, loss = 0.11012853\n",
      "Iteration 7248, loss = 0.11009376\n",
      "Iteration 7249, loss = 0.11005902\n",
      "Iteration 7250, loss = 0.11002429\n",
      "Iteration 7251, loss = 0.10998959\n",
      "Iteration 7252, loss = 0.10995491\n",
      "Iteration 7253, loss = 0.10992025\n",
      "Iteration 7254, loss = 0.10988560\n",
      "Iteration 7255, loss = 0.10985098\n",
      "Iteration 7256, loss = 0.10981638\n",
      "Iteration 7257, loss = 0.10978180\n",
      "Iteration 7258, loss = 0.10974724\n",
      "Iteration 7259, loss = 0.10971270\n",
      "Iteration 7260, loss = 0.10967818\n",
      "Iteration 7261, loss = 0.10964368\n",
      "Iteration 7262, loss = 0.10960920\n",
      "Iteration 7263, loss = 0.10957474\n",
      "Iteration 7264, loss = 0.10954031\n",
      "Iteration 7265, loss = 0.10950589\n",
      "Iteration 7266, loss = 0.10947149\n",
      "Iteration 7267, loss = 0.10943711\n",
      "Iteration 7268, loss = 0.10940276\n",
      "Iteration 7269, loss = 0.10936842\n",
      "Iteration 7270, loss = 0.10933411\n",
      "Iteration 7271, loss = 0.10929981\n",
      "Iteration 7272, loss = 0.10926553\n",
      "Iteration 7273, loss = 0.10923128\n",
      "Iteration 7274, loss = 0.10919704\n",
      "Iteration 7275, loss = 0.10916283\n",
      "Iteration 7276, loss = 0.10912863\n",
      "Iteration 7277, loss = 0.10909446\n",
      "Iteration 7278, loss = 0.10906030\n",
      "Iteration 7279, loss = 0.10902617\n",
      "Iteration 7280, loss = 0.10899205\n",
      "Iteration 7281, loss = 0.10895796\n",
      "Iteration 7282, loss = 0.10892388\n",
      "Iteration 7283, loss = 0.10888983\n",
      "Iteration 7284, loss = 0.10885579\n",
      "Iteration 7285, loss = 0.10882178\n",
      "Iteration 7286, loss = 0.10878778\n",
      "Iteration 7287, loss = 0.10875381\n",
      "Iteration 7288, loss = 0.10871985\n",
      "Iteration 7289, loss = 0.10868592\n",
      "Iteration 7290, loss = 0.10865201\n",
      "Iteration 7291, loss = 0.10861811\n",
      "Iteration 7292, loss = 0.10858424\n",
      "Iteration 7293, loss = 0.10855038\n",
      "Iteration 7294, loss = 0.10851655\n",
      "Iteration 7295, loss = 0.10848273\n",
      "Iteration 7296, loss = 0.10844893\n",
      "Iteration 7297, loss = 0.10841516\n",
      "Iteration 7298, loss = 0.10838140\n",
      "Iteration 7299, loss = 0.10834767\n",
      "Iteration 7300, loss = 0.10831395\n",
      "Iteration 7301, loss = 0.10828025\n",
      "Iteration 7302, loss = 0.10824658\n",
      "Iteration 7303, loss = 0.10821292\n",
      "Iteration 7304, loss = 0.10817928\n",
      "Iteration 7305, loss = 0.10814567\n",
      "Iteration 7306, loss = 0.10811207\n",
      "Iteration 7307, loss = 0.10807849\n",
      "Iteration 7308, loss = 0.10804493\n",
      "Iteration 7309, loss = 0.10801139\n",
      "Iteration 7310, loss = 0.10797787\n",
      "Iteration 7311, loss = 0.10794437\n",
      "Iteration 7312, loss = 0.10791089\n",
      "Iteration 7313, loss = 0.10787743\n",
      "Iteration 7314, loss = 0.10784399\n",
      "Iteration 7315, loss = 0.10781057\n",
      "Iteration 7316, loss = 0.10777717\n",
      "Iteration 7317, loss = 0.10774379\n",
      "Iteration 7318, loss = 0.10771042\n",
      "Iteration 7319, loss = 0.10767708\n",
      "Iteration 7320, loss = 0.10764376\n",
      "Iteration 7321, loss = 0.10761045\n",
      "Iteration 7322, loss = 0.10757717\n",
      "Iteration 7323, loss = 0.10754390\n",
      "Iteration 7324, loss = 0.10751065\n",
      "Iteration 7325, loss = 0.10747743\n",
      "Iteration 7326, loss = 0.10744422\n",
      "Iteration 7327, loss = 0.10741103\n",
      "Iteration 7328, loss = 0.10737786\n",
      "Iteration 7329, loss = 0.10734472\n",
      "Iteration 7330, loss = 0.10731159\n",
      "Iteration 7331, loss = 0.10727847\n",
      "Iteration 7332, loss = 0.10724538\n",
      "Iteration 7333, loss = 0.10721231\n",
      "Iteration 7334, loss = 0.10717926\n",
      "Iteration 7335, loss = 0.10714623\n",
      "Iteration 7336, loss = 0.10711321\n",
      "Iteration 7337, loss = 0.10708022\n",
      "Iteration 7338, loss = 0.10704724\n",
      "Iteration 7339, loss = 0.10701428\n",
      "Iteration 7340, loss = 0.10698135\n",
      "Iteration 7341, loss = 0.10694843\n",
      "Iteration 7342, loss = 0.10691553\n",
      "Iteration 7343, loss = 0.10688265\n",
      "Iteration 7344, loss = 0.10684979\n",
      "Iteration 7345, loss = 0.10681695\n",
      "Iteration 7346, loss = 0.10678413\n",
      "Iteration 7347, loss = 0.10675132\n",
      "Iteration 7348, loss = 0.10671854\n",
      "Iteration 7349, loss = 0.10668577\n",
      "Iteration 7350, loss = 0.10665303\n",
      "Iteration 7351, loss = 0.10662030\n",
      "Iteration 7352, loss = 0.10658759\n",
      "Iteration 7353, loss = 0.10655490\n",
      "Iteration 7354, loss = 0.10652223\n",
      "Iteration 7355, loss = 0.10648958\n",
      "Iteration 7356, loss = 0.10645695\n",
      "Iteration 7357, loss = 0.10642434\n",
      "Iteration 7358, loss = 0.10639174\n",
      "Iteration 7359, loss = 0.10635917\n",
      "Iteration 7360, loss = 0.10632661\n",
      "Iteration 7361, loss = 0.10629407\n",
      "Iteration 7362, loss = 0.10626155\n",
      "Iteration 7363, loss = 0.10622905\n",
      "Iteration 7364, loss = 0.10619657\n",
      "Iteration 7365, loss = 0.10616411\n",
      "Iteration 7366, loss = 0.10613167\n",
      "Iteration 7367, loss = 0.10609924\n",
      "Iteration 7368, loss = 0.10606683\n",
      "Iteration 7369, loss = 0.10603445\n",
      "Iteration 7370, loss = 0.10600208\n",
      "Iteration 7371, loss = 0.10596973\n",
      "Iteration 7372, loss = 0.10593740\n",
      "Iteration 7373, loss = 0.10590509\n",
      "Iteration 7374, loss = 0.10587279\n",
      "Iteration 7375, loss = 0.10584052\n",
      "Iteration 7376, loss = 0.10580826\n",
      "Iteration 7377, loss = 0.10577603\n",
      "Iteration 7378, loss = 0.10574381\n",
      "Iteration 7379, loss = 0.10571161\n",
      "Iteration 7380, loss = 0.10567942\n",
      "Iteration 7381, loss = 0.10564726\n",
      "Iteration 7382, loss = 0.10561512\n",
      "Iteration 7383, loss = 0.10558299\n",
      "Iteration 7384, loss = 0.10555088\n",
      "Iteration 7385, loss = 0.10551880\n",
      "Iteration 7386, loss = 0.10548673\n",
      "Iteration 7387, loss = 0.10545467\n",
      "Iteration 7388, loss = 0.10542264\n",
      "Iteration 7389, loss = 0.10539063\n",
      "Iteration 7390, loss = 0.10535863\n",
      "Iteration 7391, loss = 0.10532665\n",
      "Iteration 7392, loss = 0.10529469\n",
      "Iteration 7393, loss = 0.10526275\n",
      "Iteration 7394, loss = 0.10523083\n",
      "Iteration 7395, loss = 0.10519893\n",
      "Iteration 7396, loss = 0.10516704\n",
      "Iteration 7397, loss = 0.10513517\n",
      "Iteration 7398, loss = 0.10510332\n",
      "Iteration 7399, loss = 0.10507149\n",
      "Iteration 7400, loss = 0.10503968\n",
      "Iteration 7401, loss = 0.10500789\n",
      "Iteration 7402, loss = 0.10497611\n",
      "Iteration 7403, loss = 0.10494435\n",
      "Iteration 7404, loss = 0.10491262\n",
      "Iteration 7405, loss = 0.10488090\n",
      "Iteration 7406, loss = 0.10484919\n",
      "Iteration 7407, loss = 0.10481751\n",
      "Iteration 7408, loss = 0.10478584\n",
      "Iteration 7409, loss = 0.10475420\n",
      "Iteration 7410, loss = 0.10472257\n",
      "Iteration 7411, loss = 0.10469095\n",
      "Iteration 7412, loss = 0.10465936\n",
      "Iteration 7413, loss = 0.10462779\n",
      "Iteration 7414, loss = 0.10459623\n",
      "Iteration 7415, loss = 0.10456469\n",
      "Iteration 7416, loss = 0.10453317\n",
      "Iteration 7417, loss = 0.10450167\n",
      "Iteration 7418, loss = 0.10447018\n",
      "Iteration 7419, loss = 0.10443872\n",
      "Iteration 7420, loss = 0.10440727\n",
      "Iteration 7421, loss = 0.10437584\n",
      "Iteration 7422, loss = 0.10434443\n",
      "Iteration 7423, loss = 0.10431303\n",
      "Iteration 7424, loss = 0.10428166\n",
      "Iteration 7425, loss = 0.10425030\n",
      "Iteration 7426, loss = 0.10421896\n",
      "Iteration 7427, loss = 0.10418763\n",
      "Iteration 7428, loss = 0.10415633\n",
      "Iteration 7429, loss = 0.10412504\n",
      "Iteration 7430, loss = 0.10409378\n",
      "Iteration 7431, loss = 0.10406253\n",
      "Iteration 7432, loss = 0.10403129\n",
      "Iteration 7433, loss = 0.10400008\n",
      "Iteration 7434, loss = 0.10396888\n",
      "Iteration 7435, loss = 0.10393770\n",
      "Iteration 7436, loss = 0.10390654\n",
      "Iteration 7437, loss = 0.10387540\n",
      "Iteration 7438, loss = 0.10384427\n",
      "Iteration 7439, loss = 0.10381317\n",
      "Iteration 7440, loss = 0.10378208\n",
      "Iteration 7441, loss = 0.10375100\n",
      "Iteration 7442, loss = 0.10371995\n",
      "Iteration 7443, loss = 0.10368891\n",
      "Iteration 7444, loss = 0.10365789\n",
      "Iteration 7445, loss = 0.10362689\n",
      "Iteration 7446, loss = 0.10359591\n",
      "Iteration 7447, loss = 0.10356494\n",
      "Iteration 7448, loss = 0.10353400\n",
      "Iteration 7449, loss = 0.10350307\n",
      "Iteration 7450, loss = 0.10347215\n",
      "Iteration 7451, loss = 0.10344126\n",
      "Iteration 7452, loss = 0.10341038\n",
      "Iteration 7453, loss = 0.10337952\n",
      "Iteration 7454, loss = 0.10334868\n",
      "Iteration 7455, loss = 0.10331785\n",
      "Iteration 7456, loss = 0.10328705\n",
      "Iteration 7457, loss = 0.10325626\n",
      "Iteration 7458, loss = 0.10322549\n",
      "Iteration 7459, loss = 0.10319473\n",
      "Iteration 7460, loss = 0.10316400\n",
      "Iteration 7461, loss = 0.10313328\n",
      "Iteration 7462, loss = 0.10310257\n",
      "Iteration 7463, loss = 0.10307189\n",
      "Iteration 7464, loss = 0.10304122\n",
      "Iteration 7465, loss = 0.10301057\n",
      "Iteration 7466, loss = 0.10297994\n",
      "Iteration 7467, loss = 0.10294933\n",
      "Iteration 7468, loss = 0.10291873\n",
      "Iteration 7469, loss = 0.10288815\n",
      "Iteration 7470, loss = 0.10285759\n",
      "Iteration 7471, loss = 0.10282704\n",
      "Iteration 7472, loss = 0.10279652\n",
      "Iteration 7473, loss = 0.10276601\n",
      "Iteration 7474, loss = 0.10273551\n",
      "Iteration 7475, loss = 0.10270504\n",
      "Iteration 7476, loss = 0.10267458\n",
      "Iteration 7477, loss = 0.10264414\n",
      "Iteration 7478, loss = 0.10261372\n",
      "Iteration 7479, loss = 0.10258331\n",
      "Iteration 7480, loss = 0.10255292\n",
      "Iteration 7481, loss = 0.10252255\n",
      "Iteration 7482, loss = 0.10249220\n",
      "Iteration 7483, loss = 0.10246186\n",
      "Iteration 7484, loss = 0.10243154\n",
      "Iteration 7485, loss = 0.10240124\n",
      "Iteration 7486, loss = 0.10237095\n",
      "Iteration 7487, loss = 0.10234068\n",
      "Iteration 7488, loss = 0.10231043\n",
      "Iteration 7489, loss = 0.10228020\n",
      "Iteration 7490, loss = 0.10224998\n",
      "Iteration 7491, loss = 0.10221978\n",
      "Iteration 7492, loss = 0.10218960\n",
      "Iteration 7493, loss = 0.10215943\n",
      "Iteration 7494, loss = 0.10212929\n",
      "Iteration 7495, loss = 0.10209915\n",
      "Iteration 7496, loss = 0.10206904\n",
      "Iteration 7497, loss = 0.10203894\n",
      "Iteration 7498, loss = 0.10200886\n",
      "Iteration 7499, loss = 0.10197880\n",
      "Iteration 7500, loss = 0.10194875\n",
      "Iteration 7501, loss = 0.10191872\n",
      "Iteration 7502, loss = 0.10188871\n",
      "Iteration 7503, loss = 0.10185872\n",
      "Iteration 7504, loss = 0.10182874\n",
      "Iteration 7505, loss = 0.10179878\n",
      "Iteration 7506, loss = 0.10176883\n",
      "Iteration 7507, loss = 0.10173891\n",
      "Iteration 7508, loss = 0.10170900\n",
      "Iteration 7509, loss = 0.10167910\n",
      "Iteration 7510, loss = 0.10164923\n",
      "Iteration 7511, loss = 0.10161937\n",
      "Iteration 7512, loss = 0.10158952\n",
      "Iteration 7513, loss = 0.10155970\n",
      "Iteration 7514, loss = 0.10152989\n",
      "Iteration 7515, loss = 0.10150010\n",
      "Iteration 7516, loss = 0.10147032\n",
      "Iteration 7517, loss = 0.10144056\n",
      "Iteration 7518, loss = 0.10141082\n",
      "Iteration 7519, loss = 0.10138109\n",
      "Iteration 7520, loss = 0.10135139\n",
      "Iteration 7521, loss = 0.10132169\n",
      "Iteration 7522, loss = 0.10129202\n",
      "Iteration 7523, loss = 0.10126236\n",
      "Iteration 7524, loss = 0.10123272\n",
      "Iteration 7525, loss = 0.10120309\n",
      "Iteration 7526, loss = 0.10117349\n",
      "Iteration 7527, loss = 0.10114389\n",
      "Iteration 7528, loss = 0.10111432\n",
      "Iteration 7529, loss = 0.10108476\n",
      "Iteration 7530, loss = 0.10105522\n",
      "Iteration 7531, loss = 0.10102570\n",
      "Iteration 7532, loss = 0.10099619\n",
      "Iteration 7533, loss = 0.10096670\n",
      "Iteration 7534, loss = 0.10093722\n",
      "Iteration 7535, loss = 0.10090776\n",
      "Iteration 7536, loss = 0.10087832\n",
      "Iteration 7537, loss = 0.10084890\n",
      "Iteration 7538, loss = 0.10081949\n",
      "Iteration 7539, loss = 0.10079009\n",
      "Iteration 7540, loss = 0.10076072\n",
      "Iteration 7541, loss = 0.10073136\n",
      "Iteration 7542, loss = 0.10070202\n",
      "Iteration 7543, loss = 0.10067269\n",
      "Iteration 7544, loss = 0.10064338\n",
      "Iteration 7545, loss = 0.10061409\n",
      "Iteration 7546, loss = 0.10058481\n",
      "Iteration 7547, loss = 0.10055555\n",
      "Iteration 7548, loss = 0.10052631\n",
      "Iteration 7549, loss = 0.10049708\n",
      "Iteration 7550, loss = 0.10046787\n",
      "Iteration 7551, loss = 0.10043867\n",
      "Iteration 7552, loss = 0.10040949\n",
      "Iteration 7553, loss = 0.10038033\n",
      "Iteration 7554, loss = 0.10035119\n",
      "Iteration 7555, loss = 0.10032206\n",
      "Iteration 7556, loss = 0.10029294\n",
      "Iteration 7557, loss = 0.10026385\n",
      "Iteration 7558, loss = 0.10023477\n",
      "Iteration 7559, loss = 0.10020570\n",
      "Iteration 7560, loss = 0.10017665\n",
      "Iteration 7561, loss = 0.10014762\n",
      "Iteration 7562, loss = 0.10011861\n",
      "Iteration 7563, loss = 0.10008961\n",
      "Iteration 7564, loss = 0.10006063\n",
      "Iteration 7565, loss = 0.10003166\n",
      "Iteration 7566, loss = 0.10000271\n",
      "Iteration 7567, loss = 0.09997377\n",
      "Iteration 7568, loss = 0.09994486\n",
      "Iteration 7569, loss = 0.09991595\n",
      "Iteration 7570, loss = 0.09988707\n",
      "Iteration 7571, loss = 0.09985820\n",
      "Iteration 7572, loss = 0.09982934\n",
      "Iteration 7573, loss = 0.09980051\n",
      "Iteration 7574, loss = 0.09977169\n",
      "Iteration 7575, loss = 0.09974288\n",
      "Iteration 7576, loss = 0.09971409\n",
      "Iteration 7577, loss = 0.09968532\n",
      "Iteration 7578, loss = 0.09965656\n",
      "Iteration 7579, loss = 0.09962782\n",
      "Iteration 7580, loss = 0.09959910\n",
      "Iteration 7581, loss = 0.09957039\n",
      "Iteration 7582, loss = 0.09954169\n",
      "Iteration 7583, loss = 0.09951302\n",
      "Iteration 7584, loss = 0.09948436\n",
      "Iteration 7585, loss = 0.09945571\n",
      "Iteration 7586, loss = 0.09942708\n",
      "Iteration 7587, loss = 0.09939847\n",
      "Iteration 7588, loss = 0.09936987\n",
      "Iteration 7589, loss = 0.09934129\n",
      "Iteration 7590, loss = 0.09931273\n",
      "Iteration 7591, loss = 0.09928418\n",
      "Iteration 7592, loss = 0.09925564\n",
      "Iteration 7593, loss = 0.09922713\n",
      "Iteration 7594, loss = 0.09919863\n",
      "Iteration 7595, loss = 0.09917014\n",
      "Iteration 7596, loss = 0.09914167\n",
      "Iteration 7597, loss = 0.09911322\n",
      "Iteration 7598, loss = 0.09908478\n",
      "Iteration 7599, loss = 0.09905635\n",
      "Iteration 7600, loss = 0.09902795\n",
      "Iteration 7601, loss = 0.09899956\n",
      "Iteration 7602, loss = 0.09897118\n",
      "Iteration 7603, loss = 0.09894282\n",
      "Iteration 7604, loss = 0.09891448\n",
      "Iteration 7605, loss = 0.09888615\n",
      "Iteration 7606, loss = 0.09885784\n",
      "Iteration 7607, loss = 0.09882954\n",
      "Iteration 7608, loss = 0.09880126\n",
      "Iteration 7609, loss = 0.09877300\n",
      "Iteration 7610, loss = 0.09874475\n",
      "Iteration 7611, loss = 0.09871652\n",
      "Iteration 7612, loss = 0.09868830\n",
      "Iteration 7613, loss = 0.09866010\n",
      "Iteration 7614, loss = 0.09863191\n",
      "Iteration 7615, loss = 0.09860374\n",
      "Iteration 7616, loss = 0.09857558\n",
      "Iteration 7617, loss = 0.09854744\n",
      "Iteration 7618, loss = 0.09851932\n",
      "Iteration 7619, loss = 0.09849121\n",
      "Iteration 7620, loss = 0.09846312\n",
      "Iteration 7621, loss = 0.09843504\n",
      "Iteration 7622, loss = 0.09840698\n",
      "Iteration 7623, loss = 0.09837893\n",
      "Iteration 7624, loss = 0.09835090\n",
      "Iteration 7625, loss = 0.09832289\n",
      "Iteration 7626, loss = 0.09829489\n",
      "Iteration 7627, loss = 0.09826690\n",
      "Iteration 7628, loss = 0.09823893\n",
      "Iteration 7629, loss = 0.09821098\n",
      "Iteration 7630, loss = 0.09818304\n",
      "Iteration 7631, loss = 0.09815512\n",
      "Iteration 7632, loss = 0.09812721\n",
      "Iteration 7633, loss = 0.09809932\n",
      "Iteration 7634, loss = 0.09807145\n",
      "Iteration 7635, loss = 0.09804358\n",
      "Iteration 7636, loss = 0.09801574\n",
      "Iteration 7637, loss = 0.09798791\n",
      "Iteration 7638, loss = 0.09796010\n",
      "Iteration 7639, loss = 0.09793230\n",
      "Iteration 7640, loss = 0.09790451\n",
      "Iteration 7641, loss = 0.09787674\n",
      "Iteration 7642, loss = 0.09784899\n",
      "Iteration 7643, loss = 0.09782125\n",
      "Iteration 7644, loss = 0.09779353\n",
      "Iteration 7645, loss = 0.09776582\n",
      "Iteration 7646, loss = 0.09773813\n",
      "Iteration 7647, loss = 0.09771045\n",
      "Iteration 7648, loss = 0.09768279\n",
      "Iteration 7649, loss = 0.09765515\n",
      "Iteration 7650, loss = 0.09762752\n",
      "Iteration 7651, loss = 0.09759990\n",
      "Iteration 7652, loss = 0.09757230\n",
      "Iteration 7653, loss = 0.09754471\n",
      "Iteration 7654, loss = 0.09751714\n",
      "Iteration 7655, loss = 0.09748959\n",
      "Iteration 7656, loss = 0.09746205\n",
      "Iteration 7657, loss = 0.09743452\n",
      "Iteration 7658, loss = 0.09740701\n",
      "Iteration 7659, loss = 0.09737952\n",
      "Iteration 7660, loss = 0.09735204\n",
      "Iteration 7661, loss = 0.09732458\n",
      "Iteration 7662, loss = 0.09729713\n",
      "Iteration 7663, loss = 0.09726969\n",
      "Iteration 7664, loss = 0.09724228\n",
      "Iteration 7665, loss = 0.09721487\n",
      "Iteration 7666, loss = 0.09718748\n",
      "Iteration 7667, loss = 0.09716011\n",
      "Iteration 7668, loss = 0.09713275\n",
      "Iteration 7669, loss = 0.09710541\n",
      "Iteration 7670, loss = 0.09707808\n",
      "Iteration 7671, loss = 0.09705076\n",
      "Iteration 7672, loss = 0.09702347\n",
      "Iteration 7673, loss = 0.09699618\n",
      "Iteration 7674, loss = 0.09696891\n",
      "Iteration 7675, loss = 0.09694166\n",
      "Iteration 7676, loss = 0.09691442\n",
      "Iteration 7677, loss = 0.09688720\n",
      "Iteration 7678, loss = 0.09685999\n",
      "Iteration 7679, loss = 0.09683280\n",
      "Iteration 7680, loss = 0.09680562\n",
      "Iteration 7681, loss = 0.09677845\n",
      "Iteration 7682, loss = 0.09675130\n",
      "Iteration 7683, loss = 0.09672417\n",
      "Iteration 7684, loss = 0.09669705\n",
      "Iteration 7685, loss = 0.09666994\n",
      "Iteration 7686, loss = 0.09664285\n",
      "Iteration 7687, loss = 0.09661578\n",
      "Iteration 7688, loss = 0.09658872\n",
      "Iteration 7689, loss = 0.09656167\n",
      "Iteration 7690, loss = 0.09653464\n",
      "Iteration 7691, loss = 0.09650763\n",
      "Iteration 7692, loss = 0.09648063\n",
      "Iteration 7693, loss = 0.09645364\n",
      "Iteration 7694, loss = 0.09642667\n",
      "Iteration 7695, loss = 0.09639971\n",
      "Iteration 7696, loss = 0.09637277\n",
      "Iteration 7697, loss = 0.09634584\n",
      "Iteration 7698, loss = 0.09631893\n",
      "Iteration 7699, loss = 0.09629203\n",
      "Iteration 7700, loss = 0.09626515\n",
      "Iteration 7701, loss = 0.09623828\n",
      "Iteration 7702, loss = 0.09621143\n",
      "Iteration 7703, loss = 0.09618459\n",
      "Iteration 7704, loss = 0.09615776\n",
      "Iteration 7705, loss = 0.09613095\n",
      "Iteration 7706, loss = 0.09610416\n",
      "Iteration 7707, loss = 0.09607738\n",
      "Iteration 7708, loss = 0.09605061\n",
      "Iteration 7709, loss = 0.09602386\n",
      "Iteration 7710, loss = 0.09599712\n",
      "Iteration 7711, loss = 0.09597040\n",
      "Iteration 7712, loss = 0.09594369\n",
      "Iteration 7713, loss = 0.09591700\n",
      "Iteration 7714, loss = 0.09589032\n",
      "Iteration 7715, loss = 0.09586366\n",
      "Iteration 7716, loss = 0.09583701\n",
      "Iteration 7717, loss = 0.09581037\n",
      "Iteration 7718, loss = 0.09578375\n",
      "Iteration 7719, loss = 0.09575715\n",
      "Iteration 7720, loss = 0.09573056\n",
      "Iteration 7721, loss = 0.09570398\n",
      "Iteration 7722, loss = 0.09567742\n",
      "Iteration 7723, loss = 0.09565087\n",
      "Iteration 7724, loss = 0.09562433\n",
      "Iteration 7725, loss = 0.09559782\n",
      "Iteration 7726, loss = 0.09557131\n",
      "Iteration 7727, loss = 0.09554482\n",
      "Iteration 7728, loss = 0.09551834\n",
      "Iteration 7729, loss = 0.09549188\n",
      "Iteration 7730, loss = 0.09546543\n",
      "Iteration 7731, loss = 0.09543900\n",
      "Iteration 7732, loss = 0.09541258\n",
      "Iteration 7733, loss = 0.09538618\n",
      "Iteration 7734, loss = 0.09535979\n",
      "Iteration 7735, loss = 0.09533341\n",
      "Iteration 7736, loss = 0.09530705\n",
      "Iteration 7737, loss = 0.09528071\n",
      "Iteration 7738, loss = 0.09525437\n",
      "Iteration 7739, loss = 0.09522805\n",
      "Iteration 7740, loss = 0.09520175\n",
      "Iteration 7741, loss = 0.09517546\n",
      "Iteration 7742, loss = 0.09514919\n",
      "Iteration 7743, loss = 0.09512292\n",
      "Iteration 7744, loss = 0.09509668\n",
      "Iteration 7745, loss = 0.09507044\n",
      "Iteration 7746, loss = 0.09504423\n",
      "Iteration 7747, loss = 0.09501802\n",
      "Iteration 7748, loss = 0.09499183\n",
      "Iteration 7749, loss = 0.09496565\n",
      "Iteration 7750, loss = 0.09493949\n",
      "Iteration 7751, loss = 0.09491334\n",
      "Iteration 7752, loss = 0.09488721\n",
      "Iteration 7753, loss = 0.09486109\n",
      "Iteration 7754, loss = 0.09483499\n",
      "Iteration 7755, loss = 0.09480890\n",
      "Iteration 7756, loss = 0.09478282\n",
      "Iteration 7757, loss = 0.09475675\n",
      "Iteration 7758, loss = 0.09473071\n",
      "Iteration 7759, loss = 0.09470467\n",
      "Iteration 7760, loss = 0.09467865\n",
      "Iteration 7761, loss = 0.09465264\n",
      "Iteration 7762, loss = 0.09462665\n",
      "Iteration 7763, loss = 0.09460067\n",
      "Iteration 7764, loss = 0.09457471\n",
      "Iteration 7765, loss = 0.09454876\n",
      "Iteration 7766, loss = 0.09452282\n",
      "Iteration 7767, loss = 0.09449690\n",
      "Iteration 7768, loss = 0.09447099\n",
      "Iteration 7769, loss = 0.09444510\n",
      "Iteration 7770, loss = 0.09441921\n",
      "Iteration 7771, loss = 0.09439335\n",
      "Iteration 7772, loss = 0.09436750\n",
      "Iteration 7773, loss = 0.09434166\n",
      "Iteration 7774, loss = 0.09431583\n",
      "Iteration 7775, loss = 0.09429002\n",
      "Iteration 7776, loss = 0.09426422\n",
      "Iteration 7777, loss = 0.09423844\n",
      "Iteration 7778, loss = 0.09421267\n",
      "Iteration 7779, loss = 0.09418692\n",
      "Iteration 7780, loss = 0.09416117\n",
      "Iteration 7781, loss = 0.09413545\n",
      "Iteration 7782, loss = 0.09410973\n",
      "Iteration 7783, loss = 0.09408403\n",
      "Iteration 7784, loss = 0.09405835\n",
      "Iteration 7785, loss = 0.09403267\n",
      "Iteration 7786, loss = 0.09400702\n",
      "Iteration 7787, loss = 0.09398137\n",
      "Iteration 7788, loss = 0.09395574\n",
      "Iteration 7789, loss = 0.09393012\n",
      "Iteration 7790, loss = 0.09390452\n",
      "Iteration 7791, loss = 0.09387893\n",
      "Iteration 7792, loss = 0.09385336\n",
      "Iteration 7793, loss = 0.09382779\n",
      "Iteration 7794, loss = 0.09380224\n",
      "Iteration 7795, loss = 0.09377671\n",
      "Iteration 7796, loss = 0.09375119\n",
      "Iteration 7797, loss = 0.09372568\n",
      "Iteration 7798, loss = 0.09370019\n",
      "Iteration 7799, loss = 0.09367471\n",
      "Iteration 7800, loss = 0.09364924\n",
      "Iteration 7801, loss = 0.09362379\n",
      "Iteration 7802, loss = 0.09359835\n",
      "Iteration 7803, loss = 0.09357293\n",
      "Iteration 7804, loss = 0.09354751\n",
      "Iteration 7805, loss = 0.09352212\n",
      "Iteration 7806, loss = 0.09349673\n",
      "Iteration 7807, loss = 0.09347136\n",
      "Iteration 7808, loss = 0.09344600\n",
      "Iteration 7809, loss = 0.09342066\n",
      "Iteration 7810, loss = 0.09339533\n",
      "Iteration 7811, loss = 0.09337001\n",
      "Iteration 7812, loss = 0.09334471\n",
      "Iteration 7813, loss = 0.09331942\n",
      "Iteration 7814, loss = 0.09329415\n",
      "Iteration 7815, loss = 0.09326888\n",
      "Iteration 7816, loss = 0.09324364\n",
      "Iteration 7817, loss = 0.09321840\n",
      "Iteration 7818, loss = 0.09319318\n",
      "Iteration 7819, loss = 0.09316797\n",
      "Iteration 7820, loss = 0.09314278\n",
      "Iteration 7821, loss = 0.09311759\n",
      "Iteration 7822, loss = 0.09309243\n",
      "Iteration 7823, loss = 0.09306727\n",
      "Iteration 7824, loss = 0.09304213\n",
      "Iteration 7825, loss = 0.09301700\n",
      "Iteration 7826, loss = 0.09299189\n",
      "Iteration 7827, loss = 0.09296679\n",
      "Iteration 7828, loss = 0.09294170\n",
      "Iteration 7829, loss = 0.09291663\n",
      "Iteration 7830, loss = 0.09289157\n",
      "Iteration 7831, loss = 0.09286652\n",
      "Iteration 7832, loss = 0.09284149\n",
      "Iteration 7833, loss = 0.09281647\n",
      "Iteration 7834, loss = 0.09279146\n",
      "Iteration 7835, loss = 0.09276646\n",
      "Iteration 7836, loss = 0.09274148\n",
      "Iteration 7837, loss = 0.09271652\n",
      "Iteration 7838, loss = 0.09269156\n",
      "Iteration 7839, loss = 0.09266662\n",
      "Iteration 7840, loss = 0.09264170\n",
      "Iteration 7841, loss = 0.09261678\n",
      "Iteration 7842, loss = 0.09259188\n",
      "Iteration 7843, loss = 0.09256699\n",
      "Iteration 7844, loss = 0.09254212\n",
      "Iteration 7845, loss = 0.09251726\n",
      "Iteration 7846, loss = 0.09249241\n",
      "Iteration 7847, loss = 0.09246758\n",
      "Iteration 7848, loss = 0.09244276\n",
      "Iteration 7849, loss = 0.09241795\n",
      "Iteration 7850, loss = 0.09239315\n",
      "Iteration 7851, loss = 0.09236837\n",
      "Iteration 7852, loss = 0.09234360\n",
      "Iteration 7853, loss = 0.09231885\n",
      "Iteration 7854, loss = 0.09229411\n",
      "Iteration 7855, loss = 0.09226938\n",
      "Iteration 7856, loss = 0.09224466\n",
      "Iteration 7857, loss = 0.09221996\n",
      "Iteration 7858, loss = 0.09219527\n",
      "Iteration 7859, loss = 0.09217059\n",
      "Iteration 7860, loss = 0.09214593\n",
      "Iteration 7861, loss = 0.09212128\n",
      "Iteration 7862, loss = 0.09209664\n",
      "Iteration 7863, loss = 0.09207202\n",
      "Iteration 7864, loss = 0.09204741\n",
      "Iteration 7865, loss = 0.09202281\n",
      "Iteration 7866, loss = 0.09199822\n",
      "Iteration 7867, loss = 0.09197365\n",
      "Iteration 7868, loss = 0.09194909\n",
      "Iteration 7869, loss = 0.09192455\n",
      "Iteration 7870, loss = 0.09190001\n",
      "Iteration 7871, loss = 0.09187549\n",
      "Iteration 7872, loss = 0.09185099\n",
      "Iteration 7873, loss = 0.09182649\n",
      "Iteration 7874, loss = 0.09180201\n",
      "Iteration 7875, loss = 0.09177754\n",
      "Iteration 7876, loss = 0.09175309\n",
      "Iteration 7877, loss = 0.09172865\n",
      "Iteration 7878, loss = 0.09170422\n",
      "Iteration 7879, loss = 0.09167980\n",
      "Iteration 7880, loss = 0.09165540\n",
      "Iteration 7881, loss = 0.09163101\n",
      "Iteration 7882, loss = 0.09160663\n",
      "Iteration 7883, loss = 0.09158226\n",
      "Iteration 7884, loss = 0.09155791\n",
      "Iteration 7885, loss = 0.09153357\n",
      "Iteration 7886, loss = 0.09150925\n",
      "Iteration 7887, loss = 0.09148493\n",
      "Iteration 7888, loss = 0.09146063\n",
      "Iteration 7889, loss = 0.09143635\n",
      "Iteration 7890, loss = 0.09141207\n",
      "Iteration 7891, loss = 0.09138781\n",
      "Iteration 7892, loss = 0.09136356\n",
      "Iteration 7893, loss = 0.09133932\n",
      "Iteration 7894, loss = 0.09131510\n",
      "Iteration 7895, loss = 0.09129089\n",
      "Iteration 7896, loss = 0.09126669\n",
      "Iteration 7897, loss = 0.09124251\n",
      "Iteration 7898, loss = 0.09121834\n",
      "Iteration 7899, loss = 0.09119418\n",
      "Iteration 7900, loss = 0.09117003\n",
      "Iteration 7901, loss = 0.09114589\n",
      "Iteration 7902, loss = 0.09112177\n",
      "Iteration 7903, loss = 0.09109766\n",
      "Iteration 7904, loss = 0.09107357\n",
      "Iteration 7905, loss = 0.09104948\n",
      "Iteration 7906, loss = 0.09102541\n",
      "Iteration 7907, loss = 0.09100136\n",
      "Iteration 7908, loss = 0.09097731\n",
      "Iteration 7909, loss = 0.09095328\n",
      "Iteration 7910, loss = 0.09092926\n",
      "Iteration 7911, loss = 0.09090525\n",
      "Iteration 7912, loss = 0.09088126\n",
      "Iteration 7913, loss = 0.09085727\n",
      "Iteration 7914, loss = 0.09083330\n",
      "Iteration 7915, loss = 0.09080935\n",
      "Iteration 7916, loss = 0.09078540\n",
      "Iteration 7917, loss = 0.09076147\n",
      "Iteration 7918, loss = 0.09073755\n",
      "Iteration 7919, loss = 0.09071365\n",
      "Iteration 7920, loss = 0.09068975\n",
      "Iteration 7921, loss = 0.09066587\n",
      "Iteration 7922, loss = 0.09064200\n",
      "Iteration 7923, loss = 0.09061814\n",
      "Iteration 7924, loss = 0.09059430\n",
      "Iteration 7925, loss = 0.09057047\n",
      "Iteration 7926, loss = 0.09054665\n",
      "Iteration 7927, loss = 0.09052284\n",
      "Iteration 7928, loss = 0.09049905\n",
      "Iteration 7929, loss = 0.09047527\n",
      "Iteration 7930, loss = 0.09045150\n",
      "Iteration 7931, loss = 0.09042774\n",
      "Iteration 7932, loss = 0.09040400\n",
      "Iteration 7933, loss = 0.09038027\n",
      "Iteration 7934, loss = 0.09035655\n",
      "Iteration 7935, loss = 0.09033284\n",
      "Iteration 7936, loss = 0.09030915\n",
      "Iteration 7937, loss = 0.09028546\n",
      "Iteration 7938, loss = 0.09026179\n",
      "Iteration 7939, loss = 0.09023814\n",
      "Iteration 7940, loss = 0.09021449\n",
      "Iteration 7941, loss = 0.09019086\n",
      "Iteration 7942, loss = 0.09016724\n",
      "Iteration 7943, loss = 0.09014363\n",
      "Iteration 7944, loss = 0.09012004\n",
      "Iteration 7945, loss = 0.09009645\n",
      "Iteration 7946, loss = 0.09007288\n",
      "Iteration 7947, loss = 0.09004933\n",
      "Iteration 7948, loss = 0.09002578\n",
      "Iteration 7949, loss = 0.09000225\n",
      "Iteration 7950, loss = 0.08997872\n",
      "Iteration 7951, loss = 0.08995522\n",
      "Iteration 7952, loss = 0.08993172\n",
      "Iteration 7953, loss = 0.08990823\n",
      "Iteration 7954, loss = 0.08988476\n",
      "Iteration 7955, loss = 0.08986130\n",
      "Iteration 7956, loss = 0.08983785\n",
      "Iteration 7957, loss = 0.08981442\n",
      "Iteration 7958, loss = 0.08979099\n",
      "Iteration 7959, loss = 0.08976758\n",
      "Iteration 7960, loss = 0.08974418\n",
      "Iteration 7961, loss = 0.08972080\n",
      "Iteration 7962, loss = 0.08969742\n",
      "Iteration 7963, loss = 0.08967406\n",
      "Iteration 7964, loss = 0.08965071\n",
      "Iteration 7965, loss = 0.08962737\n",
      "Iteration 7966, loss = 0.08960405\n",
      "Iteration 7967, loss = 0.08958073\n",
      "Iteration 7968, loss = 0.08955743\n",
      "Iteration 7969, loss = 0.08953414\n",
      "Iteration 7970, loss = 0.08951087\n",
      "Iteration 7971, loss = 0.08948760\n",
      "Iteration 7972, loss = 0.08946435\n",
      "Iteration 7973, loss = 0.08944111\n",
      "Iteration 7974, loss = 0.08941788\n",
      "Iteration 7975, loss = 0.08939466\n",
      "Iteration 7976, loss = 0.08937146\n",
      "Iteration 7977, loss = 0.08934826\n",
      "Iteration 7978, loss = 0.08932508\n",
      "Iteration 7979, loss = 0.08930191\n",
      "Iteration 7980, loss = 0.08927876\n",
      "Iteration 7981, loss = 0.08925561\n",
      "Iteration 7982, loss = 0.08923248\n",
      "Iteration 7983, loss = 0.08920936\n",
      "Iteration 7984, loss = 0.08918625\n",
      "Iteration 7985, loss = 0.08916315\n",
      "Iteration 7986, loss = 0.08914007\n",
      "Iteration 7987, loss = 0.08911700\n",
      "Iteration 7988, loss = 0.08909394\n",
      "Iteration 7989, loss = 0.08907089\n",
      "Iteration 7990, loss = 0.08904785\n",
      "Iteration 7991, loss = 0.08902483\n",
      "Iteration 7992, loss = 0.08900182\n",
      "Iteration 7993, loss = 0.08897881\n",
      "Iteration 7994, loss = 0.08895583\n",
      "Iteration 7995, loss = 0.08893285\n",
      "Iteration 7996, loss = 0.08890988\n",
      "Iteration 7997, loss = 0.08888693\n",
      "Iteration 7998, loss = 0.08886399\n",
      "Iteration 7999, loss = 0.08884106\n",
      "Iteration 8000, loss = 0.08881814\n",
      "Iteration 8001, loss = 0.08879524\n",
      "Iteration 8002, loss = 0.08877234\n",
      "Iteration 8003, loss = 0.08874946\n",
      "Iteration 8004, loss = 0.08872659\n",
      "Iteration 8005, loss = 0.08870373\n",
      "Iteration 8006, loss = 0.08868089\n",
      "Iteration 8007, loss = 0.08865805\n",
      "Iteration 8008, loss = 0.08863523\n",
      "Iteration 8009, loss = 0.08861242\n",
      "Iteration 8010, loss = 0.08858962\n",
      "Iteration 8011, loss = 0.08856683\n",
      "Iteration 8012, loss = 0.08854406\n",
      "Iteration 8013, loss = 0.08852129\n",
      "Iteration 8014, loss = 0.08849854\n",
      "Iteration 8015, loss = 0.08847580\n",
      "Iteration 8016, loss = 0.08845307\n",
      "Iteration 8017, loss = 0.08843036\n",
      "Iteration 8018, loss = 0.08840765\n",
      "Iteration 8019, loss = 0.08838496\n",
      "Iteration 8020, loss = 0.08836228\n",
      "Iteration 8021, loss = 0.08833961\n",
      "Iteration 8022, loss = 0.08831695\n",
      "Iteration 8023, loss = 0.08829430\n",
      "Iteration 8024, loss = 0.08827167\n",
      "Iteration 8025, loss = 0.08824904\n",
      "Iteration 8026, loss = 0.08822643\n",
      "Iteration 8027, loss = 0.08820383\n",
      "Iteration 8028, loss = 0.08818124\n",
      "Iteration 8029, loss = 0.08815867\n",
      "Iteration 8030, loss = 0.08813610\n",
      "Iteration 8031, loss = 0.08811355\n",
      "Iteration 8032, loss = 0.08809101\n",
      "Iteration 8033, loss = 0.08806848\n",
      "Iteration 8034, loss = 0.08804596\n",
      "Iteration 8035, loss = 0.08802345\n",
      "Iteration 8036, loss = 0.08800095\n",
      "Iteration 8037, loss = 0.08797847\n",
      "Iteration 8038, loss = 0.08795600\n",
      "Iteration 8039, loss = 0.08793354\n",
      "Iteration 8040, loss = 0.08791109\n",
      "Iteration 8041, loss = 0.08788865\n",
      "Iteration 8042, loss = 0.08786623\n",
      "Iteration 8043, loss = 0.08784381\n",
      "Iteration 8044, loss = 0.08782141\n",
      "Iteration 8045, loss = 0.08779902\n",
      "Iteration 8046, loss = 0.08777664\n",
      "Iteration 8047, loss = 0.08775427\n",
      "Iteration 8048, loss = 0.08773191\n",
      "Iteration 8049, loss = 0.08770957\n",
      "Iteration 8050, loss = 0.08768723\n",
      "Iteration 8051, loss = 0.08766491\n",
      "Iteration 8052, loss = 0.08764260\n",
      "Iteration 8053, loss = 0.08762030\n",
      "Iteration 8054, loss = 0.08759801\n",
      "Iteration 8055, loss = 0.08757573\n",
      "Iteration 8056, loss = 0.08755347\n",
      "Iteration 8057, loss = 0.08753121\n",
      "Iteration 8058, loss = 0.08750897\n",
      "Iteration 8059, loss = 0.08748674\n",
      "Iteration 8060, loss = 0.08746452\n",
      "Iteration 8061, loss = 0.08744231\n",
      "Iteration 8062, loss = 0.08742011\n",
      "Iteration 8063, loss = 0.08739793\n",
      "Iteration 8064, loss = 0.08737575\n",
      "Iteration 8065, loss = 0.08735359\n",
      "Iteration 8066, loss = 0.08733144\n",
      "Iteration 8067, loss = 0.08730930\n",
      "Iteration 8068, loss = 0.08728717\n",
      "Iteration 8069, loss = 0.08726505\n",
      "Iteration 8070, loss = 0.08724295\n",
      "Iteration 8071, loss = 0.08722085\n",
      "Iteration 8072, loss = 0.08719877\n",
      "Iteration 8073, loss = 0.08717670\n",
      "Iteration 8074, loss = 0.08715463\n",
      "Iteration 8075, loss = 0.08713258\n",
      "Iteration 8076, loss = 0.08711055\n",
      "Iteration 8077, loss = 0.08708852\n",
      "Iteration 8078, loss = 0.08706650\n",
      "Iteration 8079, loss = 0.08704450\n",
      "Iteration 8080, loss = 0.08702250\n",
      "Iteration 8081, loss = 0.08700052\n",
      "Iteration 8082, loss = 0.08697855\n",
      "Iteration 8083, loss = 0.08695659\n",
      "Iteration 8084, loss = 0.08693464\n",
      "Iteration 8085, loss = 0.08691270\n",
      "Iteration 8086, loss = 0.08689078\n",
      "Iteration 8087, loss = 0.08686886\n",
      "Iteration 8088, loss = 0.08684696\n",
      "Iteration 8089, loss = 0.08682507\n",
      "Iteration 8090, loss = 0.08680319\n",
      "Iteration 8091, loss = 0.08678131\n",
      "Iteration 8092, loss = 0.08675946\n",
      "Iteration 8093, loss = 0.08673761\n",
      "Iteration 8094, loss = 0.08671577\n",
      "Iteration 8095, loss = 0.08669394\n",
      "Iteration 8096, loss = 0.08667213\n",
      "Iteration 8097, loss = 0.08665033\n",
      "Iteration 8098, loss = 0.08662853\n",
      "Iteration 8099, loss = 0.08660675\n",
      "Iteration 8100, loss = 0.08658498\n",
      "Iteration 8101, loss = 0.08656322\n",
      "Iteration 8102, loss = 0.08654147\n",
      "Iteration 8103, loss = 0.08651974\n",
      "Iteration 8104, loss = 0.08649801\n",
      "Iteration 8105, loss = 0.08647629\n",
      "Iteration 8106, loss = 0.08645459\n",
      "Iteration 8107, loss = 0.08643290\n",
      "Iteration 8108, loss = 0.08641122\n",
      "Iteration 8109, loss = 0.08638954\n",
      "Iteration 8110, loss = 0.08636788\n",
      "Iteration 8111, loss = 0.08634623\n",
      "Iteration 8112, loss = 0.08632460\n",
      "Iteration 8113, loss = 0.08630297\n",
      "Iteration 8114, loss = 0.08628135\n",
      "Iteration 8115, loss = 0.08625975\n",
      "Iteration 8116, loss = 0.08623815\n",
      "Iteration 8117, loss = 0.08621657\n",
      "Iteration 8118, loss = 0.08619500\n",
      "Iteration 8119, loss = 0.08617344\n",
      "Iteration 8120, loss = 0.08615189\n",
      "Iteration 8121, loss = 0.08613035\n",
      "Iteration 8122, loss = 0.08610882\n",
      "Iteration 8123, loss = 0.08608730\n",
      "Iteration 8124, loss = 0.08606579\n",
      "Iteration 8125, loss = 0.08604430\n",
      "Iteration 8126, loss = 0.08602281\n",
      "Iteration 8127, loss = 0.08600134\n",
      "Iteration 8128, loss = 0.08597988\n",
      "Iteration 8129, loss = 0.08595842\n",
      "Iteration 8130, loss = 0.08593698\n",
      "Iteration 8131, loss = 0.08591555\n",
      "Iteration 8132, loss = 0.08589413\n",
      "Iteration 8133, loss = 0.08587272\n",
      "Iteration 8134, loss = 0.08585133\n",
      "Iteration 8135, loss = 0.08582994\n",
      "Iteration 8136, loss = 0.08580856\n",
      "Iteration 8137, loss = 0.08578720\n",
      "Iteration 8138, loss = 0.08576584\n",
      "Iteration 8139, loss = 0.08574450\n",
      "Iteration 8140, loss = 0.08572316\n",
      "Iteration 8141, loss = 0.08570184\n",
      "Iteration 8142, loss = 0.08568053\n",
      "Iteration 8143, loss = 0.08565923\n",
      "Iteration 8144, loss = 0.08563794\n",
      "Iteration 8145, loss = 0.08561666\n",
      "Iteration 8146, loss = 0.08559539\n",
      "Iteration 8147, loss = 0.08557413\n",
      "Iteration 8148, loss = 0.08555289\n",
      "Iteration 8149, loss = 0.08553165\n",
      "Iteration 8150, loss = 0.08551042\n",
      "Iteration 8151, loss = 0.08548921\n",
      "Iteration 8152, loss = 0.08546800\n",
      "Iteration 8153, loss = 0.08544681\n",
      "Iteration 8154, loss = 0.08542563\n",
      "Iteration 8155, loss = 0.08540445\n",
      "Iteration 8156, loss = 0.08538329\n",
      "Iteration 8157, loss = 0.08536214\n",
      "Iteration 8158, loss = 0.08534100\n",
      "Iteration 8159, loss = 0.08531987\n",
      "Iteration 8160, loss = 0.08529875\n",
      "Iteration 8161, loss = 0.08527764\n",
      "Iteration 8162, loss = 0.08525655\n",
      "Iteration 8163, loss = 0.08523546\n",
      "Iteration 8164, loss = 0.08521438\n",
      "Iteration 8165, loss = 0.08519332\n",
      "Iteration 8166, loss = 0.08517226\n",
      "Iteration 8167, loss = 0.08515122\n",
      "Iteration 8168, loss = 0.08513018\n",
      "Iteration 8169, loss = 0.08510916\n",
      "Iteration 8170, loss = 0.08508814\n",
      "Iteration 8171, loss = 0.08506714\n",
      "Iteration 8172, loss = 0.08504615\n",
      "Iteration 8173, loss = 0.08502517\n",
      "Iteration 8174, loss = 0.08500420\n",
      "Iteration 8175, loss = 0.08498324\n",
      "Iteration 8176, loss = 0.08496229\n",
      "Iteration 8177, loss = 0.08494135\n",
      "Iteration 8178, loss = 0.08492042\n",
      "Iteration 8179, loss = 0.08489950\n",
      "Iteration 8180, loss = 0.08487859\n",
      "Iteration 8181, loss = 0.08485770\n",
      "Iteration 8182, loss = 0.08483681\n",
      "Iteration 8183, loss = 0.08481593\n",
      "Iteration 8184, loss = 0.08479507\n",
      "Iteration 8185, loss = 0.08477421\n",
      "Iteration 8186, loss = 0.08475337\n",
      "Iteration 8187, loss = 0.08473253\n",
      "Iteration 8188, loss = 0.08471171\n",
      "Iteration 8189, loss = 0.08469089\n",
      "Iteration 8190, loss = 0.08467009\n",
      "Iteration 8191, loss = 0.08464930\n",
      "Iteration 8192, loss = 0.08462851\n",
      "Iteration 8193, loss = 0.08460774\n",
      "Iteration 8194, loss = 0.08458698\n",
      "Iteration 8195, loss = 0.08456623\n",
      "Iteration 8196, loss = 0.08454549\n",
      "Iteration 8197, loss = 0.08452476\n",
      "Iteration 8198, loss = 0.08450404\n",
      "Iteration 8199, loss = 0.08448333\n",
      "Iteration 8200, loss = 0.08446263\n",
      "Iteration 8201, loss = 0.08444194\n",
      "Iteration 8202, loss = 0.08442126\n",
      "Iteration 8203, loss = 0.08440059\n",
      "Iteration 8204, loss = 0.08437993\n",
      "Iteration 8205, loss = 0.08435929\n",
      "Iteration 8206, loss = 0.08433865\n",
      "Iteration 8207, loss = 0.08431802\n",
      "Iteration 8208, loss = 0.08429741\n",
      "Iteration 8209, loss = 0.08427680\n",
      "Iteration 8210, loss = 0.08425620\n",
      "Iteration 8211, loss = 0.08423562\n",
      "Iteration 8212, loss = 0.08421504\n",
      "Iteration 8213, loss = 0.08419448\n",
      "Iteration 8214, loss = 0.08417392\n",
      "Iteration 8215, loss = 0.08415338\n",
      "Iteration 8216, loss = 0.08413284\n",
      "Iteration 8217, loss = 0.08411232\n",
      "Iteration 8218, loss = 0.08409180\n",
      "Iteration 8219, loss = 0.08407130\n",
      "Iteration 8220, loss = 0.08405080\n",
      "Iteration 8221, loss = 0.08403032\n",
      "Iteration 8222, loss = 0.08400985\n",
      "Iteration 8223, loss = 0.08398938\n",
      "Iteration 8224, loss = 0.08396893\n",
      "Iteration 8225, loss = 0.08394849\n",
      "Iteration 8226, loss = 0.08392806\n",
      "Iteration 8227, loss = 0.08390763\n",
      "Iteration 8228, loss = 0.08388722\n",
      "Iteration 8229, loss = 0.08386682\n",
      "Iteration 8230, loss = 0.08384643\n",
      "Iteration 8231, loss = 0.08382605\n",
      "Iteration 8232, loss = 0.08380567\n",
      "Iteration 8233, loss = 0.08378531\n",
      "Iteration 8234, loss = 0.08376496\n",
      "Iteration 8235, loss = 0.08374462\n",
      "Iteration 8236, loss = 0.08372429\n",
      "Iteration 8237, loss = 0.08370397\n",
      "Iteration 8238, loss = 0.08368366\n",
      "Iteration 8239, loss = 0.08366336\n",
      "Iteration 8240, loss = 0.08364306\n",
      "Iteration 8241, loss = 0.08362278\n",
      "Iteration 8242, loss = 0.08360251\n",
      "Iteration 8243, loss = 0.08358225\n",
      "Iteration 8244, loss = 0.08356200\n",
      "Iteration 8245, loss = 0.08354176\n",
      "Iteration 8246, loss = 0.08352153\n",
      "Iteration 8247, loss = 0.08350131\n",
      "Iteration 8248, loss = 0.08348110\n",
      "Iteration 8249, loss = 0.08346090\n",
      "Iteration 8250, loss = 0.08344071\n",
      "Iteration 8251, loss = 0.08342053\n",
      "Iteration 8252, loss = 0.08340036\n",
      "Iteration 8253, loss = 0.08338020\n",
      "Iteration 8254, loss = 0.08336005\n",
      "Iteration 8255, loss = 0.08333991\n",
      "Iteration 8256, loss = 0.08331978\n",
      "Iteration 8257, loss = 0.08329966\n",
      "Iteration 8258, loss = 0.08327954\n",
      "Iteration 8259, loss = 0.08325944\n",
      "Iteration 8260, loss = 0.08323935\n",
      "Iteration 8261, loss = 0.08321927\n",
      "Iteration 8262, loss = 0.08319920\n",
      "Iteration 8263, loss = 0.08317914\n",
      "Iteration 8264, loss = 0.08315909\n",
      "Iteration 8265, loss = 0.08313905\n",
      "Iteration 8266, loss = 0.08311902\n",
      "Iteration 8267, loss = 0.08309900\n",
      "Iteration 8268, loss = 0.08307899\n",
      "Iteration 8269, loss = 0.08305898\n",
      "Iteration 8270, loss = 0.08303899\n",
      "Iteration 8271, loss = 0.08301901\n",
      "Iteration 8272, loss = 0.08299904\n",
      "Iteration 8273, loss = 0.08297908\n",
      "Iteration 8274, loss = 0.08295912\n",
      "Iteration 8275, loss = 0.08293918\n",
      "Iteration 8276, loss = 0.08291925\n",
      "Iteration 8277, loss = 0.08289933\n",
      "Iteration 8278, loss = 0.08287941\n",
      "Iteration 8279, loss = 0.08285951\n",
      "Iteration 8280, loss = 0.08283962\n",
      "Iteration 8281, loss = 0.08281973\n",
      "Iteration 8282, loss = 0.08279986\n",
      "Iteration 8283, loss = 0.08278000\n",
      "Iteration 8284, loss = 0.08276014\n",
      "Iteration 8285, loss = 0.08274030\n",
      "Iteration 8286, loss = 0.08272046\n",
      "Iteration 8287, loss = 0.08270064\n",
      "Iteration 8288, loss = 0.08268083\n",
      "Iteration 8289, loss = 0.08266102\n",
      "Iteration 8290, loss = 0.08264122\n",
      "Iteration 8291, loss = 0.08262144\n",
      "Iteration 8292, loss = 0.08260166\n",
      "Iteration 8293, loss = 0.08258190\n",
      "Iteration 8294, loss = 0.08256214\n",
      "Iteration 8295, loss = 0.08254239\n",
      "Iteration 8296, loss = 0.08252266\n",
      "Iteration 8297, loss = 0.08250293\n",
      "Iteration 8298, loss = 0.08248321\n",
      "Iteration 8299, loss = 0.08246350\n",
      "Iteration 8300, loss = 0.08244381\n",
      "Iteration 8301, loss = 0.08242412\n",
      "Iteration 8302, loss = 0.08240444\n",
      "Iteration 8303, loss = 0.08238477\n",
      "Iteration 8304, loss = 0.08236511\n",
      "Iteration 8305, loss = 0.08234546\n",
      "Iteration 8306, loss = 0.08232582\n",
      "Iteration 8307, loss = 0.08230619\n",
      "Iteration 8308, loss = 0.08228657\n",
      "Iteration 8309, loss = 0.08226696\n",
      "Iteration 8310, loss = 0.08224735\n",
      "Iteration 8311, loss = 0.08222776\n",
      "Iteration 8312, loss = 0.08220818\n",
      "Iteration 8313, loss = 0.08218861\n",
      "Iteration 8314, loss = 0.08216904\n",
      "Iteration 8315, loss = 0.08214949\n",
      "Iteration 8316, loss = 0.08212995\n",
      "Iteration 8317, loss = 0.08211041\n",
      "Iteration 8318, loss = 0.08209089\n",
      "Iteration 8319, loss = 0.08207137\n",
      "Iteration 8320, loss = 0.08205186\n",
      "Iteration 8321, loss = 0.08203237\n",
      "Iteration 8322, loss = 0.08201288\n",
      "Iteration 8323, loss = 0.08199340\n",
      "Iteration 8324, loss = 0.08197394\n",
      "Iteration 8325, loss = 0.08195448\n",
      "Iteration 8326, loss = 0.08193503\n",
      "Iteration 8327, loss = 0.08191559\n",
      "Iteration 8328, loss = 0.08189616\n",
      "Iteration 8329, loss = 0.08187674\n",
      "Iteration 8330, loss = 0.08185733\n",
      "Iteration 8331, loss = 0.08183793\n",
      "Iteration 8332, loss = 0.08181854\n",
      "Iteration 8333, loss = 0.08179915\n",
      "Iteration 8334, loss = 0.08177978\n",
      "Iteration 8335, loss = 0.08176042\n",
      "Iteration 8336, loss = 0.08174106\n",
      "Iteration 8337, loss = 0.08172172\n",
      "Iteration 8338, loss = 0.08170239\n",
      "Iteration 8339, loss = 0.08168306\n",
      "Iteration 8340, loss = 0.08166374\n",
      "Iteration 8341, loss = 0.08164444\n",
      "Iteration 8342, loss = 0.08162514\n",
      "Iteration 8343, loss = 0.08160585\n",
      "Iteration 8344, loss = 0.08158657\n",
      "Iteration 8345, loss = 0.08156730\n",
      "Iteration 8346, loss = 0.08154804\n",
      "Iteration 8347, loss = 0.08152879\n",
      "Iteration 8348, loss = 0.08150955\n",
      "Iteration 8349, loss = 0.08149032\n",
      "Iteration 8350, loss = 0.08147110\n",
      "Iteration 8351, loss = 0.08145189\n",
      "Iteration 8352, loss = 0.08143268\n",
      "Iteration 8353, loss = 0.08141349\n",
      "Iteration 8354, loss = 0.08139431\n",
      "Iteration 8355, loss = 0.08137513\n",
      "Iteration 8356, loss = 0.08135596\n",
      "Iteration 8357, loss = 0.08133681\n",
      "Iteration 8358, loss = 0.08131766\n",
      "Iteration 8359, loss = 0.08129852\n",
      "Iteration 8360, loss = 0.08127939\n",
      "Iteration 8361, loss = 0.08126027\n",
      "Iteration 8362, loss = 0.08124116\n",
      "Iteration 8363, loss = 0.08122206\n",
      "Iteration 8364, loss = 0.08120297\n",
      "Iteration 8365, loss = 0.08118389\n",
      "Iteration 8366, loss = 0.08116481\n",
      "Iteration 8367, loss = 0.08114575\n",
      "Iteration 8368, loss = 0.08112670\n",
      "Iteration 8369, loss = 0.08110765\n",
      "Iteration 8370, loss = 0.08108862\n",
      "Iteration 8371, loss = 0.08106959\n",
      "Iteration 8372, loss = 0.08105057\n",
      "Iteration 8373, loss = 0.08103156\n",
      "Iteration 8374, loss = 0.08101256\n",
      "Iteration 8375, loss = 0.08099357\n",
      "Iteration 8376, loss = 0.08097459\n",
      "Iteration 8377, loss = 0.08095562\n",
      "Iteration 8378, loss = 0.08093666\n",
      "Iteration 8379, loss = 0.08091771\n",
      "Iteration 8380, loss = 0.08089876\n",
      "Iteration 8381, loss = 0.08087983\n",
      "Iteration 8382, loss = 0.08086090\n",
      "Iteration 8383, loss = 0.08084199\n",
      "Iteration 8384, loss = 0.08082308\n",
      "Iteration 8385, loss = 0.08080418\n",
      "Iteration 8386, loss = 0.08078529\n",
      "Iteration 8387, loss = 0.08076641\n",
      "Iteration 8388, loss = 0.08074754\n",
      "Iteration 8389, loss = 0.08072868\n",
      "Iteration 8390, loss = 0.08070983\n",
      "Iteration 8391, loss = 0.08069098\n",
      "Iteration 8392, loss = 0.08067215\n",
      "Iteration 8393, loss = 0.08065332\n",
      "Iteration 8394, loss = 0.08063451\n",
      "Iteration 8395, loss = 0.08061570\n",
      "Iteration 8396, loss = 0.08059690\n",
      "Iteration 8397, loss = 0.08057811\n",
      "Iteration 8398, loss = 0.08055934\n",
      "Iteration 8399, loss = 0.08054056\n",
      "Iteration 8400, loss = 0.08052180\n",
      "Iteration 8401, loss = 0.08050305\n",
      "Iteration 8402, loss = 0.08048431\n",
      "Iteration 8403, loss = 0.08046557\n",
      "Iteration 8404, loss = 0.08044685\n",
      "Iteration 8405, loss = 0.08042813\n",
      "Iteration 8406, loss = 0.08040942\n",
      "Iteration 8407, loss = 0.08039073\n",
      "Iteration 8408, loss = 0.08037204\n",
      "Iteration 8409, loss = 0.08035336\n",
      "Iteration 8410, loss = 0.08033469\n",
      "Iteration 8411, loss = 0.08031602\n",
      "Iteration 8412, loss = 0.08029737\n",
      "Iteration 8413, loss = 0.08027873\n",
      "Iteration 8414, loss = 0.08026009\n",
      "Iteration 8415, loss = 0.08024146\n",
      "Iteration 8416, loss = 0.08022285\n",
      "Iteration 8417, loss = 0.08020424\n",
      "Iteration 8418, loss = 0.08018564\n",
      "Iteration 8419, loss = 0.08016705\n",
      "Iteration 8420, loss = 0.08014847\n",
      "Iteration 8421, loss = 0.08012990\n",
      "Iteration 8422, loss = 0.08011133\n",
      "Iteration 8423, loss = 0.08009278\n",
      "Iteration 8424, loss = 0.08007423\n",
      "Iteration 8425, loss = 0.08005569\n",
      "Iteration 8426, loss = 0.08003717\n",
      "Iteration 8427, loss = 0.08001865\n",
      "Iteration 8428, loss = 0.08000014\n",
      "Iteration 8429, loss = 0.07998164\n",
      "Iteration 8430, loss = 0.07996314\n",
      "Iteration 8431, loss = 0.07994466\n",
      "Iteration 8432, loss = 0.07992619\n",
      "Iteration 8433, loss = 0.07990772\n",
      "Iteration 8434, loss = 0.07988926\n",
      "Iteration 8435, loss = 0.07987082\n",
      "Iteration 8436, loss = 0.07985238\n",
      "Iteration 8437, loss = 0.07983395\n",
      "Iteration 8438, loss = 0.07981553\n",
      "Iteration 8439, loss = 0.07979711\n",
      "Iteration 8440, loss = 0.07977871\n",
      "Iteration 8441, loss = 0.07976031\n",
      "Iteration 8442, loss = 0.07974193\n",
      "Iteration 8443, loss = 0.07972355\n",
      "Iteration 8444, loss = 0.07970518\n",
      "Iteration 8445, loss = 0.07968682\n",
      "Iteration 8446, loss = 0.07966847\n",
      "Iteration 8447, loss = 0.07965013\n",
      "Iteration 8448, loss = 0.07963180\n",
      "Iteration 8449, loss = 0.07961347\n",
      "Iteration 8450, loss = 0.07959516\n",
      "Iteration 8451, loss = 0.07957685\n",
      "Iteration 8452, loss = 0.07955855\n",
      "Iteration 8453, loss = 0.07954026\n",
      "Iteration 8454, loss = 0.07952198\n",
      "Iteration 8455, loss = 0.07950371\n",
      "Iteration 8456, loss = 0.07948545\n",
      "Iteration 8457, loss = 0.07946719\n",
      "Iteration 8458, loss = 0.07944895\n",
      "Iteration 8459, loss = 0.07943071\n",
      "Iteration 8460, loss = 0.07941248\n",
      "Iteration 8461, loss = 0.07939426\n",
      "Iteration 8462, loss = 0.07937605\n",
      "Iteration 8463, loss = 0.07935785\n",
      "Iteration 8464, loss = 0.07933966\n",
      "Iteration 8465, loss = 0.07932147\n",
      "Iteration 8466, loss = 0.07930330\n",
      "Iteration 8467, loss = 0.07928513\n",
      "Iteration 8468, loss = 0.07926697\n",
      "Iteration 8469, loss = 0.07924882\n",
      "Iteration 8470, loss = 0.07923068\n",
      "Iteration 8471, loss = 0.07921255\n",
      "Iteration 8472, loss = 0.07919442\n",
      "Iteration 8473, loss = 0.07917631\n",
      "Iteration 8474, loss = 0.07915820\n",
      "Iteration 8475, loss = 0.07914010\n",
      "Iteration 8476, loss = 0.07912201\n",
      "Iteration 8477, loss = 0.07910393\n",
      "Iteration 8478, loss = 0.07908586\n",
      "Iteration 8479, loss = 0.07906779\n",
      "Iteration 8480, loss = 0.07904974\n",
      "Iteration 8481, loss = 0.07903169\n",
      "Iteration 8482, loss = 0.07901365\n",
      "Iteration 8483, loss = 0.07899563\n",
      "Iteration 8484, loss = 0.07897761\n",
      "Iteration 8485, loss = 0.07895959\n",
      "Iteration 8486, loss = 0.07894159\n",
      "Iteration 8487, loss = 0.07892359\n",
      "Iteration 8488, loss = 0.07890561\n",
      "Iteration 8489, loss = 0.07888763\n",
      "Iteration 8490, loss = 0.07886966\n",
      "Iteration 8491, loss = 0.07885170\n",
      "Iteration 8492, loss = 0.07883375\n",
      "Iteration 8493, loss = 0.07881580\n",
      "Iteration 8494, loss = 0.07879787\n",
      "Iteration 8495, loss = 0.07877994\n",
      "Iteration 8496, loss = 0.07876202\n",
      "Iteration 8497, loss = 0.07874411\n",
      "Iteration 8498, loss = 0.07872621\n",
      "Iteration 8499, loss = 0.07870832\n",
      "Iteration 8500, loss = 0.07869044\n",
      "Iteration 8501, loss = 0.07867256\n",
      "Iteration 8502, loss = 0.07865469\n",
      "Iteration 8503, loss = 0.07863684\n",
      "Iteration 8504, loss = 0.07861898\n",
      "Iteration 8505, loss = 0.07860114\n",
      "Iteration 8506, loss = 0.07858331\n",
      "Iteration 8507, loss = 0.07856549\n",
      "Iteration 8508, loss = 0.07854767\n",
      "Iteration 8509, loss = 0.07852986\n",
      "Iteration 8510, loss = 0.07851206\n",
      "Iteration 8511, loss = 0.07849427\n",
      "Iteration 8512, loss = 0.07847649\n",
      "Iteration 8513, loss = 0.07845871\n",
      "Iteration 8514, loss = 0.07844095\n",
      "Iteration 8515, loss = 0.07842319\n",
      "Iteration 8516, loss = 0.07840544\n",
      "Iteration 8517, loss = 0.07838770\n",
      "Iteration 8518, loss = 0.07836997\n",
      "Iteration 8519, loss = 0.07835224\n",
      "Iteration 8520, loss = 0.07833453\n",
      "Iteration 8521, loss = 0.07831682\n",
      "Iteration 8522, loss = 0.07829912\n",
      "Iteration 8523, loss = 0.07828143\n",
      "Iteration 8524, loss = 0.07826375\n",
      "Iteration 8525, loss = 0.07824608\n",
      "Iteration 8526, loss = 0.07822841\n",
      "Iteration 8527, loss = 0.07821076\n",
      "Iteration 8528, loss = 0.07819311\n",
      "Iteration 8529, loss = 0.07817547\n",
      "Iteration 8530, loss = 0.07815784\n",
      "Iteration 8531, loss = 0.07814021\n",
      "Iteration 8532, loss = 0.07812260\n",
      "Iteration 8533, loss = 0.07810499\n",
      "Iteration 8534, loss = 0.07808739\n",
      "Iteration 8535, loss = 0.07806980\n",
      "Iteration 8536, loss = 0.07805222\n",
      "Iteration 8537, loss = 0.07803465\n",
      "Iteration 8538, loss = 0.07801708\n",
      "Iteration 8539, loss = 0.07799952\n",
      "Iteration 8540, loss = 0.07798197\n",
      "Iteration 8541, loss = 0.07796443\n",
      "Iteration 8542, loss = 0.07794690\n",
      "Iteration 8543, loss = 0.07792938\n",
      "Iteration 8544, loss = 0.07791186\n",
      "Iteration 8545, loss = 0.07789435\n",
      "Iteration 8546, loss = 0.07787686\n",
      "Iteration 8547, loss = 0.07785936\n",
      "Iteration 8548, loss = 0.07784188\n",
      "Iteration 8549, loss = 0.07782441\n",
      "Iteration 8550, loss = 0.07780694\n",
      "Iteration 8551, loss = 0.07778948\n",
      "Iteration 8552, loss = 0.07777203\n",
      "Iteration 8553, loss = 0.07775459\n",
      "Iteration 8554, loss = 0.07773716\n",
      "Iteration 8555, loss = 0.07771973\n",
      "Iteration 8556, loss = 0.07770232\n",
      "Iteration 8557, loss = 0.07768491\n",
      "Iteration 8558, loss = 0.07766751\n",
      "Iteration 8559, loss = 0.07765011\n",
      "Iteration 8560, loss = 0.07763273\n",
      "Iteration 8561, loss = 0.07761535\n",
      "Iteration 8562, loss = 0.07759799\n",
      "Iteration 8563, loss = 0.07758063\n",
      "Iteration 8564, loss = 0.07756327\n",
      "Iteration 8565, loss = 0.07754593\n",
      "Iteration 8566, loss = 0.07752860\n",
      "Iteration 8567, loss = 0.07751127\n",
      "Iteration 8568, loss = 0.07749395\n",
      "Iteration 8569, loss = 0.07747664\n",
      "Iteration 8570, loss = 0.07745933\n",
      "Iteration 8571, loss = 0.07744204\n",
      "Iteration 8572, loss = 0.07742475\n",
      "Iteration 8573, loss = 0.07740747\n",
      "Iteration 8574, loss = 0.07739020\n",
      "Iteration 8575, loss = 0.07737294\n",
      "Iteration 8576, loss = 0.07735569\n",
      "Iteration 8577, loss = 0.07733844\n",
      "Iteration 8578, loss = 0.07732120\n",
      "Iteration 8579, loss = 0.07730397\n",
      "Iteration 8580, loss = 0.07728675\n",
      "Iteration 8581, loss = 0.07726954\n",
      "Iteration 8582, loss = 0.07725233\n",
      "Iteration 8583, loss = 0.07723513\n",
      "Iteration 8584, loss = 0.07721794\n",
      "Iteration 8585, loss = 0.07720076\n",
      "Iteration 8586, loss = 0.07718359\n",
      "Iteration 8587, loss = 0.07716642\n",
      "Iteration 8588, loss = 0.07714927\n",
      "Iteration 8589, loss = 0.07713212\n",
      "Iteration 8590, loss = 0.07711498\n",
      "Iteration 8591, loss = 0.07709784\n",
      "Iteration 8592, loss = 0.07708072\n",
      "Iteration 8593, loss = 0.07706360\n",
      "Iteration 8594, loss = 0.07704649\n",
      "Iteration 8595, loss = 0.07702939\n",
      "Iteration 8596, loss = 0.07701229\n",
      "Iteration 8597, loss = 0.07699521\n",
      "Iteration 8598, loss = 0.07697813\n",
      "Iteration 8599, loss = 0.07696106\n",
      "Iteration 8600, loss = 0.07694400\n",
      "Iteration 8601, loss = 0.07692695\n",
      "Iteration 8602, loss = 0.07690990\n",
      "Iteration 8603, loss = 0.07689286\n",
      "Iteration 8604, loss = 0.07687583\n",
      "Iteration 8605, loss = 0.07685881\n",
      "Iteration 8606, loss = 0.07684180\n",
      "Iteration 8607, loss = 0.07682479\n",
      "Iteration 8608, loss = 0.07680779\n",
      "Iteration 8609, loss = 0.07679080\n",
      "Iteration 8610, loss = 0.07677382\n",
      "Iteration 8611, loss = 0.07675685\n",
      "Iteration 8612, loss = 0.07673988\n",
      "Iteration 8613, loss = 0.07672292\n",
      "Iteration 8614, loss = 0.07670597\n",
      "Iteration 8615, loss = 0.07668903\n",
      "Iteration 8616, loss = 0.07667209\n",
      "Iteration 8617, loss = 0.07665517\n",
      "Iteration 8618, loss = 0.07663825\n",
      "Iteration 8619, loss = 0.07662134\n",
      "Iteration 8620, loss = 0.07660443\n",
      "Iteration 8621, loss = 0.07658754\n",
      "Iteration 8622, loss = 0.07657065\n",
      "Iteration 8623, loss = 0.07655377\n",
      "Iteration 8624, loss = 0.07653690\n",
      "Iteration 8625, loss = 0.07652003\n",
      "Iteration 8626, loss = 0.07650318\n",
      "Iteration 8627, loss = 0.07648633\n",
      "Iteration 8628, loss = 0.07646949\n",
      "Iteration 8629, loss = 0.07645266\n",
      "Iteration 8630, loss = 0.07643583\n",
      "Iteration 8631, loss = 0.07641901\n",
      "Iteration 8632, loss = 0.07640220\n",
      "Iteration 8633, loss = 0.07638540\n",
      "Iteration 8634, loss = 0.07636861\n",
      "Iteration 8635, loss = 0.07635182\n",
      "Iteration 8636, loss = 0.07633505\n",
      "Iteration 8637, loss = 0.07631828\n",
      "Iteration 8638, loss = 0.07630151\n",
      "Iteration 8639, loss = 0.07628476\n",
      "Iteration 8640, loss = 0.07626801\n",
      "Iteration 8641, loss = 0.07625127\n",
      "Iteration 8642, loss = 0.07623454\n",
      "Iteration 8643, loss = 0.07621782\n",
      "Iteration 8644, loss = 0.07620110\n",
      "Iteration 8645, loss = 0.07618439\n",
      "Iteration 8646, loss = 0.07616769\n",
      "Iteration 8647, loss = 0.07615100\n",
      "Iteration 8648, loss = 0.07613431\n",
      "Iteration 8649, loss = 0.07611764\n",
      "Iteration 8650, loss = 0.07610097\n",
      "Iteration 8651, loss = 0.07608431\n",
      "Iteration 8652, loss = 0.07606765\n",
      "Iteration 8653, loss = 0.07605100\n",
      "Iteration 8654, loss = 0.07603437\n",
      "Iteration 8655, loss = 0.07601773\n",
      "Iteration 8656, loss = 0.07600111\n",
      "Iteration 8657, loss = 0.07598450\n",
      "Iteration 8658, loss = 0.07596789\n",
      "Iteration 8659, loss = 0.07595129\n",
      "Iteration 8660, loss = 0.07593469\n",
      "Iteration 8661, loss = 0.07591811\n",
      "Iteration 8662, loss = 0.07590153\n",
      "Iteration 8663, loss = 0.07588496\n",
      "Iteration 8664, loss = 0.07586840\n",
      "Iteration 8665, loss = 0.07585185\n",
      "Iteration 8666, loss = 0.07583530\n",
      "Iteration 8667, loss = 0.07581876\n",
      "Iteration 8668, loss = 0.07580223\n",
      "Iteration 8669, loss = 0.07578570\n",
      "Iteration 8670, loss = 0.07576919\n",
      "Iteration 8671, loss = 0.07575268\n",
      "Iteration 8672, loss = 0.07573618\n",
      "Iteration 8673, loss = 0.07571969\n",
      "Iteration 8674, loss = 0.07570320\n",
      "Iteration 8675, loss = 0.07568672\n",
      "Iteration 8676, loss = 0.07567025\n",
      "Iteration 8677, loss = 0.07565379\n",
      "Iteration 8678, loss = 0.07563733\n",
      "Iteration 8679, loss = 0.07562088\n",
      "Iteration 8680, loss = 0.07560444\n",
      "Iteration 8681, loss = 0.07558801\n",
      "Iteration 8682, loss = 0.07557159\n",
      "Iteration 8683, loss = 0.07555517\n",
      "Iteration 8684, loss = 0.07553876\n",
      "Iteration 8685, loss = 0.07552236\n",
      "Iteration 8686, loss = 0.07550596\n",
      "Iteration 8687, loss = 0.07548957\n",
      "Iteration 8688, loss = 0.07547319\n",
      "Iteration 8689, loss = 0.07545682\n",
      "Iteration 8690, loss = 0.07544046\n",
      "Iteration 8691, loss = 0.07542410\n",
      "Iteration 8692, loss = 0.07540775\n",
      "Iteration 8693, loss = 0.07539141\n",
      "Iteration 8694, loss = 0.07537507\n",
      "Iteration 8695, loss = 0.07535875\n",
      "Iteration 8696, loss = 0.07534243\n",
      "Iteration 8697, loss = 0.07532611\n",
      "Iteration 8698, loss = 0.07530981\n",
      "Iteration 8699, loss = 0.07529351\n",
      "Iteration 8700, loss = 0.07527722\n",
      "Iteration 8701, loss = 0.07526094\n",
      "Iteration 8702, loss = 0.07524466\n",
      "Iteration 8703, loss = 0.07522840\n",
      "Iteration 8704, loss = 0.07521214\n",
      "Iteration 8705, loss = 0.07519589\n",
      "Iteration 8706, loss = 0.07517964\n",
      "Iteration 8707, loss = 0.07516340\n",
      "Iteration 8708, loss = 0.07514717\n",
      "Iteration 8709, loss = 0.07513095\n",
      "Iteration 8710, loss = 0.07511473\n",
      "Iteration 8711, loss = 0.07509853\n",
      "Iteration 8712, loss = 0.07508233\n",
      "Iteration 8713, loss = 0.07506613\n",
      "Iteration 8714, loss = 0.07504995\n",
      "Iteration 8715, loss = 0.07503377\n",
      "Iteration 8716, loss = 0.07501760\n",
      "Iteration 8717, loss = 0.07500144\n",
      "Iteration 8718, loss = 0.07498528\n",
      "Iteration 8719, loss = 0.07496913\n",
      "Iteration 8720, loss = 0.07495299\n",
      "Iteration 8721, loss = 0.07493686\n",
      "Iteration 8722, loss = 0.07492073\n",
      "Iteration 8723, loss = 0.07490461\n",
      "Iteration 8724, loss = 0.07488850\n",
      "Iteration 8725, loss = 0.07487240\n",
      "Iteration 8726, loss = 0.07485630\n",
      "Iteration 8727, loss = 0.07484021\n",
      "Iteration 8728, loss = 0.07482413\n",
      "Iteration 8729, loss = 0.07480805\n",
      "Iteration 8730, loss = 0.07479198\n",
      "Iteration 8731, loss = 0.07477592\n",
      "Iteration 8732, loss = 0.07475987\n",
      "Iteration 8733, loss = 0.07474383\n",
      "Iteration 8734, loss = 0.07472779\n",
      "Iteration 8735, loss = 0.07471176\n",
      "Iteration 8736, loss = 0.07469573\n",
      "Iteration 8737, loss = 0.07467972\n",
      "Iteration 8738, loss = 0.07466371\n",
      "Iteration 8739, loss = 0.07464771\n",
      "Iteration 8740, loss = 0.07463171\n",
      "Iteration 8741, loss = 0.07461572\n",
      "Iteration 8742, loss = 0.07459974\n",
      "Iteration 8743, loss = 0.07458377\n",
      "Iteration 8744, loss = 0.07456781\n",
      "Iteration 8745, loss = 0.07455185\n",
      "Iteration 8746, loss = 0.07453590\n",
      "Iteration 8747, loss = 0.07451995\n",
      "Iteration 8748, loss = 0.07450402\n",
      "Iteration 8749, loss = 0.07448809\n",
      "Iteration 8750, loss = 0.07447217\n",
      "Iteration 8751, loss = 0.07445625\n",
      "Iteration 8752, loss = 0.07444035\n",
      "Iteration 8753, loss = 0.07442445\n",
      "Iteration 8754, loss = 0.07440855\n",
      "Iteration 8755, loss = 0.07439267\n",
      "Iteration 8756, loss = 0.07437679\n",
      "Iteration 8757, loss = 0.07436092\n",
      "Iteration 8758, loss = 0.07434506\n",
      "Iteration 8759, loss = 0.07432920\n",
      "Iteration 8760, loss = 0.07431335\n",
      "Iteration 8761, loss = 0.07429751\n",
      "Iteration 8762, loss = 0.07428167\n",
      "Iteration 8763, loss = 0.07426584\n",
      "Iteration 8764, loss = 0.07425002\n",
      "Iteration 8765, loss = 0.07423421\n",
      "Iteration 8766, loss = 0.07421840\n",
      "Iteration 8767, loss = 0.07420260\n",
      "Iteration 8768, loss = 0.07418681\n",
      "Iteration 8769, loss = 0.07417103\n",
      "Iteration 8770, loss = 0.07415525\n",
      "Iteration 8771, loss = 0.07413948\n",
      "Iteration 8772, loss = 0.07412372\n",
      "Iteration 8773, loss = 0.07410796\n",
      "Iteration 8774, loss = 0.07409221\n",
      "Iteration 8775, loss = 0.07407647\n",
      "Iteration 8776, loss = 0.07406073\n",
      "Iteration 8777, loss = 0.07404501\n",
      "Iteration 8778, loss = 0.07402929\n",
      "Iteration 8779, loss = 0.07401357\n",
      "Iteration 8780, loss = 0.07399787\n",
      "Iteration 8781, loss = 0.07398217\n",
      "Iteration 8782, loss = 0.07396648\n",
      "Iteration 8783, loss = 0.07395079\n",
      "Iteration 8784, loss = 0.07393511\n",
      "Iteration 8785, loss = 0.07391944\n",
      "Iteration 8786, loss = 0.07390378\n",
      "Iteration 8787, loss = 0.07388812\n",
      "Iteration 8788, loss = 0.07387247\n",
      "Iteration 8789, loss = 0.07385683\n",
      "Iteration 8790, loss = 0.07384120\n",
      "Iteration 8791, loss = 0.07382557\n",
      "Iteration 8792, loss = 0.07380995\n",
      "Iteration 8793, loss = 0.07379433\n",
      "Iteration 8794, loss = 0.07377872\n",
      "Iteration 8795, loss = 0.07376312\n",
      "Iteration 8796, loss = 0.07374753\n",
      "Iteration 8797, loss = 0.07373195\n",
      "Iteration 8798, loss = 0.07371637\n",
      "Iteration 8799, loss = 0.07370080\n",
      "Iteration 8800, loss = 0.07368523\n",
      "Iteration 8801, loss = 0.07366967\n",
      "Iteration 8802, loss = 0.07365412\n",
      "Iteration 8803, loss = 0.07363858\n",
      "Iteration 8804, loss = 0.07362304\n",
      "Iteration 8805, loss = 0.07360751\n",
      "Iteration 8806, loss = 0.07359199\n",
      "Iteration 8807, loss = 0.07357647\n",
      "Iteration 8808, loss = 0.07356096\n",
      "Iteration 8809, loss = 0.07354546\n",
      "Iteration 8810, loss = 0.07352997\n",
      "Iteration 8811, loss = 0.07351448\n",
      "Iteration 8812, loss = 0.07349900\n",
      "Iteration 8813, loss = 0.07348353\n",
      "Iteration 8814, loss = 0.07346806\n",
      "Iteration 8815, loss = 0.07345260\n",
      "Iteration 8816, loss = 0.07343715\n",
      "Iteration 8817, loss = 0.07342170\n",
      "Iteration 8818, loss = 0.07340626\n",
      "Iteration 8819, loss = 0.07339083\n",
      "Iteration 8820, loss = 0.07337540\n",
      "Iteration 8821, loss = 0.07335999\n",
      "Iteration 8822, loss = 0.07334457\n",
      "Iteration 8823, loss = 0.07332917\n",
      "Iteration 8824, loss = 0.07331377\n",
      "Iteration 8825, loss = 0.07329838\n",
      "Iteration 8826, loss = 0.07328300\n",
      "Iteration 8827, loss = 0.07326762\n",
      "Iteration 8828, loss = 0.07325225\n",
      "Iteration 8829, loss = 0.07323689\n",
      "Iteration 8830, loss = 0.07322153\n",
      "Iteration 8831, loss = 0.07320618\n",
      "Iteration 8832, loss = 0.07319084\n",
      "Iteration 8833, loss = 0.07317551\n",
      "Iteration 8834, loss = 0.07316018\n",
      "Iteration 8835, loss = 0.07314486\n",
      "Iteration 8836, loss = 0.07312954\n",
      "Iteration 8837, loss = 0.07311423\n",
      "Iteration 8838, loss = 0.07309893\n",
      "Iteration 8839, loss = 0.07308364\n",
      "Iteration 8840, loss = 0.07306835\n",
      "Iteration 8841, loss = 0.07305307\n",
      "Iteration 8842, loss = 0.07303780\n",
      "Iteration 8843, loss = 0.07302253\n",
      "Iteration 8844, loss = 0.07300727\n",
      "Iteration 8845, loss = 0.07299202\n",
      "Iteration 8846, loss = 0.07297677\n",
      "Iteration 8847, loss = 0.07296153\n",
      "Iteration 8848, loss = 0.07294630\n",
      "Iteration 8849, loss = 0.07293108\n",
      "Iteration 8850, loss = 0.07291586\n",
      "Iteration 8851, loss = 0.07290064\n",
      "Iteration 8852, loss = 0.07288544\n",
      "Iteration 8853, loss = 0.07287024\n",
      "Iteration 8854, loss = 0.07285505\n",
      "Iteration 8855, loss = 0.07283986\n",
      "Iteration 8856, loss = 0.07282469\n",
      "Iteration 8857, loss = 0.07280952\n",
      "Iteration 8858, loss = 0.07279435\n",
      "Iteration 8859, loss = 0.07277919\n",
      "Iteration 8860, loss = 0.07276404\n",
      "Iteration 8861, loss = 0.07274890\n",
      "Iteration 8862, loss = 0.07273376\n",
      "Iteration 8863, loss = 0.07271863\n",
      "Iteration 8864, loss = 0.07270351\n",
      "Iteration 8865, loss = 0.07268839\n",
      "Iteration 8866, loss = 0.07267328\n",
      "Iteration 8867, loss = 0.07265818\n",
      "Iteration 8868, loss = 0.07264308\n",
      "Iteration 8869, loss = 0.07262799\n",
      "Iteration 8870, loss = 0.07261291\n",
      "Iteration 8871, loss = 0.07259783\n",
      "Iteration 8872, loss = 0.07258276\n",
      "Iteration 8873, loss = 0.07256770\n",
      "Iteration 8874, loss = 0.07255264\n",
      "Iteration 8875, loss = 0.07253759\n",
      "Iteration 8876, loss = 0.07252255\n",
      "Iteration 8877, loss = 0.07250751\n",
      "Iteration 8878, loss = 0.07249248\n",
      "Iteration 8879, loss = 0.07247746\n",
      "Iteration 8880, loss = 0.07246244\n",
      "Iteration 8881, loss = 0.07244743\n",
      "Iteration 8882, loss = 0.07243243\n",
      "Iteration 8883, loss = 0.07241743\n",
      "Iteration 8884, loss = 0.07240244\n",
      "Iteration 8885, loss = 0.07238746\n",
      "Iteration 8886, loss = 0.07237248\n",
      "Iteration 8887, loss = 0.07235751\n",
      "Iteration 8888, loss = 0.07234255\n",
      "Iteration 8889, loss = 0.07232760\n",
      "Iteration 8890, loss = 0.07231265\n",
      "Iteration 8891, loss = 0.07229770\n",
      "Iteration 8892, loss = 0.07228277\n",
      "Iteration 8893, loss = 0.07226784\n",
      "Iteration 8894, loss = 0.07225291\n",
      "Iteration 8895, loss = 0.07223800\n",
      "Iteration 8896, loss = 0.07222309\n",
      "Iteration 8897, loss = 0.07220819\n",
      "Iteration 8898, loss = 0.07219329\n",
      "Iteration 8899, loss = 0.07217840\n",
      "Iteration 8900, loss = 0.07216352\n",
      "Iteration 8901, loss = 0.07214864\n",
      "Iteration 8902, loss = 0.07213377\n",
      "Iteration 8903, loss = 0.07211891\n",
      "Iteration 8904, loss = 0.07210405\n",
      "Iteration 8905, loss = 0.07208920\n",
      "Iteration 8906, loss = 0.07207435\n",
      "Iteration 8907, loss = 0.07205952\n",
      "Iteration 8908, loss = 0.07204469\n",
      "Iteration 8909, loss = 0.07202986\n",
      "Iteration 8910, loss = 0.07201505\n",
      "Iteration 8911, loss = 0.07200024\n",
      "Iteration 8912, loss = 0.07198543\n",
      "Iteration 8913, loss = 0.07197063\n",
      "Iteration 8914, loss = 0.07195584\n",
      "Iteration 8915, loss = 0.07194106\n",
      "Iteration 8916, loss = 0.07192628\n",
      "Iteration 8917, loss = 0.07191151\n",
      "Iteration 8918, loss = 0.07189674\n",
      "Iteration 8919, loss = 0.07188198\n",
      "Iteration 8920, loss = 0.07186723\n",
      "Iteration 8921, loss = 0.07185249\n",
      "Iteration 8922, loss = 0.07183775\n",
      "Iteration 8923, loss = 0.07182302\n",
      "Iteration 8924, loss = 0.07180829\n",
      "Iteration 8925, loss = 0.07179357\n",
      "Iteration 8926, loss = 0.07177886\n",
      "Iteration 8927, loss = 0.07176415\n",
      "Iteration 8928, loss = 0.07174945\n",
      "Iteration 8929, loss = 0.07173476\n",
      "Iteration 8930, loss = 0.07172007\n",
      "Iteration 8931, loss = 0.07170539\n",
      "Iteration 8932, loss = 0.07169072\n",
      "Iteration 8933, loss = 0.07167605\n",
      "Iteration 8934, loss = 0.07166139\n",
      "Iteration 8935, loss = 0.07164674\n",
      "Iteration 8936, loss = 0.07163209\n",
      "Iteration 8937, loss = 0.07161745\n",
      "Iteration 8938, loss = 0.07160281\n",
      "Iteration 8939, loss = 0.07158818\n",
      "Iteration 8940, loss = 0.07157356\n",
      "Iteration 8941, loss = 0.07155895\n",
      "Iteration 8942, loss = 0.07154434\n",
      "Iteration 8943, loss = 0.07152973\n",
      "Iteration 8944, loss = 0.07151514\n",
      "Iteration 8945, loss = 0.07150055\n",
      "Iteration 8946, loss = 0.07148596\n",
      "Iteration 8947, loss = 0.07147139\n",
      "Iteration 8948, loss = 0.07145682\n",
      "Iteration 8949, loss = 0.07144225\n",
      "Iteration 8950, loss = 0.07142769\n",
      "Iteration 8951, loss = 0.07141314\n",
      "Iteration 8952, loss = 0.07139860\n",
      "Iteration 8953, loss = 0.07138406\n",
      "Iteration 8954, loss = 0.07136953\n",
      "Iteration 8955, loss = 0.07135500\n",
      "Iteration 8956, loss = 0.07134048\n",
      "Iteration 8957, loss = 0.07132597\n",
      "Iteration 8958, loss = 0.07131146\n",
      "Iteration 8959, loss = 0.07129696\n",
      "Iteration 8960, loss = 0.07128247\n",
      "Iteration 8961, loss = 0.07126798\n",
      "Iteration 8962, loss = 0.07125350\n",
      "Iteration 8963, loss = 0.07123903\n",
      "Iteration 8964, loss = 0.07122456\n",
      "Iteration 8965, loss = 0.07121010\n",
      "Iteration 8966, loss = 0.07119564\n",
      "Iteration 8967, loss = 0.07118119\n",
      "Iteration 8968, loss = 0.07116675\n",
      "Iteration 8969, loss = 0.07115231\n",
      "Iteration 8970, loss = 0.07113788\n",
      "Iteration 8971, loss = 0.07112346\n",
      "Iteration 8972, loss = 0.07110904\n",
      "Iteration 8973, loss = 0.07109463\n",
      "Iteration 8974, loss = 0.07108022\n",
      "Iteration 8975, loss = 0.07106582\n",
      "Iteration 8976, loss = 0.07105143\n",
      "Iteration 8977, loss = 0.07103705\n",
      "Iteration 8978, loss = 0.07102267\n",
      "Iteration 8979, loss = 0.07100829\n",
      "Iteration 8980, loss = 0.07099393\n",
      "Iteration 8981, loss = 0.07097956\n",
      "Iteration 8982, loss = 0.07096521\n",
      "Iteration 8983, loss = 0.07095086\n",
      "Iteration 8984, loss = 0.07093652\n",
      "Iteration 8985, loss = 0.07092218\n",
      "Iteration 8986, loss = 0.07090786\n",
      "Iteration 8987, loss = 0.07089353\n",
      "Iteration 8988, loss = 0.07087922\n",
      "Iteration 8989, loss = 0.07086490\n",
      "Iteration 8990, loss = 0.07085060\n",
      "Iteration 8991, loss = 0.07083630\n",
      "Iteration 8992, loss = 0.07082201\n",
      "Iteration 8993, loss = 0.07080772\n",
      "Iteration 8994, loss = 0.07079345\n",
      "Iteration 8995, loss = 0.07077917\n",
      "Iteration 8996, loss = 0.07076491\n",
      "Iteration 8997, loss = 0.07075065\n",
      "Iteration 8998, loss = 0.07073639\n",
      "Iteration 8999, loss = 0.07072214\n",
      "Iteration 9000, loss = 0.07070790\n",
      "Iteration 9001, loss = 0.07069367\n",
      "Iteration 9002, loss = 0.07067944\n",
      "Iteration 9003, loss = 0.07066521\n",
      "Iteration 9004, loss = 0.07065100\n",
      "Iteration 9005, loss = 0.07063678\n",
      "Iteration 9006, loss = 0.07062258\n",
      "Iteration 9007, loss = 0.07060838\n",
      "Iteration 9008, loss = 0.07059419\n",
      "Iteration 9009, loss = 0.07058000\n",
      "Iteration 9010, loss = 0.07056582\n",
      "Iteration 9011, loss = 0.07055165\n",
      "Iteration 9012, loss = 0.07053748\n",
      "Iteration 9013, loss = 0.07052332\n",
      "Iteration 9014, loss = 0.07050917\n",
      "Iteration 9015, loss = 0.07049502\n",
      "Iteration 9016, loss = 0.07048087\n",
      "Iteration 9017, loss = 0.07046674\n",
      "Iteration 9018, loss = 0.07045261\n",
      "Iteration 9019, loss = 0.07043848\n",
      "Iteration 9020, loss = 0.07042436\n",
      "Iteration 9021, loss = 0.07041025\n",
      "Iteration 9022, loss = 0.07039615\n",
      "Iteration 9023, loss = 0.07038205\n",
      "Iteration 9024, loss = 0.07036795\n",
      "Iteration 9025, loss = 0.07035387\n",
      "Iteration 9026, loss = 0.07033978\n",
      "Iteration 9027, loss = 0.07032571\n",
      "Iteration 9028, loss = 0.07031164\n",
      "Iteration 9029, loss = 0.07029758\n",
      "Iteration 9030, loss = 0.07028352\n",
      "Iteration 9031, loss = 0.07026947\n",
      "Iteration 9032, loss = 0.07025542\n",
      "Iteration 9033, loss = 0.07024139\n",
      "Iteration 9034, loss = 0.07022735\n",
      "Iteration 9035, loss = 0.07021333\n",
      "Iteration 9036, loss = 0.07019931\n",
      "Iteration 9037, loss = 0.07018529\n",
      "Iteration 9038, loss = 0.07017128\n",
      "Iteration 9039, loss = 0.07015728\n",
      "Iteration 9040, loss = 0.07014329\n",
      "Iteration 9041, loss = 0.07012930\n",
      "Iteration 9042, loss = 0.07011531\n",
      "Iteration 9043, loss = 0.07010133\n",
      "Iteration 9044, loss = 0.07008736\n",
      "Iteration 9045, loss = 0.07007340\n",
      "Iteration 9046, loss = 0.07005944\n",
      "Iteration 9047, loss = 0.07004548\n",
      "Iteration 9048, loss = 0.07003154\n",
      "Iteration 9049, loss = 0.07001759\n",
      "Iteration 9050, loss = 0.07000366\n",
      "Iteration 9051, loss = 0.06998973\n",
      "Iteration 9052, loss = 0.06997581\n",
      "Iteration 9053, loss = 0.06996189\n",
      "Iteration 9054, loss = 0.06994798\n",
      "Iteration 9055, loss = 0.06993407\n",
      "Iteration 9056, loss = 0.06992017\n",
      "Iteration 9057, loss = 0.06990628\n",
      "Iteration 9058, loss = 0.06989239\n",
      "Iteration 9059, loss = 0.06987851\n",
      "Iteration 9060, loss = 0.06986464\n",
      "Iteration 9061, loss = 0.06985077\n",
      "Iteration 9062, loss = 0.06983690\n",
      "Iteration 9063, loss = 0.06982305\n",
      "Iteration 9064, loss = 0.06980920\n",
      "Iteration 9065, loss = 0.06979535\n",
      "Iteration 9066, loss = 0.06978151\n",
      "Iteration 9067, loss = 0.06976768\n",
      "Iteration 9068, loss = 0.06975385\n",
      "Iteration 9069, loss = 0.06974003\n",
      "Iteration 9070, loss = 0.06972621\n",
      "Iteration 9071, loss = 0.06971241\n",
      "Iteration 9072, loss = 0.06969860\n",
      "Iteration 9073, loss = 0.06968480\n",
      "Iteration 9074, loss = 0.06967101\n",
      "Iteration 9075, loss = 0.06965723\n",
      "Iteration 9076, loss = 0.06964345\n",
      "Iteration 9077, loss = 0.06962967\n",
      "Iteration 9078, loss = 0.06961591\n",
      "Iteration 9079, loss = 0.06960214\n",
      "Iteration 9080, loss = 0.06958839\n",
      "Iteration 9081, loss = 0.06957464\n",
      "Iteration 9082, loss = 0.06956090\n",
      "Iteration 9083, loss = 0.06954716\n",
      "Iteration 9084, loss = 0.06953342\n",
      "Iteration 9085, loss = 0.06951970\n",
      "Iteration 9086, loss = 0.06950598\n",
      "Iteration 9087, loss = 0.06949226\n",
      "Iteration 9088, loss = 0.06947856\n",
      "Iteration 9089, loss = 0.06946485\n",
      "Iteration 9090, loss = 0.06945116\n",
      "Iteration 9091, loss = 0.06943747\n",
      "Iteration 9092, loss = 0.06942378\n",
      "Iteration 9093, loss = 0.06941010\n",
      "Iteration 9094, loss = 0.06939643\n",
      "Iteration 9095, loss = 0.06938276\n",
      "Iteration 9096, loss = 0.06936910\n",
      "Iteration 9097, loss = 0.06935544\n",
      "Iteration 9098, loss = 0.06934179\n",
      "Iteration 9099, loss = 0.06932815\n",
      "Iteration 9100, loss = 0.06931451\n",
      "Iteration 9101, loss = 0.06930088\n",
      "Iteration 9102, loss = 0.06928726\n",
      "Iteration 9103, loss = 0.06927363\n",
      "Iteration 9104, loss = 0.06926002\n",
      "Iteration 9105, loss = 0.06924641\n",
      "Iteration 9106, loss = 0.06923281\n",
      "Iteration 9107, loss = 0.06921921\n",
      "Iteration 9108, loss = 0.06920562\n",
      "Iteration 9109, loss = 0.06919204\n",
      "Iteration 9110, loss = 0.06917846\n",
      "Iteration 9111, loss = 0.06916488\n",
      "Iteration 9112, loss = 0.06915132\n",
      "Iteration 9113, loss = 0.06913775\n",
      "Iteration 9114, loss = 0.06912420\n",
      "Iteration 9115, loss = 0.06911065\n",
      "Iteration 9116, loss = 0.06909710\n",
      "Iteration 9117, loss = 0.06908356\n",
      "Iteration 9118, loss = 0.06907003\n",
      "Iteration 9119, loss = 0.06905650\n",
      "Iteration 9120, loss = 0.06904298\n",
      "Iteration 9121, loss = 0.06902947\n",
      "Iteration 9122, loss = 0.06901596\n",
      "Iteration 9123, loss = 0.06900245\n",
      "Iteration 9124, loss = 0.06898895\n",
      "Iteration 9125, loss = 0.06897546\n",
      "Iteration 9126, loss = 0.06896198\n",
      "Iteration 9127, loss = 0.06894850\n",
      "Iteration 9128, loss = 0.06893502\n",
      "Iteration 9129, loss = 0.06892155\n",
      "Iteration 9130, loss = 0.06890809\n",
      "Iteration 9131, loss = 0.06889463\n",
      "Iteration 9132, loss = 0.06888118\n",
      "Iteration 9133, loss = 0.06886773\n",
      "Iteration 9134, loss = 0.06885429\n",
      "Iteration 9135, loss = 0.06884086\n",
      "Iteration 9136, loss = 0.06882743\n",
      "Iteration 9137, loss = 0.06881400\n",
      "Iteration 9138, loss = 0.06880059\n",
      "Iteration 9139, loss = 0.06878717\n",
      "Iteration 9140, loss = 0.06877377\n",
      "Iteration 9141, loss = 0.06876037\n",
      "Iteration 9142, loss = 0.06874697\n",
      "Iteration 9143, loss = 0.06873358\n",
      "Iteration 9144, loss = 0.06872020\n",
      "Iteration 9145, loss = 0.06870682\n",
      "Iteration 9146, loss = 0.06869345\n",
      "Iteration 9147, loss = 0.06868009\n",
      "Iteration 9148, loss = 0.06866673\n",
      "Iteration 9149, loss = 0.06865337\n",
      "Iteration 9150, loss = 0.06864002\n",
      "Iteration 9151, loss = 0.06862668\n",
      "Iteration 9152, loss = 0.06861334\n",
      "Iteration 9153, loss = 0.06860001\n",
      "Iteration 9154, loss = 0.06858668\n",
      "Iteration 9155, loss = 0.06857336\n",
      "Iteration 9156, loss = 0.06856005\n",
      "Iteration 9157, loss = 0.06854674\n",
      "Iteration 9158, loss = 0.06853343\n",
      "Iteration 9159, loss = 0.06852014\n",
      "Iteration 9160, loss = 0.06850684\n",
      "Iteration 9161, loss = 0.06849356\n",
      "Iteration 9162, loss = 0.06848028\n",
      "Iteration 9163, loss = 0.06846700\n",
      "Iteration 9164, loss = 0.06845373\n",
      "Iteration 9165, loss = 0.06844047\n",
      "Iteration 9166, loss = 0.06842721\n",
      "Iteration 9167, loss = 0.06841396\n",
      "Iteration 9168, loss = 0.06840071\n",
      "Iteration 9169, loss = 0.06838747\n",
      "Iteration 9170, loss = 0.06837423\n",
      "Iteration 9171, loss = 0.06836100\n",
      "Iteration 9172, loss = 0.06834778\n",
      "Iteration 9173, loss = 0.06833456\n",
      "Iteration 9174, loss = 0.06832135\n",
      "Iteration 9175, loss = 0.06830814\n",
      "Iteration 9176, loss = 0.06829494\n",
      "Iteration 9177, loss = 0.06828174\n",
      "Iteration 9178, loss = 0.06826855\n",
      "Iteration 9179, loss = 0.06825536\n",
      "Iteration 9180, loss = 0.06824218\n",
      "Iteration 9181, loss = 0.06822901\n",
      "Iteration 9182, loss = 0.06821584\n",
      "Iteration 9183, loss = 0.06820268\n",
      "Iteration 9184, loss = 0.06818952\n",
      "Iteration 9185, loss = 0.06817637\n",
      "Iteration 9186, loss = 0.06816322\n",
      "Iteration 9187, loss = 0.06815008\n",
      "Iteration 9188, loss = 0.06813695\n",
      "Iteration 9189, loss = 0.06812382\n",
      "Iteration 9190, loss = 0.06811069\n",
      "Iteration 9191, loss = 0.06809757\n",
      "Iteration 9192, loss = 0.06808446\n",
      "Iteration 9193, loss = 0.06807135\n",
      "Iteration 9194, loss = 0.06805825\n",
      "Iteration 9195, loss = 0.06804516\n",
      "Iteration 9196, loss = 0.06803207\n",
      "Iteration 9197, loss = 0.06801898\n",
      "Iteration 9198, loss = 0.06800590\n",
      "Iteration 9199, loss = 0.06799283\n",
      "Iteration 9200, loss = 0.06797976\n",
      "Iteration 9201, loss = 0.06796670\n",
      "Iteration 9202, loss = 0.06795364\n",
      "Iteration 9203, loss = 0.06794059\n",
      "Iteration 9204, loss = 0.06792754\n",
      "Iteration 9205, loss = 0.06791450\n",
      "Iteration 9206, loss = 0.06790146\n",
      "Iteration 9207, loss = 0.06788843\n",
      "Iteration 9208, loss = 0.06787541\n",
      "Iteration 9209, loss = 0.06786239\n",
      "Iteration 9210, loss = 0.06784938\n",
      "Iteration 9211, loss = 0.06783637\n",
      "Iteration 9212, loss = 0.06782337\n",
      "Iteration 9213, loss = 0.06781037\n",
      "Iteration 9214, loss = 0.06779738\n",
      "Iteration 9215, loss = 0.06778439\n",
      "Iteration 9216, loss = 0.06777141\n",
      "Iteration 9217, loss = 0.06775844\n",
      "Iteration 9218, loss = 0.06774547\n",
      "Iteration 9219, loss = 0.06773251\n",
      "Iteration 9220, loss = 0.06771955\n",
      "Iteration 9221, loss = 0.06770659\n",
      "Iteration 9222, loss = 0.06769365\n",
      "Iteration 9223, loss = 0.06768070\n",
      "Iteration 9224, loss = 0.06766777\n",
      "Iteration 9225, loss = 0.06765484\n",
      "Iteration 9226, loss = 0.06764191\n",
      "Iteration 9227, loss = 0.06762899\n",
      "Iteration 9228, loss = 0.06761607\n",
      "Iteration 9229, loss = 0.06760317\n",
      "Iteration 9230, loss = 0.06759026\n",
      "Iteration 9231, loss = 0.06757736\n",
      "Iteration 9232, loss = 0.06756447\n",
      "Iteration 9233, loss = 0.06755158\n",
      "Iteration 9234, loss = 0.06753870\n",
      "Iteration 9235, loss = 0.06752582\n",
      "Iteration 9236, loss = 0.06751295\n",
      "Iteration 9237, loss = 0.06750009\n",
      "Iteration 9238, loss = 0.06748722\n",
      "Iteration 9239, loss = 0.06747437\n",
      "Iteration 9240, loss = 0.06746152\n",
      "Iteration 9241, loss = 0.06744867\n",
      "Iteration 9242, loss = 0.06743584\n",
      "Iteration 9243, loss = 0.06742300\n",
      "Iteration 9244, loss = 0.06741017\n",
      "Iteration 9245, loss = 0.06739735\n",
      "Iteration 9246, loss = 0.06738453\n",
      "Iteration 9247, loss = 0.06737172\n",
      "Iteration 9248, loss = 0.06735891\n",
      "Iteration 9249, loss = 0.06734611\n",
      "Iteration 9250, loss = 0.06733332\n",
      "Iteration 9251, loss = 0.06732053\n",
      "Iteration 9252, loss = 0.06730774\n",
      "Iteration 9253, loss = 0.06729496\n",
      "Iteration 9254, loss = 0.06728219\n",
      "Iteration 9255, loss = 0.06726942\n",
      "Iteration 9256, loss = 0.06725665\n",
      "Iteration 9257, loss = 0.06724390\n",
      "Iteration 9258, loss = 0.06723114\n",
      "Iteration 9259, loss = 0.06721839\n",
      "Iteration 9260, loss = 0.06720565\n",
      "Iteration 9261, loss = 0.06719292\n",
      "Iteration 9262, loss = 0.06718018\n",
      "Iteration 9263, loss = 0.06716746\n",
      "Iteration 9264, loss = 0.06715474\n",
      "Iteration 9265, loss = 0.06714202\n",
      "Iteration 9266, loss = 0.06712931\n",
      "Iteration 9267, loss = 0.06711660\n",
      "Iteration 9268, loss = 0.06710390\n",
      "Iteration 9269, loss = 0.06709121\n",
      "Iteration 9270, loss = 0.06707852\n",
      "Iteration 9271, loss = 0.06706584\n",
      "Iteration 9272, loss = 0.06705316\n",
      "Iteration 9273, loss = 0.06704049\n",
      "Iteration 9274, loss = 0.06702782\n",
      "Iteration 9275, loss = 0.06701515\n",
      "Iteration 9276, loss = 0.06700250\n",
      "Iteration 9277, loss = 0.06698985\n",
      "Iteration 9278, loss = 0.06697720\n",
      "Iteration 9279, loss = 0.06696456\n",
      "Iteration 9280, loss = 0.06695192\n",
      "Iteration 9281, loss = 0.06693929\n",
      "Iteration 9282, loss = 0.06692666\n",
      "Iteration 9283, loss = 0.06691404\n",
      "Iteration 9284, loss = 0.06690143\n",
      "Iteration 9285, loss = 0.06688882\n",
      "Iteration 9286, loss = 0.06687621\n",
      "Iteration 9287, loss = 0.06686361\n",
      "Iteration 9288, loss = 0.06685102\n",
      "Iteration 9289, loss = 0.06683843\n",
      "Iteration 9290, loss = 0.06682585\n",
      "Iteration 9291, loss = 0.06681327\n",
      "Iteration 9292, loss = 0.06680070\n",
      "Iteration 9293, loss = 0.06678813\n",
      "Iteration 9294, loss = 0.06677557\n",
      "Iteration 9295, loss = 0.06676301\n",
      "Iteration 9296, loss = 0.06675046\n",
      "Iteration 9297, loss = 0.06673791\n",
      "Iteration 9298, loss = 0.06672537\n",
      "Iteration 9299, loss = 0.06671283\n",
      "Iteration 9300, loss = 0.06670030\n",
      "Iteration 9301, loss = 0.06668777\n",
      "Iteration 9302, loss = 0.06667525\n",
      "Iteration 9303, loss = 0.06666274\n",
      "Iteration 9304, loss = 0.06665023\n",
      "Iteration 9305, loss = 0.06663772\n",
      "Iteration 9306, loss = 0.06662522\n",
      "Iteration 9307, loss = 0.06661273\n",
      "Iteration 9308, loss = 0.06660024\n",
      "Iteration 9309, loss = 0.06658775\n",
      "Iteration 9310, loss = 0.06657527\n",
      "Iteration 9311, loss = 0.06656280\n",
      "Iteration 9312, loss = 0.06655033\n",
      "Iteration 9313, loss = 0.06653787\n",
      "Iteration 9314, loss = 0.06652541\n",
      "Iteration 9315, loss = 0.06651296\n",
      "Iteration 9316, loss = 0.06650051\n",
      "Iteration 9317, loss = 0.06648807\n",
      "Iteration 9318, loss = 0.06647563\n",
      "Iteration 9319, loss = 0.06646319\n",
      "Iteration 9320, loss = 0.06645077\n",
      "Iteration 9321, loss = 0.06643834\n",
      "Iteration 9322, loss = 0.06642593\n",
      "Iteration 9323, loss = 0.06641352\n",
      "Iteration 9324, loss = 0.06640111\n",
      "Iteration 9325, loss = 0.06638871\n",
      "Iteration 9326, loss = 0.06637631\n",
      "Iteration 9327, loss = 0.06636392\n",
      "Iteration 9328, loss = 0.06635153\n",
      "Iteration 9329, loss = 0.06633915\n",
      "Iteration 9330, loss = 0.06632678\n",
      "Iteration 9331, loss = 0.06631440\n",
      "Iteration 9332, loss = 0.06630204\n",
      "Iteration 9333, loss = 0.06628968\n",
      "Iteration 9334, loss = 0.06627732\n",
      "Iteration 9335, loss = 0.06626497\n",
      "Iteration 9336, loss = 0.06625263\n",
      "Iteration 9337, loss = 0.06624029\n",
      "Iteration 9338, loss = 0.06622795\n",
      "Iteration 9339, loss = 0.06621562\n",
      "Iteration 9340, loss = 0.06620330\n",
      "Iteration 9341, loss = 0.06619098\n",
      "Iteration 9342, loss = 0.06617866\n",
      "Iteration 9343, loss = 0.06616635\n",
      "Iteration 9344, loss = 0.06615405\n",
      "Iteration 9345, loss = 0.06614175\n",
      "Iteration 9346, loss = 0.06612945\n",
      "Iteration 9347, loss = 0.06611717\n",
      "Iteration 9348, loss = 0.06610488\n",
      "Iteration 9349, loss = 0.06609260\n",
      "Iteration 9350, loss = 0.06608033\n",
      "Iteration 9351, loss = 0.06606806\n",
      "Iteration 9352, loss = 0.06605580\n",
      "Iteration 9353, loss = 0.06604354\n",
      "Iteration 9354, loss = 0.06603128\n",
      "Iteration 9355, loss = 0.06601903\n",
      "Iteration 9356, loss = 0.06600679\n",
      "Iteration 9357, loss = 0.06599455\n",
      "Iteration 9358, loss = 0.06598232\n",
      "Iteration 9359, loss = 0.06597009\n",
      "Iteration 9360, loss = 0.06595787\n",
      "Iteration 9361, loss = 0.06594565\n",
      "Iteration 9362, loss = 0.06593343\n",
      "Iteration 9363, loss = 0.06592123\n",
      "Iteration 9364, loss = 0.06590902\n",
      "Iteration 9365, loss = 0.06589682\n",
      "Iteration 9366, loss = 0.06588463\n",
      "Iteration 9367, loss = 0.06587244\n",
      "Iteration 9368, loss = 0.06586026\n",
      "Iteration 9369, loss = 0.06584808\n",
      "Iteration 9370, loss = 0.06583591\n",
      "Iteration 9371, loss = 0.06582374\n",
      "Iteration 9372, loss = 0.06581158\n",
      "Iteration 9373, loss = 0.06579942\n",
      "Iteration 9374, loss = 0.06578727\n",
      "Iteration 9375, loss = 0.06577512\n",
      "Iteration 9376, loss = 0.06576298\n",
      "Iteration 9377, loss = 0.06575084\n",
      "Iteration 9378, loss = 0.06573870\n",
      "Iteration 9379, loss = 0.06572658\n",
      "Iteration 9380, loss = 0.06571445\n",
      "Iteration 9381, loss = 0.06570233\n",
      "Iteration 9382, loss = 0.06569022\n",
      "Iteration 9383, loss = 0.06567811\n",
      "Iteration 9384, loss = 0.06566601\n",
      "Iteration 9385, loss = 0.06565391\n",
      "Iteration 9386, loss = 0.06564182\n",
      "Iteration 9387, loss = 0.06562973\n",
      "Iteration 9388, loss = 0.06561765\n",
      "Iteration 9389, loss = 0.06560557\n",
      "Iteration 9390, loss = 0.06559350\n",
      "Iteration 9391, loss = 0.06558143\n",
      "Iteration 9392, loss = 0.06556936\n",
      "Iteration 9393, loss = 0.06555730\n",
      "Iteration 9394, loss = 0.06554525\n",
      "Iteration 9395, loss = 0.06553320\n",
      "Iteration 9396, loss = 0.06552116\n",
      "Iteration 9397, loss = 0.06550912\n",
      "Iteration 9398, loss = 0.06549709\n",
      "Iteration 9399, loss = 0.06548506\n",
      "Iteration 9400, loss = 0.06547303\n",
      "Iteration 9401, loss = 0.06546101\n",
      "Iteration 9402, loss = 0.06544900\n",
      "Iteration 9403, loss = 0.06543699\n",
      "Iteration 9404, loss = 0.06542499\n",
      "Iteration 9405, loss = 0.06541299\n",
      "Iteration 9406, loss = 0.06540099\n",
      "Iteration 9407, loss = 0.06538900\n",
      "Iteration 9408, loss = 0.06537702\n",
      "Iteration 9409, loss = 0.06536504\n",
      "Iteration 9410, loss = 0.06535306\n",
      "Iteration 9411, loss = 0.06534109\n",
      "Iteration 9412, loss = 0.06532913\n",
      "Iteration 9413, loss = 0.06531717\n",
      "Iteration 9414, loss = 0.06530521\n",
      "Iteration 9415, loss = 0.06529326\n",
      "Iteration 9416, loss = 0.06528132\n",
      "Iteration 9417, loss = 0.06526938\n",
      "Iteration 9418, loss = 0.06525744\n",
      "Iteration 9419, loss = 0.06524551\n",
      "Iteration 9420, loss = 0.06523358\n",
      "Iteration 9421, loss = 0.06522166\n",
      "Iteration 9422, loss = 0.06520975\n",
      "Iteration 9423, loss = 0.06519784\n",
      "Iteration 9424, loss = 0.06518593\n",
      "Iteration 9425, loss = 0.06517403\n",
      "Iteration 9426, loss = 0.06516213\n",
      "Iteration 9427, loss = 0.06515024\n",
      "Iteration 9428, loss = 0.06513835\n",
      "Iteration 9429, loss = 0.06512647\n",
      "Iteration 9430, loss = 0.06511459\n",
      "Iteration 9431, loss = 0.06510272\n",
      "Iteration 9432, loss = 0.06509085\n",
      "Iteration 9433, loss = 0.06507899\n",
      "Iteration 9434, loss = 0.06506713\n",
      "Iteration 9435, loss = 0.06505528\n",
      "Iteration 9436, loss = 0.06504343\n",
      "Iteration 9437, loss = 0.06503159\n",
      "Iteration 9438, loss = 0.06501975\n",
      "Iteration 9439, loss = 0.06500791\n",
      "Iteration 9440, loss = 0.06499609\n",
      "Iteration 9441, loss = 0.06498426\n",
      "Iteration 9442, loss = 0.06497244\n",
      "Iteration 9443, loss = 0.06496063\n",
      "Iteration 9444, loss = 0.06494882\n",
      "Iteration 9445, loss = 0.06493701\n",
      "Iteration 9446, loss = 0.06492521\n",
      "Iteration 9447, loss = 0.06491342\n",
      "Iteration 9448, loss = 0.06490163\n",
      "Iteration 9449, loss = 0.06488984\n",
      "Iteration 9450, loss = 0.06487806\n",
      "Iteration 9451, loss = 0.06486628\n",
      "Iteration 9452, loss = 0.06485451\n",
      "Iteration 9453, loss = 0.06484275\n",
      "Iteration 9454, loss = 0.06483098\n",
      "Iteration 9455, loss = 0.06481923\n",
      "Iteration 9456, loss = 0.06480748\n",
      "Iteration 9457, loss = 0.06479573\n",
      "Iteration 9458, loss = 0.06478399\n",
      "Iteration 9459, loss = 0.06477225\n",
      "Iteration 9460, loss = 0.06476051\n",
      "Iteration 9461, loss = 0.06474879\n",
      "Iteration 9462, loss = 0.06473706\n",
      "Iteration 9463, loss = 0.06472534\n",
      "Iteration 9464, loss = 0.06471363\n",
      "Iteration 9465, loss = 0.06470192\n",
      "Iteration 9466, loss = 0.06469022\n",
      "Iteration 9467, loss = 0.06467852\n",
      "Iteration 9468, loss = 0.06466682\n",
      "Iteration 9469, loss = 0.06465513\n",
      "Iteration 9470, loss = 0.06464344\n",
      "Iteration 9471, loss = 0.06463176\n",
      "Iteration 9472, loss = 0.06462009\n",
      "Iteration 9473, loss = 0.06460842\n",
      "Iteration 9474, loss = 0.06459675\n",
      "Iteration 9475, loss = 0.06458509\n",
      "Iteration 9476, loss = 0.06457343\n",
      "Iteration 9477, loss = 0.06456178\n",
      "Iteration 9478, loss = 0.06455013\n",
      "Iteration 9479, loss = 0.06453849\n",
      "Iteration 9480, loss = 0.06452685\n",
      "Iteration 9481, loss = 0.06451521\n",
      "Iteration 9482, loss = 0.06450359\n",
      "Iteration 9483, loss = 0.06449196\n",
      "Iteration 9484, loss = 0.06448034\n",
      "Iteration 9485, loss = 0.06446873\n",
      "Iteration 9486, loss = 0.06445712\n",
      "Iteration 9487, loss = 0.06444551\n",
      "Iteration 9488, loss = 0.06443391\n",
      "Iteration 9489, loss = 0.06442232\n",
      "Iteration 9490, loss = 0.06441072\n",
      "Iteration 9491, loss = 0.06439914\n",
      "Iteration 9492, loss = 0.06438756\n",
      "Iteration 9493, loss = 0.06437598\n",
      "Iteration 9494, loss = 0.06436441\n",
      "Iteration 9495, loss = 0.06435284\n",
      "Iteration 9496, loss = 0.06434127\n",
      "Iteration 9497, loss = 0.06432972\n",
      "Iteration 9498, loss = 0.06431816\n",
      "Iteration 9499, loss = 0.06430661\n",
      "Iteration 9500, loss = 0.06429507\n",
      "Iteration 9501, loss = 0.06428353\n",
      "Iteration 9502, loss = 0.06427199\n",
      "Iteration 9503, loss = 0.06426046\n",
      "Iteration 9504, loss = 0.06424894\n",
      "Iteration 9505, loss = 0.06423741\n",
      "Iteration 9506, loss = 0.06422590\n",
      "Iteration 9507, loss = 0.06421439\n",
      "Iteration 9508, loss = 0.06420288\n",
      "Iteration 9509, loss = 0.06419138\n",
      "Iteration 9510, loss = 0.06417988\n",
      "Iteration 9511, loss = 0.06416838\n",
      "Iteration 9512, loss = 0.06415690\n",
      "Iteration 9513, loss = 0.06414541\n",
      "Iteration 9514, loss = 0.06413393\n",
      "Iteration 9515, loss = 0.06412246\n",
      "Iteration 9516, loss = 0.06411099\n",
      "Iteration 9517, loss = 0.06409952\n",
      "Iteration 9518, loss = 0.06408806\n",
      "Iteration 9519, loss = 0.06407660\n",
      "Iteration 9520, loss = 0.06406515\n",
      "Iteration 9521, loss = 0.06405370\n",
      "Iteration 9522, loss = 0.06404226\n",
      "Iteration 9523, loss = 0.06403082\n",
      "Iteration 9524, loss = 0.06401939\n",
      "Iteration 9525, loss = 0.06400796\n",
      "Iteration 9526, loss = 0.06399654\n",
      "Iteration 9527, loss = 0.06398512\n",
      "Iteration 9528, loss = 0.06397370\n",
      "Iteration 9529, loss = 0.06396229\n",
      "Iteration 9530, loss = 0.06395089\n",
      "Iteration 9531, loss = 0.06393948\n",
      "Iteration 9532, loss = 0.06392809\n",
      "Iteration 9533, loss = 0.06391669\n",
      "Iteration 9534, loss = 0.06390531\n",
      "Iteration 9535, loss = 0.06389392\n",
      "Iteration 9536, loss = 0.06388255\n",
      "Iteration 9537, loss = 0.06387117\n",
      "Iteration 9538, loss = 0.06385980\n",
      "Iteration 9539, loss = 0.06384844\n",
      "Iteration 9540, loss = 0.06383708\n",
      "Iteration 9541, loss = 0.06382572\n",
      "Iteration 9542, loss = 0.06381437\n",
      "Iteration 9543, loss = 0.06380302\n",
      "Iteration 9544, loss = 0.06379168\n",
      "Iteration 9545, loss = 0.06378034\n",
      "Iteration 9546, loss = 0.06376901\n",
      "Iteration 9547, loss = 0.06375768\n",
      "Iteration 9548, loss = 0.06374636\n",
      "Iteration 9549, loss = 0.06373504\n",
      "Iteration 9550, loss = 0.06372373\n",
      "Iteration 9551, loss = 0.06371242\n",
      "Iteration 9552, loss = 0.06370111\n",
      "Iteration 9553, loss = 0.06368981\n",
      "Iteration 9554, loss = 0.06367851\n",
      "Iteration 9555, loss = 0.06366722\n",
      "Iteration 9556, loss = 0.06365593\n",
      "Iteration 9557, loss = 0.06364465\n",
      "Iteration 9558, loss = 0.06363337\n",
      "Iteration 9559, loss = 0.06362210\n",
      "Iteration 9560, loss = 0.06361083\n",
      "Iteration 9561, loss = 0.06359956\n",
      "Iteration 9562, loss = 0.06358830\n",
      "Iteration 9563, loss = 0.06357705\n",
      "Iteration 9564, loss = 0.06356579\n",
      "Iteration 9565, loss = 0.06355455\n",
      "Iteration 9566, loss = 0.06354330\n",
      "Iteration 9567, loss = 0.06353207\n",
      "Iteration 9568, loss = 0.06352083\n",
      "Iteration 9569, loss = 0.06350960\n",
      "Iteration 9570, loss = 0.06349838\n",
      "Iteration 9571, loss = 0.06348716\n",
      "Iteration 9572, loss = 0.06347594\n",
      "Iteration 9573, loss = 0.06346473\n",
      "Iteration 9574, loss = 0.06345353\n",
      "Iteration 9575, loss = 0.06344232\n",
      "Iteration 9576, loss = 0.06343113\n",
      "Iteration 9577, loss = 0.06341993\n",
      "Iteration 9578, loss = 0.06340874\n",
      "Iteration 9579, loss = 0.06339756\n",
      "Iteration 9580, loss = 0.06338638\n",
      "Iteration 9581, loss = 0.06337520\n",
      "Iteration 9582, loss = 0.06336403\n",
      "Iteration 9583, loss = 0.06335287\n",
      "Iteration 9584, loss = 0.06334170\n",
      "Iteration 9585, loss = 0.06333055\n",
      "Iteration 9586, loss = 0.06331939\n",
      "Iteration 9587, loss = 0.06330824\n",
      "Iteration 9588, loss = 0.06329710\n",
      "Iteration 9589, loss = 0.06328596\n",
      "Iteration 9590, loss = 0.06327482\n",
      "Iteration 9591, loss = 0.06326369\n",
      "Iteration 9592, loss = 0.06325257\n",
      "Iteration 9593, loss = 0.06324145\n",
      "Iteration 9594, loss = 0.06323033\n",
      "Iteration 9595, loss = 0.06321921\n",
      "Iteration 9596, loss = 0.06320811\n",
      "Iteration 9597, loss = 0.06319700\n",
      "Iteration 9598, loss = 0.06318590\n",
      "Iteration 9599, loss = 0.06317481\n",
      "Iteration 9600, loss = 0.06316371\n",
      "Iteration 9601, loss = 0.06315263\n",
      "Iteration 9602, loss = 0.06314155\n",
      "Iteration 9603, loss = 0.06313047\n",
      "Iteration 9604, loss = 0.06311939\n",
      "Iteration 9605, loss = 0.06310832\n",
      "Iteration 9606, loss = 0.06309726\n",
      "Iteration 9607, loss = 0.06308620\n",
      "Iteration 9608, loss = 0.06307514\n",
      "Iteration 9609, loss = 0.06306409\n",
      "Iteration 9610, loss = 0.06305304\n",
      "Iteration 9611, loss = 0.06304200\n",
      "Iteration 9612, loss = 0.06303096\n",
      "Iteration 9613, loss = 0.06301993\n",
      "Iteration 9614, loss = 0.06300890\n",
      "Iteration 9615, loss = 0.06299787\n",
      "Iteration 9616, loss = 0.06298685\n",
      "Iteration 9617, loss = 0.06297584\n",
      "Iteration 9618, loss = 0.06296482\n",
      "Iteration 9619, loss = 0.06295382\n",
      "Iteration 9620, loss = 0.06294281\n",
      "Iteration 9621, loss = 0.06293181\n",
      "Iteration 9622, loss = 0.06292082\n",
      "Iteration 9623, loss = 0.06290983\n",
      "Iteration 9624, loss = 0.06289884\n",
      "Iteration 9625, loss = 0.06288786\n",
      "Iteration 9626, loss = 0.06287688\n",
      "Iteration 9627, loss = 0.06286591\n",
      "Iteration 9628, loss = 0.06285494\n",
      "Iteration 9629, loss = 0.06284398\n",
      "Iteration 9630, loss = 0.06283302\n",
      "Iteration 9631, loss = 0.06282206\n",
      "Iteration 9632, loss = 0.06281111\n",
      "Iteration 9633, loss = 0.06280016\n",
      "Iteration 9634, loss = 0.06278922\n",
      "Iteration 9635, loss = 0.06277828\n",
      "Iteration 9636, loss = 0.06276735\n",
      "Iteration 9637, loss = 0.06275642\n",
      "Iteration 9638, loss = 0.06274549\n",
      "Iteration 9639, loss = 0.06273457\n",
      "Iteration 9640, loss = 0.06272366\n",
      "Iteration 9641, loss = 0.06271274\n",
      "Iteration 9642, loss = 0.06270184\n",
      "Iteration 9643, loss = 0.06269093\n",
      "Iteration 9644, loss = 0.06268003\n",
      "Iteration 9645, loss = 0.06266914\n",
      "Iteration 9646, loss = 0.06265825\n",
      "Iteration 9647, loss = 0.06264736\n",
      "Iteration 9648, loss = 0.06263648\n",
      "Iteration 9649, loss = 0.06262560\n",
      "Iteration 9650, loss = 0.06261473\n",
      "Iteration 9651, loss = 0.06260386\n",
      "Iteration 9652, loss = 0.06259299\n",
      "Iteration 9653, loss = 0.06258213\n",
      "Iteration 9654, loss = 0.06257127\n",
      "Iteration 9655, loss = 0.06256042\n",
      "Iteration 9656, loss = 0.06254957\n",
      "Iteration 9657, loss = 0.06253873\n",
      "Iteration 9658, loss = 0.06252789\n",
      "Iteration 9659, loss = 0.06251706\n",
      "Iteration 9660, loss = 0.06250622\n",
      "Iteration 9661, loss = 0.06249540\n",
      "Iteration 9662, loss = 0.06248458\n",
      "Iteration 9663, loss = 0.06247376\n",
      "Iteration 9664, loss = 0.06246294\n",
      "Iteration 9665, loss = 0.06245213\n",
      "Iteration 9666, loss = 0.06244133\n",
      "Iteration 9667, loss = 0.06243053\n",
      "Iteration 9668, loss = 0.06241973\n",
      "Iteration 9669, loss = 0.06240894\n",
      "Iteration 9670, loss = 0.06239815\n",
      "Iteration 9671, loss = 0.06238737\n",
      "Iteration 9672, loss = 0.06237659\n",
      "Iteration 9673, loss = 0.06236581\n",
      "Iteration 9674, loss = 0.06235504\n",
      "Iteration 9675, loss = 0.06234427\n",
      "Iteration 9676, loss = 0.06233351\n",
      "Iteration 9677, loss = 0.06232275\n",
      "Iteration 9678, loss = 0.06231200\n",
      "Iteration 9679, loss = 0.06230125\n",
      "Iteration 9680, loss = 0.06229050\n",
      "Iteration 9681, loss = 0.06227976\n",
      "Iteration 9682, loss = 0.06226902\n",
      "Iteration 9683, loss = 0.06225829\n",
      "Iteration 9684, loss = 0.06224756\n",
      "Iteration 9685, loss = 0.06223683\n",
      "Iteration 9686, loss = 0.06222611\n",
      "Iteration 9687, loss = 0.06221540\n",
      "Iteration 9688, loss = 0.06220468\n",
      "Iteration 9689, loss = 0.06219398\n",
      "Iteration 9690, loss = 0.06218327\n",
      "Iteration 9691, loss = 0.06217257\n",
      "Iteration 9692, loss = 0.06216188\n",
      "Iteration 9693, loss = 0.06215119\n",
      "Iteration 9694, loss = 0.06214050\n",
      "Iteration 9695, loss = 0.06212982\n",
      "Iteration 9696, loss = 0.06211914\n",
      "Iteration 9697, loss = 0.06210846\n",
      "Iteration 9698, loss = 0.06209779\n",
      "Iteration 9699, loss = 0.06208713\n",
      "Iteration 9700, loss = 0.06207646\n",
      "Iteration 9701, loss = 0.06206581\n",
      "Iteration 9702, loss = 0.06205515\n",
      "Iteration 9703, loss = 0.06204450\n",
      "Iteration 9704, loss = 0.06203386\n",
      "Iteration 9705, loss = 0.06202322\n",
      "Iteration 9706, loss = 0.06201258\n",
      "Iteration 9707, loss = 0.06200195\n",
      "Iteration 9708, loss = 0.06199132\n",
      "Iteration 9709, loss = 0.06198069\n",
      "Iteration 9710, loss = 0.06197007\n",
      "Iteration 9711, loss = 0.06195946\n",
      "Iteration 9712, loss = 0.06194884\n",
      "Iteration 9713, loss = 0.06193824\n",
      "Iteration 9714, loss = 0.06192763\n",
      "Iteration 9715, loss = 0.06191703\n",
      "Iteration 9716, loss = 0.06190644\n",
      "Iteration 9717, loss = 0.06189585\n",
      "Iteration 9718, loss = 0.06188526\n",
      "Iteration 9719, loss = 0.06187468\n",
      "Iteration 9720, loss = 0.06186410\n",
      "Iteration 9721, loss = 0.06185352\n",
      "Iteration 9722, loss = 0.06184295\n",
      "Iteration 9723, loss = 0.06183238\n",
      "Iteration 9724, loss = 0.06182182\n",
      "Iteration 9725, loss = 0.06181126\n",
      "Iteration 9726, loss = 0.06180071\n",
      "Iteration 9727, loss = 0.06179016\n",
      "Iteration 9728, loss = 0.06177961\n",
      "Iteration 9729, loss = 0.06176907\n",
      "Iteration 9730, loss = 0.06175853\n",
      "Iteration 9731, loss = 0.06174800\n",
      "Iteration 9732, loss = 0.06173747\n",
      "Iteration 9733, loss = 0.06172695\n",
      "Iteration 9734, loss = 0.06171642\n",
      "Iteration 9735, loss = 0.06170591\n",
      "Iteration 9736, loss = 0.06169539\n",
      "Iteration 9737, loss = 0.06168488\n",
      "Iteration 9738, loss = 0.06167438\n",
      "Iteration 9739, loss = 0.06166388\n",
      "Iteration 9740, loss = 0.06165338\n",
      "Iteration 9741, loss = 0.06164289\n",
      "Iteration 9742, loss = 0.06163240\n",
      "Iteration 9743, loss = 0.06162192\n",
      "Iteration 9744, loss = 0.06161144\n",
      "Iteration 9745, loss = 0.06160096\n",
      "Iteration 9746, loss = 0.06159049\n",
      "Iteration 9747, loss = 0.06158002\n",
      "Iteration 9748, loss = 0.06156955\n",
      "Iteration 9749, loss = 0.06155909\n",
      "Iteration 9750, loss = 0.06154864\n",
      "Iteration 9751, loss = 0.06153819\n",
      "Iteration 9752, loss = 0.06152774\n",
      "Iteration 9753, loss = 0.06151729\n",
      "Iteration 9754, loss = 0.06150686\n",
      "Iteration 9755, loss = 0.06149642\n",
      "Iteration 9756, loss = 0.06148599\n",
      "Iteration 9757, loss = 0.06147556\n",
      "Iteration 9758, loss = 0.06146514\n",
      "Iteration 9759, loss = 0.06145472\n",
      "Iteration 9760, loss = 0.06144430\n",
      "Iteration 9761, loss = 0.06143389\n",
      "Iteration 9762, loss = 0.06142348\n",
      "Iteration 9763, loss = 0.06141308\n",
      "Iteration 9764, loss = 0.06140268\n",
      "Iteration 9765, loss = 0.06139228\n",
      "Iteration 9766, loss = 0.06138189\n",
      "Iteration 9767, loss = 0.06137151\n",
      "Iteration 9768, loss = 0.06136112\n",
      "Iteration 9769, loss = 0.06135074\n",
      "Iteration 9770, loss = 0.06134037\n",
      "Iteration 9771, loss = 0.06133000\n",
      "Iteration 9772, loss = 0.06131963\n",
      "Iteration 9773, loss = 0.06130927\n",
      "Iteration 9774, loss = 0.06129891\n",
      "Iteration 9775, loss = 0.06128855\n",
      "Iteration 9776, loss = 0.06127820\n",
      "Iteration 9777, loss = 0.06126786\n",
      "Iteration 9778, loss = 0.06125751\n",
      "Iteration 9779, loss = 0.06124717\n",
      "Iteration 9780, loss = 0.06123684\n",
      "Iteration 9781, loss = 0.06122651\n",
      "Iteration 9782, loss = 0.06121618\n",
      "Iteration 9783, loss = 0.06120586\n",
      "Iteration 9784, loss = 0.06119554\n",
      "Iteration 9785, loss = 0.06118522\n",
      "Iteration 9786, loss = 0.06117491\n",
      "Iteration 9787, loss = 0.06116460\n",
      "Iteration 9788, loss = 0.06115430\n",
      "Iteration 9789, loss = 0.06114400\n",
      "Iteration 9790, loss = 0.06113371\n",
      "Iteration 9791, loss = 0.06112342\n",
      "Iteration 9792, loss = 0.06111313\n",
      "Iteration 9793, loss = 0.06110285\n",
      "Iteration 9794, loss = 0.06109257\n",
      "Iteration 9795, loss = 0.06108229\n",
      "Iteration 9796, loss = 0.06107202\n",
      "Iteration 9797, loss = 0.06106175\n",
      "Iteration 9798, loss = 0.06105149\n",
      "Iteration 9799, loss = 0.06104123\n",
      "Iteration 9800, loss = 0.06103097\n",
      "Iteration 9801, loss = 0.06102072\n",
      "Iteration 9802, loss = 0.06101047\n",
      "Iteration 9803, loss = 0.06100023\n",
      "Iteration 9804, loss = 0.06098999\n",
      "Iteration 9805, loss = 0.06097976\n",
      "Iteration 9806, loss = 0.06096952\n",
      "Iteration 9807, loss = 0.06095930\n",
      "Iteration 9808, loss = 0.06094907\n",
      "Iteration 9809, loss = 0.06093885\n",
      "Iteration 9810, loss = 0.06092864\n",
      "Iteration 9811, loss = 0.06091842\n",
      "Iteration 9812, loss = 0.06090822\n",
      "Iteration 9813, loss = 0.06089801\n",
      "Iteration 9814, loss = 0.06088781\n",
      "Iteration 9815, loss = 0.06087762\n",
      "Iteration 9816, loss = 0.06086742\n",
      "Iteration 9817, loss = 0.06085723\n",
      "Iteration 9818, loss = 0.06084705\n",
      "Iteration 9819, loss = 0.06083687\n",
      "Iteration 9820, loss = 0.06082669\n",
      "Iteration 9821, loss = 0.06081652\n",
      "Iteration 9822, loss = 0.06080635\n",
      "Iteration 9823, loss = 0.06079619\n",
      "Iteration 9824, loss = 0.06078602\n",
      "Iteration 9825, loss = 0.06077587\n",
      "Iteration 9826, loss = 0.06076571\n",
      "Iteration 9827, loss = 0.06075556\n",
      "Iteration 9828, loss = 0.06074542\n",
      "Iteration 9829, loss = 0.06073528\n",
      "Iteration 9830, loss = 0.06072514\n",
      "Iteration 9831, loss = 0.06071501\n",
      "Iteration 9832, loss = 0.06070488\n",
      "Iteration 9833, loss = 0.06069475\n",
      "Iteration 9834, loss = 0.06068463\n",
      "Iteration 9835, loss = 0.06067451\n",
      "Iteration 9836, loss = 0.06066440\n",
      "Iteration 9837, loss = 0.06065428\n",
      "Iteration 9838, loss = 0.06064418\n",
      "Iteration 9839, loss = 0.06063408\n",
      "Iteration 9840, loss = 0.06062398\n",
      "Iteration 9841, loss = 0.06061388\n",
      "Iteration 9842, loss = 0.06060379\n",
      "Iteration 9843, loss = 0.06059370\n",
      "Iteration 9844, loss = 0.06058362\n",
      "Iteration 9845, loss = 0.06057354\n",
      "Iteration 9846, loss = 0.06056346\n",
      "Iteration 9847, loss = 0.06055339\n",
      "Iteration 9848, loss = 0.06054333\n",
      "Iteration 9849, loss = 0.06053326\n",
      "Iteration 9850, loss = 0.06052320\n",
      "Iteration 9851, loss = 0.06051314\n",
      "Iteration 9852, loss = 0.06050309\n",
      "Iteration 9853, loss = 0.06049304\n",
      "Iteration 9854, loss = 0.06048300\n",
      "Iteration 9855, loss = 0.06047296\n",
      "Iteration 9856, loss = 0.06046292\n",
      "Iteration 9857, loss = 0.06045289\n",
      "Iteration 9858, loss = 0.06044286\n",
      "Iteration 9859, loss = 0.06043283\n",
      "Iteration 9860, loss = 0.06042281\n",
      "Iteration 9861, loss = 0.06041279\n",
      "Iteration 9862, loss = 0.06040278\n",
      "Iteration 9863, loss = 0.06039277\n",
      "Iteration 9864, loss = 0.06038276\n",
      "Iteration 9865, loss = 0.06037276\n",
      "Iteration 9866, loss = 0.06036276\n",
      "Iteration 9867, loss = 0.06035276\n",
      "Iteration 9868, loss = 0.06034277\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=5, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=10000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=1e-05, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "outp =  nn.predict([[0, 0],[0, 1], [1, 0], [1, 1]])\n",
    "print outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
